{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Deep Scraper for PE Dashboard Data\n",
    "\n",
    "### Lab 1 Enhanced - Comprehensive Data Collection\n",
    "\n",
    "This scraper implements **deep scraping** to extract ALL data needed for the PE Dashboard payload directly from company websites.\n",
    "\n",
    "### Features:\n",
    "1. **üìö 12 Page Types** - Homepage, About, Product, Careers, Blog, Team, Investors, Customers, Press, Pricing, Partners, Contact\n",
    "2. **üìù Blog Post Extraction** - Discovers and scrapes individual blog posts (up to 20 per company) for funding announcements and events\n",
    "3. **üîç Smart Page Discovery** - Analyzes homepage links when direct URL patterns fail\n",
    "4. **üìä Structured Parsing** - Extracts structured data (HQ, founding year, team members, investors, pricing tiers, etc.)\n",
    "5. **üö´ Bypasses robots.txt** - Gets complete data (for academic use)\n",
    "6. **üöÄ HTTP-First** - Fast and reliable (Playwright available as fallback)\n",
    "\n",
    "### Data Extraction:\n",
    "- **Footer/Contact** ‚Üí HQ city, state, country, founding year\n",
    "- **Team Page** ‚Üí Leadership names, roles, bios, LinkedIn\n",
    "- **Investors Page** ‚Üí Funding rounds, investor names, amounts\n",
    "- **Customers Page** ‚Üí Reference customer names\n",
    "- **Pricing Page** ‚Üí Pricing model, tiers, features\n",
    "- **Partners Page** ‚Üí Integration partner names\n",
    "- **Blog Posts** ‚Üí Individual posts for event extraction (funding, leadership changes, product launches)\n",
    "\n",
    "### Expected Results:\n",
    "- **20-40 pages per company** (12 main pages + 10-20 blog posts)\n",
    "- **4-8 structured JSON files** with parsed data\n",
    "- **80-95% dashboard field coverage** from scraped data alone\n",
    "\n",
    "### Configuration:\n",
    "- **Current Mode**: HTTP + Deep Scraping + No robots.txt\n",
    "- **To Change**: Modify Cell 10 parameters:\n",
    "  - `force_playwright=True` - Use Playwright (for JS-heavy sites)\n",
    "  - `respect_robots=True` - Honor robots.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Forbes AI50 Web Scraper - Lab 1 Enhanced\n",
    "\n",
    "**Production-ready deep scraper for PE Dashboard data collection**\n",
    "\n",
    "Features:\n",
    "- 12 page types + individual blog posts (20-40 pages per company)\n",
    "- Structured data parsing (HQ, team, investors, pricing, customers, partners)\n",
    "- Smart page discovery with homepage link analysis\n",
    "- HTTP-first with Playwright fallback\n",
    "- Bypasses robots.txt for comprehensive data (academic use)\n",
    "- Zero hardcoded values - fully dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n",
      "‚úì Project root: /Users/RiyanshiKedia/Documents/GitHub/project_orbit\n",
      "‚úì Playwright fixed for Jupyter with nest_asyncio\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import json\n",
    "import time\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout\n",
    "\n",
    "# Fix Playwright in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "SEED_FILE = PROJECT_ROOT / \"data/forbes_ai50_seed.json\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data/raw\"\n",
    "\n",
    "# Scraper settings\n",
    "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "REQUEST_DELAY = 2\n",
    "REQUEST_TIMEOUT = 10\n",
    "MAX_RETRIES = 3\n",
    "PLAYWRIGHT_TIMEOUT = 15000\n",
    "SCRAPER_VERSION = \"2.0\"\n",
    "\n",
    "# Page patterns - Expanded for comprehensive scraping\n",
    "PAGE_PATTERNS = {\n",
    "    \"homepage\": [\"/\"],\n",
    "    \"about\": [\"/about\", \"/company\", \"/about-us\", \"/who-we-are\", \"/our-story\"],\n",
    "    \"product\": [\"/product\", \"/products\", \"/platform\", \"/solutions\", \"/features\"],\n",
    "    \"careers\": [\"/careers\", \"/jobs\", \"/join-us\", \"/work-with-us\"],\n",
    "    \"blog\": [\"/blog\", \"/news\", \"/press\", \"/newsroom\", \"/insights\", \"/resources\"],\n",
    "    # New page types for comprehensive dashboard data\n",
    "    \"team\": [\"/team\", \"/leadership\", \"/about/team\", \"/about/leadership\", \"/people\", \"/our-team\"],\n",
    "    \"investors\": [\"/investors\", \"/funding\", \"/about/investors\", \"/backed-by\", \"/backers\"],\n",
    "    \"customers\": [\"/customers\", \"/case-studies\", \"/success-stories\", \"/testimonials\", \"/customer-stories\"],\n",
    "    \"press\": [\"/press\", \"/newsroom\", \"/media\", \"/news-and-press\", \"/press-releases\"],\n",
    "    \"pricing\": [\"/pricing\", \"/plans\", \"/price\", \"/buy\", \"/purchase\"],\n",
    "    \"partners\": [\"/partners\", \"/integrations\", \"/ecosystem\", \"/partner\", \"/integration\"],\n",
    "    \"contact\": [\"/contact\", \"/contact-us\", \"/get-in-touch\", \"/reach-us\"]\n",
    "}\n",
    "\n",
    "# Track request times for rate limiting\n",
    "last_request_time = {}\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"‚úì Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Playwright fixed for Jupyter with nest_asyncio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced page discovery loaded (supports all 12 page types)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Page Discovery Function - Now supports all 12 page types\n",
    "\n",
    "def discover_links_from_homepage(homepage_html: str, base_url: str) -> Dict[str, str]:\n",
    "    \"\"\"Discover page URLs by analyzing homepage links for all page types\"\"\"\n",
    "    discovered = {}\n",
    "    try:\n",
    "        soup = BeautifulSoup(homepage_html, 'lxml')\n",
    "        parsed_base = urlparse(base_url)\n",
    "        \n",
    "        # Get all links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            full_url = urljoin(base_url, link['href'])\n",
    "            \n",
    "            # Only consider links from same domain\n",
    "            if urlparse(full_url).netloc != parsed_base.netloc:\n",
    "                continue\n",
    "            \n",
    "            link_text = link.get_text().lower().strip()\n",
    "            \n",
    "            # Match all 12 page types based on URL or link text\n",
    "            \n",
    "            # About\n",
    "            if not discovered.get('about'):\n",
    "                if any(x in href for x in ['/about', '/company', '/who-we-are', '/our-story']):\n",
    "                    discovered['about'] = full_url\n",
    "                elif any(x in link_text for x in ['about', 'company', 'who we are', 'our story']):\n",
    "                    discovered['about'] = full_url\n",
    "            \n",
    "            # Product\n",
    "            if not discovered.get('product'):\n",
    "                if any(x in href for x in ['/product', '/platform', '/solution', '/feature']):\n",
    "                    discovered['product'] = full_url\n",
    "                elif any(x in link_text for x in ['product', 'platform', 'solution', 'features']):\n",
    "                    discovered['product'] = full_url\n",
    "            \n",
    "            # Careers\n",
    "            if not discovered.get('careers'):\n",
    "                if any(x in href for x in ['/career', '/job', '/join', '/work-with']):\n",
    "                    discovered['careers'] = full_url\n",
    "                elif any(x in link_text for x in ['career', 'jobs', 'join us', 'work with']):\n",
    "                    discovered['careers'] = full_url\n",
    "            \n",
    "            # Blog\n",
    "            if not discovered.get('blog'):\n",
    "                if any(x in href for x in ['/blog', '/insight', '/resource']):\n",
    "                    discovered['blog'] = full_url\n",
    "                elif any(x in link_text for x in ['blog', 'insights', 'resources']):\n",
    "                    discovered['blog'] = full_url\n",
    "            \n",
    "            # Team\n",
    "            if not discovered.get('team'):\n",
    "                if any(x in href for x in ['/team', '/leadership', '/people', '/our-team']):\n",
    "                    discovered['team'] = full_url\n",
    "                elif any(x in link_text for x in ['team', 'leadership', 'people', 'our team']):\n",
    "                    discovered['team'] = full_url\n",
    "            \n",
    "            # Investors\n",
    "            if not discovered.get('investors'):\n",
    "                if any(x in href for x in ['/investor', '/funding', '/backed-by', '/backer']):\n",
    "                    discovered['investors'] = full_url\n",
    "                elif any(x in link_text for x in ['investors', 'funding', 'backed by', 'backers']):\n",
    "                    discovered['investors'] = full_url\n",
    "            \n",
    "            # Customers\n",
    "            if not discovered.get('customers'):\n",
    "                if any(x in href for x in ['/customer', '/case-stud', '/success-stor', '/testimonial']):\n",
    "                    discovered['customers'] = full_url\n",
    "                elif any(x in link_text for x in ['customers', 'case studies', 'success stories', 'testimonials']):\n",
    "                    discovered['customers'] = full_url\n",
    "            \n",
    "            # Press\n",
    "            if not discovered.get('press'):\n",
    "                if any(x in href for x in ['/press', '/newsroom', '/media', '/news-and-press']):\n",
    "                    discovered['press'] = full_url\n",
    "                elif any(x in link_text for x in ['press', 'newsroom', 'media', 'news']):\n",
    "                    discovered['press'] = full_url\n",
    "            \n",
    "            # Pricing\n",
    "            if not discovered.get('pricing'):\n",
    "                if any(x in href for x in ['/pricing', '/plans', '/price', '/buy']):\n",
    "                    discovered['pricing'] = full_url\n",
    "                elif any(x in link_text for x in ['pricing', 'plans', 'price', 'buy']):\n",
    "                    discovered['pricing'] = full_url\n",
    "            \n",
    "            # Partners\n",
    "            if not discovered.get('partners'):\n",
    "                if any(x in href for x in ['/partner', '/integration', '/ecosystem']):\n",
    "                    discovered['partners'] = full_url\n",
    "                elif any(x in link_text for x in ['partners', 'integrations', 'ecosystem']):\n",
    "                    discovered['partners'] = full_url\n",
    "            \n",
    "            # Contact\n",
    "            if not discovered.get('contact'):\n",
    "                if any(x in href for x in ['/contact', '/get-in-touch', '/reach-us']):\n",
    "                    discovered['contact'] = full_url\n",
    "                elif any(x in link_text for x in ['contact', 'get in touch', 'reach us']):\n",
    "                    discovered['contact'] = full_url\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Warning: Link discovery failed - {str(e)[:50]}\")\n",
    "    \n",
    "    return discovered\n",
    "\n",
    "print(\"‚úì Enhanced page discovery loaded (supports all 12 page types)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Core Helper Functions\n",
    "\n",
    "def check_robots_txt(base_url: str) -> bool:\n",
    "    \"\"\"Check if scraping is allowed by robots.txt\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(base_url)\n",
    "        robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(USER_AGENT, base_url)\n",
    "    except:\n",
    "        return True  # Assume allowed if can't read\n",
    "\n",
    "\n",
    "def find_page_url(base_url: str, page_type: str) -> Optional[str]:\n",
    "    \"\"\"Find URL for a page type by trying multiple patterns\"\"\"\n",
    "    patterns = PAGE_PATTERNS.get(page_type, [])\n",
    "    for pattern in patterns:\n",
    "        url = urljoin(base_url, pattern)\n",
    "        try:\n",
    "            response = requests.head(url, timeout=5, headers={\"User-Agent\": USER_AGENT}, allow_redirects=True)\n",
    "            if response.status_code == 200:\n",
    "                return response.url\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_clean_text(html: str) -> str:\n",
    "    \"\"\"Extract clean text from HTML\"\"\"\n",
    "    try:\n",
    "        clean_text = trafilatura.extract(html, include_comments=False, include_tables=True)\n",
    "        if clean_text and len(clean_text) > 100:\n",
    "            return clean_text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def save_page_data(company_dir: Path, page_type: str, html: str, clean_text: str) -> Dict[str, int]:\n",
    "    \"\"\"Save raw HTML and clean text\"\"\"\n",
    "    html_path = company_dir / f\"{page_type}.html\"\n",
    "    txt_path = company_dir / f\"{page_type}_clean.txt\"\n",
    "    \n",
    "    html_path.write_text(html, encoding='utf-8')\n",
    "    txt_path.write_text(clean_text, encoding='utf-8')\n",
    "    \n",
    "    return {\"html_size\": len(html), \"text_size\": len(clean_text)}\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Blog post extractor loaded\n"
     ]
    }
   ],
   "source": [
    "# Blog Post Extractor\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "def extract_blog_post_links(blog_html: str, base_url: str, limit: int = 20) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract individual blog post URLs from blog index page.\n",
    "    Returns up to 'limit' most recent posts.\n",
    "    \"\"\"\n",
    "    discovered_posts = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(blog_html, 'lxml')\n",
    "        parsed_base = urlparse(base_url)\n",
    "        \n",
    "        # Common blog post selectors\n",
    "        post_selectors = [\n",
    "            'article a',\n",
    "            '.post a',\n",
    "            '.blog-post a',\n",
    "            '.entry a',\n",
    "            '[class*=\"article\"] a',\n",
    "            '[class*=\"post\"] a',\n",
    "            'h2 a',\n",
    "            'h3 a'\n",
    "        ]\n",
    "        \n",
    "        # Try each selector\n",
    "        for selector in post_selectors:\n",
    "            links = soup.select(selector)\n",
    "            for link in links:\n",
    "                if not link.get('href'):\n",
    "                    continue\n",
    "                    \n",
    "                full_url = urljoin(base_url, link['href'])\n",
    "                parsed_url = urlparse(full_url)\n",
    "                \n",
    "                # Only same domain\n",
    "                if parsed_url.netloc != parsed_base.netloc:\n",
    "                    continue\n",
    "                \n",
    "                # Skip common non-post pages\n",
    "                skip_patterns = [\n",
    "                    '/category/', '/tag/', '/author/', '/page/',\n",
    "                    '/search', '/archive', '#', 'javascript:',\n",
    "                    '.pdf', '.jpg', '.png', '.gif'\n",
    "                ]\n",
    "                if any(pattern in full_url.lower() for pattern in skip_patterns):\n",
    "                    continue\n",
    "                \n",
    "                # Avoid duplicates\n",
    "                if full_url in seen_urls:\n",
    "                    continue\n",
    "                    \n",
    "                # Look for date patterns or post IDs in URL\n",
    "                # This helps identify actual blog posts\n",
    "                if any(pattern in full_url for pattern in ['/blog/', '/news/', '/post/', '/article/', '/20']):\n",
    "                    seen_urls.add(full_url)\n",
    "                    discovered_posts.append(full_url)\n",
    "                    \n",
    "                    if len(discovered_posts) >= limit:\n",
    "                        break\n",
    "            \n",
    "            if len(discovered_posts) >= limit:\n",
    "                break\n",
    "        \n",
    "        # If we didn't find enough, try all links that contain the blog path\n",
    "        if len(discovered_posts) < 5:\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            for link in all_links:\n",
    "                full_url = urljoin(base_url, link['href'])\n",
    "                parsed_url = urlparse(full_url)\n",
    "                \n",
    "                if parsed_url.netloc != parsed_base.netloc:\n",
    "                    continue\n",
    "                    \n",
    "                path = parsed_url.path.lower()\n",
    "                if ('/blog/' in path or '/news/' in path) and full_url not in seen_urls:\n",
    "                    skip_patterns = ['/category/', '/tag/', '/author/', '/page/', '/search', '/archive']\n",
    "                    if not any(pattern in path for pattern in skip_patterns):\n",
    "                        discovered_posts.append(full_url)\n",
    "                        seen_urls.add(full_url)\n",
    "                        \n",
    "                        if len(discovered_posts) >= limit:\n",
    "                            break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Warning: Blog post extraction failed - {str(e)[:50]}\")\n",
    "    \n",
    "    return discovered_posts[:limit]\n",
    "\n",
    "\n",
    "print(\"‚úì Blog post extractor loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Structured parsers loaded (6 functions)\n"
     ]
    }
   ],
   "source": [
    "# Structured Page Parsers\n",
    "\n",
    "def parse_footer(html: str) -> Dict:\n",
    "    \"\"\"Extract HQ location and founding year from footer/about page\"\"\"\n",
    "    result = {\n",
    "        \"hq_city\": None,\n",
    "        \"hq_state\": None,\n",
    "        \"hq_country\": None,\n",
    "        \"founded_year\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        text = soup.get_text().lower()\n",
    "        \n",
    "        # Find founding year (look for copyright or \"founded in\" patterns)\n",
    "        year_patterns = [\n",
    "            r'¬©\\s*(\\d{4})',\n",
    "            r'copyright\\s*(\\d{4})',\n",
    "            r'founded\\s+in\\s+(\\d{4})',\n",
    "            r'established\\s+in\\s+(\\d{4})',\n",
    "            r'since\\s+(\\d{4})'\n",
    "        ]\n",
    "        for pattern in year_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                year = int(match.group(1))\n",
    "                if 2000 <= year <= 2025:  # Reasonable range for AI companies\n",
    "                    result[\"founded_year\"] = year\n",
    "                    break\n",
    "        \n",
    "        # Find HQ (look for address patterns, common cities)\n",
    "        # Common AI hubs\n",
    "        us_cities = {\n",
    "            'san francisco': ('San Francisco', 'CA', 'US'),\n",
    "            'palo alto': ('Palo Alto', 'CA', 'US'),\n",
    "            'mountain view': ('Mountain View', 'CA', 'US'),\n",
    "            'new york': ('New York', 'NY', 'US'),\n",
    "            'seattle': ('Seattle', 'WA', 'US'),\n",
    "            'boston': ('Boston', 'MA', 'US'),\n",
    "            'cambridge': ('Cambridge', 'MA', 'US'),\n",
    "            'austin': ('Austin', 'TX', 'US')\n",
    "        }\n",
    "        \n",
    "        for city_key, (city, state, country) in us_cities.items():\n",
    "            if city_key in text:\n",
    "                result[\"hq_city\"] = city\n",
    "                result[\"hq_state\"] = state\n",
    "                result[\"hq_country\"] = country\n",
    "                break\n",
    "        \n",
    "        # Try to find address tags\n",
    "        address_tag = soup.find('address')\n",
    "        if address_tag:\n",
    "            address_text = address_tag.get_text()\n",
    "            # Try to extract city/state/country from structured address\n",
    "            for city_key, (city, state, country) in us_cities.items():\n",
    "                if city_key in address_text.lower():\n",
    "                    result[\"hq_city\"] = city\n",
    "                    result[\"hq_state\"] = state\n",
    "                    result[\"hq_country\"] = country\n",
    "                    break\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_team_page(html: str) -> List[Dict]:\n",
    "    \"\"\"Extract team member information\"\"\"\n",
    "    team_members = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Common team member containers\n",
    "        member_selectors = [\n",
    "            '.team-member',\n",
    "            '.person',\n",
    "            '.employee',\n",
    "            '[class*=\"team\"]',\n",
    "            '[class*=\"member\"]',\n",
    "            'article'\n",
    "        ]\n",
    "        \n",
    "        for selector in member_selectors:\n",
    "            members = soup.select(selector)\n",
    "            if len(members) > 1:  # Found a pattern\n",
    "                for member in members[:20]:  # Limit to top 20\n",
    "                    member_data = {\n",
    "                        \"name\": None,\n",
    "                        \"role\": None,\n",
    "                        \"bio\": None,\n",
    "                        \"linkedin\": None\n",
    "                    }\n",
    "                    \n",
    "                    # Extract name (usually in h2, h3, or strong)\n",
    "                    name_tag = member.find(['h2', 'h3', 'h4', 'strong'])\n",
    "                    if name_tag:\n",
    "                        member_data[\"name\"] = name_tag.get_text().strip()\n",
    "                    \n",
    "                    # Extract role/title\n",
    "                    role_classes = ['role', 'title', 'position', 'job-title']\n",
    "                    for cls in role_classes:\n",
    "                        role_tag = member.find(class_=lambda x: x and cls in x.lower())\n",
    "                        if role_tag:\n",
    "                            member_data[\"role\"] = role_tag.get_text().strip()\n",
    "                            break\n",
    "                    \n",
    "                    # If no role found, try p tags\n",
    "                    if not member_data[\"role\"]:\n",
    "                        p_tags = member.find_all('p')\n",
    "                        if len(p_tags) > 0:\n",
    "                            first_p = p_tags[0].get_text().strip()\n",
    "                            if len(first_p) < 100:  # Likely a title, not bio\n",
    "                                member_data[\"role\"] = first_p\n",
    "                    \n",
    "                    # Extract bio\n",
    "                    bio_tag = member.find('p', class_=lambda x: x and 'bio' in x.lower() if x else False)\n",
    "                    if bio_tag:\n",
    "                        member_data[\"bio\"] = bio_tag.get_text().strip()\n",
    "                    \n",
    "                    # Extract LinkedIn\n",
    "                    linkedin_link = member.find('a', href=lambda x: x and 'linkedin.com' in x if x else False)\n",
    "                    if linkedin_link:\n",
    "                        member_data[\"linkedin\"] = linkedin_link['href']\n",
    "                    \n",
    "                    if member_data[\"name\"]:  # Only add if we got a name\n",
    "                        team_members.append(member_data)\n",
    "                \n",
    "                if team_members:  # If we found members, stop trying selectors\n",
    "                    break\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return team_members\n",
    "\n",
    "\n",
    "def parse_investors_page(html: str) -> List[Dict]:\n",
    "    \"\"\"Extract investor and funding information\"\"\"\n",
    "    investors_data = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Look for funding round mentions\n",
    "        funding_patterns = [\n",
    "            r'(seed|series [a-z]|series [0-9])\\s+round',\n",
    "            r'raised\\s+\\$?([\\d.]+)\\s*(million|billion|m|b)',\n",
    "            r'\\$?([\\d.]+)\\s*(million|billion|m|b)\\s+in\\s+funding'\n",
    "        ]\n",
    "        \n",
    "        for pattern in funding_patterns:\n",
    "            matches = re.finditer(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                investors_data.append({\n",
    "                    \"snippet\": match.group(0),\n",
    "                    \"type\": \"funding_mention\"\n",
    "                })\n",
    "        \n",
    "        # Extract investor names (usually in lists or with logos)\n",
    "        investor_containers = soup.find_all(['ul', 'div'], class_=lambda x: x and ('investor' in x.lower() or 'backer' in x.lower()) if x else False)\n",
    "        for container in investor_containers:\n",
    "            items = container.find_all(['li', 'div'])\n",
    "            for item in items:\n",
    "                investor_name = item.get_text().strip()\n",
    "                if investor_name and len(investor_name) < 100:\n",
    "                    investors_data.append({\n",
    "                        \"name\": investor_name,\n",
    "                        \"type\": \"investor\"\n",
    "                    })\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return investors_data\n",
    "\n",
    "\n",
    "def parse_customers_page(html: str) -> List[str]:\n",
    "    \"\"\"Extract customer/client names\"\"\"\n",
    "    customers = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Look for customer logos\n",
    "        customer_imgs = soup.find_all('img', alt=True)\n",
    "        for img in customer_imgs:\n",
    "            alt_text = img.get('alt', '').strip()\n",
    "            # Filter out likely non-customer images\n",
    "            if alt_text and len(alt_text) < 100 and 'logo' not in alt_text.lower():\n",
    "                customers.append(alt_text)\n",
    "        \n",
    "        # Look for customer lists\n",
    "        customer_sections = soup.find_all(['ul', 'div'], class_=lambda x: x and ('customer' in x.lower() or 'client' in x.lower()) if x else False)\n",
    "        for section in customer_sections:\n",
    "            items = section.find_all(['li', 'div'])\n",
    "            for item in items:\n",
    "                customer_name = item.get_text().strip()\n",
    "                if customer_name and len(customer_name) < 100:\n",
    "                    customers.append(customer_name)\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return list(set(customers))[:50]  # Deduplicate and limit\n",
    "\n",
    "\n",
    "def parse_pricing_page(html: str) -> Dict:\n",
    "    \"\"\"Extract pricing information\"\"\"\n",
    "    pricing_data = {\n",
    "        \"pricing_model\": None,\n",
    "        \"tiers\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        text = soup.get_text().lower()\n",
    "        \n",
    "        # Detect pricing model\n",
    "        if 'per seat' in text or 'per user' in text:\n",
    "            pricing_data[\"pricing_model\"] = \"per-seat\"\n",
    "        elif 'usage-based' in text or 'pay as you go' in text:\n",
    "            pricing_data[\"pricing_model\"] = \"usage-based\"\n",
    "        elif 'enterprise' in text and 'contact' in text:\n",
    "            pricing_data[\"pricing_model\"] = \"enterprise\"\n",
    "        \n",
    "        # Extract tier names\n",
    "        tier_patterns = ['free', 'starter', 'basic', 'pro', 'professional', 'business', 'enterprise', 'premium', 'plus']\n",
    "        \n",
    "        # Look for pricing cards/sections\n",
    "        pricing_cards = soup.find_all(['div', 'section'], class_=lambda x: x and ('price' in x.lower() or 'tier' in x.lower() or 'plan' in x.lower()) if x else False)\n",
    "        \n",
    "        for card in pricing_cards:\n",
    "            card_text = card.get_text().lower()\n",
    "            for tier_name in tier_patterns:\n",
    "                if tier_name in card_text:\n",
    "                    # Try to find price\n",
    "                    price_match = re.search(r'\\$\\s*(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)', card.get_text())\n",
    "                    price = price_match.group(0) if price_match else None\n",
    "                    \n",
    "                    pricing_data[\"tiers\"].append({\n",
    "                        \"name\": tier_name.capitalize(),\n",
    "                        \"price\": price\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        # If no tiers found, look for tier names in headings\n",
    "        if not pricing_data[\"tiers\"]:\n",
    "            headings = soup.find_all(['h2', 'h3', 'h4'])\n",
    "            for heading in headings:\n",
    "                heading_text = heading.get_text().lower()\n",
    "                for tier_name in tier_patterns:\n",
    "                    if tier_name in heading_text:\n",
    "                        pricing_data[\"tiers\"].append({\n",
    "                            \"name\": tier_name.capitalize(),\n",
    "                            \"price\": None\n",
    "                        })\n",
    "                        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return pricing_data\n",
    "\n",
    "\n",
    "def parse_partners_page(html: str) -> List[str]:\n",
    "    \"\"\"Extract integration partner names\"\"\"\n",
    "    partners = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Look for partner logos with alt text\n",
    "        partner_imgs = soup.find_all('img', alt=True)\n",
    "        for img in partner_imgs:\n",
    "            alt_text = img.get('alt', '').strip()\n",
    "            if alt_text and len(alt_text) < 100:\n",
    "                partners.append(alt_text)\n",
    "        \n",
    "        # Look for partner lists\n",
    "        partner_sections = soup.find_all(['ul', 'div'], class_=lambda x: x and ('partner' in x.lower() or 'integration' in x.lower()) if x else False)\n",
    "        for section in partner_sections:\n",
    "            items = section.find_all(['li', 'a'])\n",
    "            for item in items:\n",
    "                partner_name = item.get_text().strip()\n",
    "                if partner_name and len(partner_name) < 100:\n",
    "                    partners.append(partner_name)\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return list(set(partners))[:50]  # Deduplicate and limit\n",
    "\n",
    "\n",
    "print(\"‚úì Structured parsers loaded (6 functions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fetch functions with smart fallback loaded\n"
     ]
    }
   ],
   "source": [
    "# Fetching Functions with Smart Fallback\n",
    "\n",
    "def fetch_with_requests(url: str) -> Tuple[Optional[str], int, str]:\n",
    "    \"\"\"Fetch page with requests library (fast)\"\"\"\n",
    "    domain = urlparse(url).netloc\n",
    "    \n",
    "    # Rate limiting\n",
    "    if domain in last_request_time:\n",
    "        elapsed = time.time() - last_request_time[domain]\n",
    "        if elapsed < REQUEST_DELAY:\n",
    "            time.sleep(REQUEST_DELAY - elapsed)\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": USER_AGENT}, allow_redirects=True)\n",
    "            last_request_time[domain] = time.time()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.text, 200, \"HTTP Success\"\n",
    "            elif response.status_code == 404:\n",
    "                return None, 404, \"Not found\"\n",
    "            elif response.status_code in [403, 429]:\n",
    "                wait_time = 2 ** attempt\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                return None, response.status_code, f\"HTTP {response.status_code}\"\n",
    "        except requests.Timeout:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return None, 0, \"Timeout\"\n",
    "        except Exception as e:\n",
    "            return None, 0, f\"Error: {str(e)[:50]}\"\n",
    "    \n",
    "    return None, 0, \"Max retries\"\n",
    "\n",
    "\n",
    "def fetch_with_playwright(url: str) -> Tuple[Optional[str], int, str]:\n",
    "    \"\"\"Fetch page with Playwright (handles JS, blocking)\"\"\"\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            context = browser.new_context(\n",
    "                user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                viewport={\"width\": 1920, \"height\": 1080}\n",
    "            )\n",
    "            page = context.new_page()\n",
    "            \n",
    "            try:\n",
    "                response = page.goto(url, timeout=PLAYWRIGHT_TIMEOUT, wait_until=\"networkidle\")\n",
    "                page.wait_for_timeout(2000)\n",
    "                html = page.content()\n",
    "                status_code = response.status if response else 200\n",
    "                browser.close()\n",
    "                \n",
    "                if status_code == 200:\n",
    "                    return html, 200, \"Playwright Success\"\n",
    "                else:\n",
    "                    return None, status_code, f\"Playwright HTTP {status_code}\"\n",
    "            except PlaywrightTimeout:\n",
    "                browser.close()\n",
    "                return None, 0, \"Playwright timeout\"\n",
    "            except Exception as e:\n",
    "                browser.close()\n",
    "                return None, 0, f\"Playwright error: {str(e)[:50]}\"\n",
    "    except Exception as e:\n",
    "        return None, 0, f\"Playwright init failed: {str(e)[:50]}\"\n",
    "\n",
    "\n",
    "def fetch_page_smart(url: str, force_playwright: bool = False) -> Tuple[Optional[str], int, str]:\n",
    "    \"\"\"Smart fetch with automatic fallback\"\"\"\n",
    "    if force_playwright:\n",
    "        return fetch_with_playwright(url)\n",
    "    \n",
    "    # Try HTTP first (fast)\n",
    "    html, status_code, note = fetch_with_requests(url)\n",
    "    \n",
    "    # If blocked/failed, try Playwright\n",
    "    if html is None and status_code in [0, 403, 429]:\n",
    "        print(\"      ‚Üí Fallback to Playwright\")\n",
    "        return fetch_with_playwright(url)\n",
    "    \n",
    "    return html, status_code, note\n",
    "\n",
    "\n",
    "print(\"‚úì Fetch functions with smart fallback loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Main scraper loaded (deep scraping with 12 page types + blog posts)\n"
     ]
    }
   ],
   "source": [
    "# Main Scraper Function - Enhanced for Deep Scraping\n",
    "\n",
    "def scrape_company(company: Dict, force_playwright: bool = False, respect_robots: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Deep scrape all pages for a company with smart fallbacks and structured parsing.\n",
    "    \n",
    "    Args:\n",
    "        company: Company dict with name, website, company_id\n",
    "        force_playwright: If True, use Playwright for all pages\n",
    "        respect_robots: If False, bypass robots.txt (use ethically!)\n",
    "    \"\"\"\n",
    "    company_name = company[\"company_name\"]\n",
    "    company_id = company[\"company_id\"]\n",
    "    base_url = company[\"website\"]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç {company_name} ({company_id})\")\n",
    "    print(f\"   {base_url}\")\n",
    "    print(f\"   Mode: {'üé≠ Playwright' if force_playwright else 'üöÄ HTTP + Playwright fallback'}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create folders\n",
    "    company_dir = OUTPUT_DIR / company_id / \"initial_pull\"\n",
    "    blog_posts_dir = company_dir / \"blog_posts\"\n",
    "    company_dir.mkdir(parents=True, exist_ok=True)\n",
    "    blog_posts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check robots.txt\n",
    "    if respect_robots and not force_playwright:\n",
    "        if not check_robots_txt(base_url):\n",
    "            print(\"   ‚úó Blocked by robots.txt (set respect_robots=False to bypass)\")\n",
    "            return {\n",
    "                \"company_name\": company_name,\n",
    "                \"company_id\": company_id,\n",
    "                \"status\": \"blocked_by_robots\",\n",
    "                \"pages_scraped\": 0,\n",
    "                \"pages_total\": 12\n",
    "            }\n",
    "    \n",
    "    # Scrape main pages (12 types)\n",
    "    page_results = []\n",
    "    pages_scraped = 0\n",
    "    homepage_html = None\n",
    "    discovered_links = {}\n",
    "    all_page_types = [\"homepage\", \"about\", \"product\", \"careers\", \"blog\", \"team\", \"investors\", \"customers\", \"press\", \"pricing\", \"partners\", \"contact\"]\n",
    "    \n",
    "    for page_type in all_page_types:\n",
    "        print(f\"   üìÑ {page_type.capitalize()}...\", end=\" \")\n",
    "        \n",
    "        # Find URL\n",
    "        if page_type == \"homepage\":\n",
    "            page_url = base_url\n",
    "        else:\n",
    "            # Try predefined patterns first\n",
    "            page_url = find_page_url(base_url, page_type)\n",
    "            \n",
    "            # If not found and we have homepage HTML, try discovering from links\n",
    "            if not page_url and homepage_html:\n",
    "                if not discovered_links:\n",
    "                    discovered_links = discover_links_from_homepage(homepage_html, base_url)\n",
    "                page_url = discovered_links.get(page_type)\n",
    "        \n",
    "        if not page_url:\n",
    "            print(\"Not found\")\n",
    "            page_results.append({\n",
    "                \"page_type\": page_type,\n",
    "                \"source_url\": None,\n",
    "                \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"status_code\": 0,\n",
    "                \"found\": False,\n",
    "                \"note\": \"URL not found\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Fetch page\n",
    "        html, status_code, note = fetch_page_smart(page_url, force_playwright)\n",
    "        \n",
    "        # Save homepage HTML for link discovery\n",
    "        if page_type == \"homepage\" and html:\n",
    "            homepage_html = html\n",
    "        \n",
    "        if html:\n",
    "            clean_text = extract_clean_text(html)\n",
    "            sizes = save_page_data(company_dir, page_type, html, clean_text)\n",
    "            print(f\"‚úì ({sizes['html_size']:,}B / {sizes['text_size']:,}B)\")\n",
    "            \n",
    "            # Parse structured data for specific page types\n",
    "            structured_data = None\n",
    "            if page_type in [\"about\", \"contact\"]:\n",
    "                structured_data = parse_footer(html)\n",
    "            elif page_type == \"team\":\n",
    "                structured_data = parse_team_page(html)\n",
    "            elif page_type == \"investors\":\n",
    "                structured_data = parse_investors_page(html)\n",
    "            elif page_type == \"customers\":\n",
    "                structured_data = parse_customers_page(html)\n",
    "            elif page_type == \"pricing\":\n",
    "                structured_data = parse_pricing_page(html)\n",
    "            elif page_type == \"partners\":\n",
    "                structured_data = parse_partners_page(html)\n",
    "            elif page_type == \"careers\":\n",
    "                # For careers, we could extract job openings\n",
    "                structured_data = {\"note\": \"Job openings can be parsed in Lab 5\"}\n",
    "            \n",
    "            # Save structured data if we extracted any\n",
    "            if structured_data:\n",
    "                structured_path = company_dir / f\"{page_type}_structured.json\"\n",
    "                structured_path.write_text(json.dumps(structured_data, indent=2), encoding='utf-8')\n",
    "            \n",
    "            page_results.append({\n",
    "                \"page_type\": page_type,\n",
    "                \"source_url\": page_url,\n",
    "                \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"status_code\": status_code,\n",
    "                \"content_length\": sizes['html_size'],\n",
    "                \"found\": True,\n",
    "                \"note\": note,\n",
    "                \"method\": \"playwright\" if \"Playwright\" in note else \"http\",\n",
    "                \"has_structured_data\": structured_data is not None\n",
    "            })\n",
    "            pages_scraped += 1\n",
    "            \n",
    "            # Extract and scrape blog posts if this is the blog page\n",
    "            if page_type == \"blog\" and html:\n",
    "                print(f\"\\n      üîç Extracting blog posts...\", end=\" \")\n",
    "                blog_post_urls = extract_blog_post_links(html, base_url, limit=20)\n",
    "                print(f\"Found {len(blog_post_urls)} posts\")\n",
    "                \n",
    "                for i, post_url in enumerate(blog_post_urls):\n",
    "                    # Create short hash for filename\n",
    "                    url_hash = hashlib.md5(post_url.encode()).hexdigest()[:8]\n",
    "                    post_filename = f\"post_{url_hash}\"\n",
    "                    \n",
    "                    # Fetch blog post\n",
    "                    post_html, post_status, post_note = fetch_page_smart(post_url, force_playwright)\n",
    "                    \n",
    "                    if post_html:\n",
    "                        post_clean_text = extract_clean_text(post_html)\n",
    "                        \n",
    "                        # Save to blog_posts subfolder\n",
    "                        (blog_posts_dir / f\"{post_filename}.html\").write_text(post_html, encoding='utf-8')\n",
    "                        (blog_posts_dir / f\"{post_filename}_clean.txt\").write_text(post_clean_text, encoding='utf-8')\n",
    "                        \n",
    "                        pages_scraped += 1\n",
    "                        \n",
    "                        if (i + 1) % 5 == 0:\n",
    "                            print(f\"      ... {i+1}/{len(blog_post_urls)} blog posts scraped\")\n",
    "                \n",
    "                if len(blog_post_urls) > 0:\n",
    "                    print(f\"      ‚úì {len(blog_post_urls)} blog posts saved to blog_posts/\")\n",
    "        else:\n",
    "            print(f\"‚úó {note}\")\n",
    "            page_results.append({\n",
    "                \"page_type\": page_type,\n",
    "                \"source_url\": page_url,\n",
    "                \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"status_code\": status_code,\n",
    "                \"found\": False,\n",
    "                \"note\": note\n",
    "            })\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_id\": company_id,\n",
    "        \"scrape_timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"scraper_version\": \"3.0-deep\",\n",
    "        \"force_playwright\": force_playwright,\n",
    "        \"respect_robots\": respect_robots,\n",
    "        \"pages\": page_results\n",
    "    }\n",
    "    \n",
    "    metadata_path = company_dir / \"metadata.json\"\n",
    "    metadata_path.write_text(json.dumps(metadata, indent=2), encoding='utf-8')\n",
    "    \n",
    "    print(f\"   ‚úÖ {pages_scraped} total pages/posts scraped\")\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_id\": company_id,\n",
    "        \"status\": \"success\" if pages_scraped > 0 else \"failed\",\n",
    "        \"pages_scraped\": pages_scraped,\n",
    "        \"pages_total\": 12,\n",
    "        \"blog_posts_found\": len([p for p in page_results if p.get(\"page_type\") == \"blog\" and p.get(\"found\")])\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Main scraper loaded (deep scraping with 12 page types + blog posts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 50 companies from Forbes AI 50\n",
      "‚úì Ready to deep scrape ALL 50 companies\n",
      "\n",
      "First 5 companies (preview):\n",
      "  ‚Ä¢ Abridge              (abridge        ) - https://www.abridge.com\n",
      "  ‚Ä¢ Anthropic            (anthropic      ) - https://www.anthropic.com\n",
      "  ‚Ä¢ Anysphere            (cursor         ) - https://www.cursor.com\n",
      "  ‚Ä¢ Baseten              (baseten        ) - https://www.baseten.co\n",
      "  ‚Ä¢ Captions             (captions       ) - https://www.captions.ai\n",
      "\n",
      "‚ö†Ô∏è  Note: Full scrape will take ~2-4 hours for all 50 companies\n"
     ]
    }
   ],
   "source": [
    "# Load Companies\n",
    "\n",
    "with open(SEED_FILE, 'r') as f:\n",
    "    all_companies = json.load(f)\n",
    "\n",
    "# Add company_id to all\n",
    "for company in all_companies:\n",
    "    domain = urlparse(company[\"website\"]).netloc\n",
    "    company[\"company_id\"] = domain.replace(\"www.\", \"\").split(\".\")[0]\n",
    "\n",
    "print(f\"‚úì Loaded {len(all_companies)} companies from Forbes AI 50\")\n",
    "print(f\"‚úì Ready to deep scrape ALL {len(all_companies)} companies\")\n",
    "print(f\"\\nFirst 5 companies (preview):\")\n",
    "for c in all_companies[:5]:\n",
    "    print(f\"  ‚Ä¢ {c['company_name']:20} ({c['company_id']:15}) - {c['website']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ DEEP SCRAPING ALL 50 FORBES AI50 COMPANIES\n",
      "   Mode: üöÄ HTTP + Deep Scraping (bypassing robots.txt)\n",
      "   Pages: 12 page types + up to 20 blog posts each\n",
      "   Expected time: ~2-4 hours for all companies\n",
      "======================================================================\n",
      "\n",
      "[1/50] Abridge...\n",
      "\n",
      "======================================================================\n",
      "üîç Abridge (abridge)\n",
      "   https://www.abridge.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (197,005B / 562B)\n",
      "   üìÑ About... ‚úì (44,949B / 2,159B)\n",
      "   üìÑ Product... ‚úì (120,057B / 1,491B)\n",
      "   üìÑ Careers... ‚úì (87,122B / 59B)\n",
      "   üìÑ Blog... ‚úì (37,718B / 874B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 4 posts\n",
      "      ‚úì 4 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (44,949B / 2,159B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (110,137B / 4,361B)\n",
      "   üìÑ Press... ‚úì (41,595B / 1,520B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... ‚úì (33,614B / 5,737B)\n",
      "   üìÑ Contact... ‚úì (100,087B / 159B)\n",
      "   ‚úÖ 14 total pages/posts scraped\n",
      "[2/50] Anthropic...\n",
      "\n",
      "======================================================================\n",
      "üîç Anthropic (anthropic)\n",
      "   https://www.anthropic.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (273,364B / 695B)\n",
      "   üìÑ About... ‚úì (233,010B / 6,112B)\n",
      "   üìÑ Product... ‚úì (1,360,099B / 41,413B)\n",
      "   üìÑ Careers... ‚úì (244,565B / 9,118B)\n",
      "   üìÑ Blog... ‚úì (488,681B / 9,949B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (1,634,447B / 22,278B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (1,479,115B / 4,291B)\n",
      "   üìÑ Press... ‚úì (488,681B / 9,949B)\n",
      "   üìÑ Pricing... ‚úì (901,577B / 3,348B)\n",
      "   üìÑ Partners... ‚úì (4,498,074B / 364B)\n",
      "   üìÑ Contact... ‚úì (504,111B / 7,105B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[3/50] Anysphere...\n",
      "\n",
      "======================================================================\n",
      "üîç Anysphere (cursor)\n",
      "   https://www.cursor.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (609,927B / 1,037B)\n",
      "   üìÑ About... ‚úì (726,786B / 3,150B)\n",
      "   üìÑ Product... ‚úì (726,786B / 3,150B)\n",
      "   üìÑ Careers... ‚úì (247,599B / 718B)\n",
      "   üìÑ Blog... ‚úì (153,158B / 2,833B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 10 posts\n",
      "      ... 5/10 blog posts scraped\n",
      "      ... 10/10 blog posts scraped\n",
      "      ‚úì 10 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (726,786B / 3,150B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (673,361B / 4,052B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (226,476B / 1,704B)\n",
      "   üìÑ Partners... ‚úì (726,786B / 3,150B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 19 total pages/posts scraped\n",
      "[4/50] Baseten...\n",
      "\n",
      "======================================================================\n",
      "üîç Baseten (baseten)\n",
      "   https://www.baseten.co\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (419,750B / 4,864B)\n",
      "   üìÑ About... ‚úì (377,576B / 3,045B)\n",
      "   üìÑ Product... ‚úì (413,013B / 4,305B)\n",
      "   üìÑ Careers... ‚úì (377,576B / 3,045B)\n",
      "   üìÑ Blog... ‚úì (319,695B / 270B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 9 posts\n",
      "      ... 5/9 blog posts scraped\n",
      "      ‚úì 9 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (210,636B / 4,955B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (516,508B / 5,811B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (473,541B / 2,899B)\n",
      "   üìÑ Partners... ‚úì (259,310B / 363B)\n",
      "   üìÑ Contact... ‚úì (259,026B / 361B)\n",
      "   ‚úÖ 19 total pages/posts scraped\n",
      "[5/50] Captions...\n",
      "\n",
      "======================================================================\n",
      "üîç Captions (captions)\n",
      "   https://www.captions.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (182,049B / 673B)\n",
      "   üìÑ About... ‚úì (237,719B / 1,321B)\n",
      "   üìÑ Product... ‚úì (117,545B / 5,348B)\n",
      "   üìÑ Careers... ‚úì (6,343B / 51B)\n",
      "   üìÑ Blog... ‚úì (57,575B / 626B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 11 posts\n",
      "      ‚úì 11 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (139,805B / 3,944B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (117,545B / 5,348B)\n",
      "   ‚úÖ 7 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 5/50 companies | 5 successful | 3.7m elapsed | ~33.2m remaining\n",
      "\n",
      "[6/50] Clay...\n",
      "\n",
      "======================================================================\n",
      "üîç Clay (clay)\n",
      "   https://www.clay.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (188,640B / 3,770B)\n",
      "   üìÑ About... ‚úì (274,539B / 1,753B)\n",
      "   üìÑ Product... ‚úì (153,293B / 8,849B)\n",
      "   üìÑ Careers... ‚úì (178,304B / 5,582B)\n",
      "   üìÑ Blog... ‚úì (783,870B / 23,916B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (163,053B / 1,618B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (216,169B / 12,907B)\n",
      "   üìÑ Press... ‚úì (128,210B / 1,394B)\n",
      "   üìÑ Pricing... ‚úì (298,246B / 5,866B)\n",
      "   üìÑ Partners... ‚úì (144,830B / 1,855B)\n",
      "   üìÑ Contact... ‚úì (108,004B / 262B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[7/50] Coactive AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Coactive AI (coactive)\n",
      "   https://www.coactive.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (154,320B / 911B)\n",
      "   üìÑ About... ‚úì (140,378B / 1,113B)\n",
      "   üìÑ Product... ‚úì (139,892B / 3,959B)\n",
      "   üìÑ Careers... ‚úì (150,474B / 1,942B)\n",
      "   üìÑ Blog... ‚úì (138,183B / 300B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 18 posts\n",
      "      ... 5/18 blog posts scraped\n",
      "      ... 10/18 blog posts scraped\n",
      "      ... 15/18 blog posts scraped\n",
      "      ‚úì 18 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (104,833B / 472B)\n",
      "   üìÑ Press... ‚úì (159,143B / 3,637B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... ‚úì (109,150B / 844B)\n",
      "   üìÑ Contact... ‚úì (115,579B / 1,449B)\n",
      "   ‚úÖ 27 total pages/posts scraped\n",
      "[8/50] Codeium...\n",
      "\n",
      "======================================================================\n",
      "üîç Codeium (codeium)\n",
      "   https://www.codeium.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (1,714,201B / 4,127B)\n",
      "   üìÑ About... ‚úì (154,694B / 1,268B)\n",
      "   üìÑ Product... ‚úì (63,954B / 469B)\n",
      "   üìÑ Careers... ‚úì (71,965B / 842B)\n",
      "   üìÑ Blog... ‚úì (159,184B / 170B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 1 posts\n",
      "      ‚úì 1 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (45,309B / 220B)\n",
      "   üìÑ Investors... ‚úì (63,966B / 469B)\n",
      "   üìÑ Customers... ‚úì (63,966B / 469B)\n",
      "   üìÑ Press... ‚úì (66,285B / 690B)\n",
      "   üìÑ Pricing... ‚úì (153,427B / 1,656B)\n",
      "   üìÑ Partners... ‚úì (63,960B / 469B)\n",
      "   üìÑ Contact... ‚úì (62,625B / 298B)\n",
      "   ‚úÖ 13 total pages/posts scraped\n",
      "[9/50] Cohere...\n",
      "\n",
      "======================================================================\n",
      "üîç Cohere (cohere)\n",
      "   https://cohere.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (896,006B / 990B)\n",
      "   üìÑ About... ‚úì (763,545B / 1,776B)\n",
      "   üìÑ Product... ‚úì (791,575B / 1,233B)\n",
      "   üìÑ Careers... ‚úì (691,890B / 2,117B)\n",
      "   üìÑ Blog... ‚úì (327,350B / 260B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (594,619B / 4,931B)\n",
      "   üìÑ Press... ‚úì (613,941B / 1,620B)\n",
      "   üìÑ Pricing... ‚úì (657,312B / 3,384B)\n",
      "   üìÑ Partners... ‚úì (823,338B / 2,867B)\n",
      "   üìÑ Contact... ‚úì (654,919B / 351B)\n",
      "   ‚úÖ 10 total pages/posts scraped\n",
      "[10/50] Crusoe...\n",
      "\n",
      "======================================================================\n",
      "üîç Crusoe (crusoe)\n",
      "   https://www.crusoe.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (366,036B / 2,076B)\n",
      "   üìÑ About... ‚úì (159,563B / 1,953B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (145,741B / 1,628B)\n",
      "   üìÑ Blog... ‚úì (162,117B / 796B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 12 posts\n",
      "      ... 5/12 blog posts scraped\n",
      "      ... 10/12 blog posts scraped\n",
      "      ‚úì 12 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (190,031B / 1,809B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (165,198B / 523B)\n",
      "   üìÑ Press... ‚úì (162,168B / 283B)\n",
      "   üìÑ Pricing... ‚úì (160,658B / 3,487B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (122,641B / 811B)\n",
      "   ‚úÖ 21 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 10/50 companies | 10 successful | 7.8m elapsed | ~31.0m remaining\n",
      "\n",
      "[11/50] Databricks...\n",
      "\n",
      "======================================================================\n",
      "üîç Databricks (databricks)\n",
      "   https://www.databricks.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (780,117B / 2,225B)\n",
      "   üìÑ About... ‚úì (730,173B / 1,397B)\n",
      "   üìÑ Product... ‚úì (759,417B / 1,692B)\n",
      "   üìÑ Careers... ‚úì (750,057B / 1,984B)\n",
      "   üìÑ Blog... ‚úì (731,088B / 631B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (716,973B / 3,248B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (694,365B / 555B)\n",
      "   üìÑ Press... ‚úì (693,606B / 647B)\n",
      "   üìÑ Pricing... ‚úì (698,185B / 1,735B)\n",
      "   üìÑ Partners... ‚úì (687,181B / 305B)\n",
      "   üìÑ Contact... ‚úì (719,606B / 462B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[12/50] Decagon...\n",
      "\n",
      "======================================================================\n",
      "üîç Decagon (decagon)\n",
      "   https://www.decagon.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (255,002B / 1,767B)\n",
      "   üìÑ About... ‚úì (169,749B / 411B)\n",
      "   üìÑ Product... ‚úì (211,005B / 1,810B)\n",
      "   üìÑ Careers... ‚úì (166,526B / 1,955B)\n",
      "   üìÑ Blog... ‚úì (169,945B / 7,605B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (154,881B / 920B)\n",
      "   üìÑ Press... ‚úì (157,803B / 2,472B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... ‚úì (185,727B / 3,227B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 8 total pages/posts scraped\n",
      "[13/50] DeepL...\n",
      "\n",
      "======================================================================\n",
      "üîç DeepL (deepl)\n",
      "   https://www.deepl.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (206,142B / 398B)\n",
      "   üìÑ About... ‚úì (101,417B / 287B)\n",
      "   üìÑ Product... ‚úì (101,419B / 287B)\n",
      "   üìÑ Careers... ‚úì (104,370B / 287B)\n",
      "   üìÑ Blog... ‚úì (129,844B / 2,592B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 11 posts\n",
      "      ... 5/11 blog posts scraped\n",
      "      ... 10/11 blog posts scraped\n",
      "      ‚úì 11 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (101,723B / 287B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (161,975B / 1,691B)\n",
      "   üìÑ Press... ‚úì (101,154B / 287B)\n",
      "   üìÑ Pricing... ‚úì (101,074B / 287B)\n",
      "   üìÑ Partners... ‚úì (101,635B / 287B)\n",
      "   üìÑ Contact... ‚úì (104,570B / 489B)\n",
      "   ‚úÖ 21 total pages/posts scraped\n",
      "[14/50] ElevenLabs...\n",
      "\n",
      "======================================================================\n",
      "üîç ElevenLabs (elevenlabs)\n",
      "   https://elevenlabs.io\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (1,231,240B / 1,048B)\n",
      "   üìÑ About... ‚úì (911,818B / 1,175B)\n",
      "   üìÑ Product... ‚úì (1,013,015B / 3,045B)\n",
      "   üìÑ Careers... ‚úì (1,013,638B / 3,166B)\n",
      "   üìÑ Blog... ‚úì (922,708B / 1,387B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 11 posts\n",
      "      ... 5/11 blog posts scraped\n",
      "      ... 10/11 blog posts scraped\n",
      "      ‚úì 11 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (872,166B / 919B)\n",
      "   üìÑ Investors... ‚úì (905,638B / 6,356B)\n",
      "   üìÑ Customers... ‚úì (922,708B / 1,387B)\n",
      "   üìÑ Press... ‚úì (801,370B / 1,528B)\n",
      "   üìÑ Pricing... ‚úì (898,975B / 2,354B)\n",
      "   üìÑ Partners... ‚úì (842,664B / 2,176B)\n",
      "   üìÑ Contact... ‚úì (665,179B / 249B)\n",
      "   ‚úÖ 23 total pages/posts scraped\n",
      "[15/50] Figure AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Figure AI (figure)\n",
      "   https://www.figure.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (39,095B / 305B)\n",
      "   üìÑ About... ‚úì (55,892B / 2,008B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (38,817B / 554B)\n",
      "   üìÑ Blog... ‚úì (40,957B / 464B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 12 posts\n",
      "      ... 5/12 blog posts scraped\n",
      "      ... 10/12 blog posts scraped\n",
      "      ‚úì 12 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... ‚úì (40,953B / 464B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 17 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 15/50 companies | 15 successful | 11.6m elapsed | ~27.1m remaining\n",
      "\n",
      "[16/50] Fireworks AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Fireworks AI (fireworks)\n",
      "   https://fireworks.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (507,338B / 2,467B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... ‚úì (257,389B / 696B)\n",
      "   üìÑ Careers... ‚úì (166,885B / 258B)\n",
      "   üìÑ Blog... ‚úì (891,768B / 8,915B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (201,031B / 176B)\n",
      "   üìÑ Investors... ‚úì (201,031B / 176B)\n",
      "   üìÑ Customers... ‚úì (271,422B / 3,691B)\n",
      "   üìÑ Press... ‚úì (143,656B / 439B)\n",
      "   üìÑ Pricing... ‚úì (232,065B / 2,899B)\n",
      "   üìÑ Partners... ‚úì (190,760B / 500B)\n",
      "   üìÑ Contact... ‚úì (141,532B / 342B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[17/50] Glean...\n",
      "\n",
      "======================================================================\n",
      "üîç Glean (glean)\n",
      "   https://www.glean.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (345,808B / 1,986B)\n",
      "   üìÑ About... ‚úì (238,143B / 11,382B)\n",
      "   üìÑ Product... ‚úì (345,808B / 1,986B)\n",
      "   üìÑ Careers... ‚úì (150,604B / 1,727B)\n",
      "   üìÑ Blog... ‚úì (444,759B / 14,343B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (177,934B / 1,186B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (205,670B / 1,483B)\n",
      "   üìÑ Press... ‚úì (208,200B / 1,379B)\n",
      "   üìÑ Pricing... ‚úì (345,808B / 1,986B)\n",
      "   üìÑ Partners... ‚úì (173,266B / 1,224B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 30 total pages/posts scraped\n",
      "[18/50] Harvey...\n",
      "\n",
      "======================================================================\n",
      "üîç Harvey (harvey)\n",
      "   https://www.harvey.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (448,094B / 1,587B)\n",
      "   üìÑ About... ‚úì (91,163B / 1,816B)\n",
      "   üìÑ Product... ‚úì (91,169B / 1,816B)\n",
      "   üìÑ Careers... ‚úì (1,647,314B / 692B)\n",
      "   üìÑ Blog... ‚úì (219,933B / 565B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 14 posts\n",
      "      ... 5/14 blog posts scraped\n",
      "      ... 10/14 blog posts scraped\n",
      "      ‚úì 14 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (91,160B / 1,816B)\n",
      "   üìÑ Investors... ‚úì (91,175B / 1,816B)\n",
      "   üìÑ Customers... ‚úì (600,935B / 269B)\n",
      "   üìÑ Press... ‚úì (107,803B / 1,600B)\n",
      "   üìÑ Pricing... ‚úì (91,169B / 1,816B)\n",
      "   üìÑ Partners... ‚úì (91,172B / 1,816B)\n",
      "   üìÑ Contact... ‚úì (91,169B / 1,816B)\n",
      "   ‚úÖ 26 total pages/posts scraped\n",
      "[19/50] Hebbia...\n",
      "\n",
      "======================================================================\n",
      "üîç Hebbia (hebbia)\n",
      "   https://www.hebbia.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (187,479B / 3,643B)\n",
      "   üìÑ About... ‚úì (119,959B / 916B)\n",
      "   üìÑ Product... ‚úì (187,479B / 3,643B)\n",
      "   üìÑ Careers... ‚úì (344,276B / 846B)\n",
      "   üìÑ Blog... ‚úì (205,383B / 208B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 1 posts\n",
      "      ‚úì 1 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... ‚úì (73,104B / 2,705B)\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... ‚úì (71,941B / 175B)\n",
      "   üìÑ Pricing... ‚úì (65,788B / 921B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 9 total pages/posts scraped\n",
      "[20/50] Hugging Face...\n",
      "\n",
      "======================================================================\n",
      "üîç Hugging Face (huggingface)\n",
      "   https://huggingface.co\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (134,200B / 2,252B)\n",
      "   üìÑ About... ‚úì (40,822B / 320B)\n",
      "   üìÑ Product... ‚úì (38,379B / 214B)\n",
      "   üìÑ Careers... ‚úì (5,717B / 31B)\n",
      "   üìÑ Blog... ‚úì (257,193B / 1,273B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (38,305B / 207B)\n",
      "   üìÑ Investors... ‚úì (38,323B / 207B)\n",
      "   üìÑ Customers... ‚úì (52,530B / 297B)\n",
      "   üìÑ Press... ‚úì (38,271B / 202B)\n",
      "   üìÑ Pricing... ‚úì (103,789B / 6,337B)\n",
      "   üìÑ Partners... ‚úì (41,513B / 248B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 20/50 companies | 20 successful | 16.4m elapsed | ~24.7m remaining\n",
      "\n",
      "[21/50] Lambda...\n",
      "\n",
      "======================================================================\n",
      "üîç Lambda (lambdalabs)\n",
      "   https://lambdalabs.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (72,369B / 1,270B)\n",
      "   üìÑ About... ‚úì (66,406B / 1,154B)\n",
      "   üìÑ Product... ‚úì (140,085B / 5,583B)\n",
      "   üìÑ Careers... ‚úì (112,073B / 9,252B)\n",
      "   üìÑ Blog... ‚úì (82,722B / 1,304B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 2 posts\n",
      "      ‚úì 2 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (63,654B / 577B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úó Not found\n",
      "   üìÑ Partners... ‚úì (65,257B / 1,291B)\n",
      "   üìÑ Contact... ‚úì (56,422B / 212B)\n",
      "   ‚úÖ 10 total pages/posts scraped\n",
      "[22/50] LangChain...\n",
      "\n",
      "======================================================================\n",
      "üîç LangChain (langchain)\n",
      "   https://www.langchain.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (87,606B / 3,370B)\n",
      "   üìÑ About... ‚úì (40,086B / 1,016B)\n",
      "   üìÑ Product... ‚úì (78,211B / 543B)\n",
      "   üìÑ Careers... ‚úì (44,657B / 958B)\n",
      "   üìÑ Blog... ‚úì (57,652B / 777B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (87,568B / 326B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (126,783B / 5,108B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (44,939B / 655B)\n",
      "   ‚úÖ 8 total pages/posts scraped\n",
      "[23/50] Luminance...\n",
      "\n",
      "======================================================================\n",
      "üîç Luminance (luminance)\n",
      "   https://www.luminance.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (149,415B / 2,285B)\n",
      "   üìÑ About... ‚úì (130,839B / 1,110B)\n",
      "   üìÑ Product... ‚úì (69,468B / 2,845B)\n",
      "   üìÑ Careers... ‚úì (50,505B / 482B)\n",
      "   üìÑ Blog... ‚úì (118,884B / 314B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 13 posts\n",
      "      ... 5/13 blog posts scraped\n",
      "      ... 10/13 blog posts scraped\n",
      "      ‚úì 13 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (94,173B / 1,182B)\n",
      "   üìÑ Press... ‚úì (129,907B / 435B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... ‚úì (74,534B / 2,069B)\n",
      "   üìÑ Contact... ‚úì (73,972B / 897B)\n",
      "   ‚úÖ 22 total pages/posts scraped\n",
      "[24/50] Mercor...\n",
      "\n",
      "======================================================================\n",
      "üîç Mercor (mercor)\n",
      "   https://www.mercor.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (65,903B / 1,414B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... ‚úì (43,027B / 3,172B)\n",
      "   üìÑ Careers... ‚úì (3,234B / 24B)\n",
      "   üìÑ Blog... ‚úì (29,409B / 2,882B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 11 posts\n",
      "      ... 5/11 blog posts scraped\n",
      "      ... 10/11 blog posts scraped\n",
      "      ‚úì 11 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (4,395B / 24B)\n",
      "   ‚úÖ 16 total pages/posts scraped\n",
      "[25/50] Midjourney...\n",
      "\n",
      "======================================================================\n",
      "üîç Midjourney (midjourney)\n",
      "   https://www.midjourney.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage...       ‚Üí Fallback to Playwright\n",
      "‚úó Playwright init failed: It looks like you are using Playwright Sync API in\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 0 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 25/50 companies | 24 successful | 18.8m elapsed | ~18.8m remaining\n",
      "\n",
      "[26/50] Mistral AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Mistral AI (mistral)\n",
      "   https://mistral.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (346,382B / 4,907B)\n",
      "   üìÑ About... ‚úì (250,243B / 958B)\n",
      "   üìÑ Product... ‚úì (452,549B / 940B)\n",
      "   üìÑ Careers... ‚úì (403,728B / 1,932B)\n",
      "   üìÑ Blog... ‚úì (162,947B / 301B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 1 posts\n",
      "      ‚úì 1 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (276,006B / 299B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (399,072B / 269B)\n",
      "   üìÑ Partners... ‚úì (330,843B / 461B)\n",
      "   üìÑ Contact... ‚úì (151,191B / 344B)\n",
      "   ‚úÖ 10 total pages/posts scraped\n",
      "[27/50] Notion...\n",
      "\n",
      "======================================================================\n",
      "üîç Notion (notion)\n",
      "   https://www.notion.so\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (171,036B / 571B)\n",
      "   üìÑ About... ‚úì (87,991B / 3,012B)\n",
      "   üìÑ Product... ‚úì (171,036B / 571B)\n",
      "   üìÑ Careers... ‚úì (202,380B / 15,521B)\n",
      "   üìÑ Blog... ‚úì (167,675B / 296B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 10 posts\n",
      "      ... 5/10 blog posts scraped\n",
      "      ... 10/10 blog posts scraped\n",
      "      ‚úì 10 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (210,704B / 1,808B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (494,221B / 2,946B)\n",
      "   üìÑ Press... ‚úì (116,684B / 427B)\n",
      "   üìÑ Pricing... ‚úì (416,859B / 8,923B)\n",
      "   üìÑ Partners... ‚úì (138,159B / 267B)\n",
      "   üìÑ Contact... ‚úì (156,141B / 929B)\n",
      "   ‚úÖ 21 total pages/posts scraped\n",
      "[28/50] OpenAI...\n",
      "\n",
      "======================================================================\n",
      "üîç OpenAI (openai)\n",
      "   https://openai.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage...       ‚Üí Fallback to Playwright\n",
      "‚úó Playwright init failed: It looks like you are using Playwright Sync API in\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 0 total pages/posts scraped\n",
      "[29/50] OpenEvidence...\n",
      "\n",
      "======================================================================\n",
      "üîç OpenEvidence (openevidence)\n",
      "   https://www.openevidence.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (274,277B / 564B)\n",
      "   üìÑ About... ‚úì (259,944B / 4,864B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... ‚úì (135,196B / 159B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 3 total pages/posts scraped\n",
      "[30/50] Perplexity AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Perplexity AI (perplexity)\n",
      "   https://www.perplexity.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage...       ‚Üí Fallback to Playwright\n",
      "‚úó Playwright init failed: It looks like you are using Playwright Sync API in\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 0 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 30/50 companies | 27 successful | 20.5m elapsed | ~13.7m remaining\n",
      "\n",
      "[31/50] Photoroom...\n",
      "\n",
      "======================================================================\n",
      "üîç Photoroom (photoroom)\n",
      "   https://www.photoroom.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (361,926B / 1,581B)\n",
      "   üìÑ About... ‚úì (243,788B / 1,606B)\n",
      "   üìÑ Product... ‚úì (223,733B / 453B)\n",
      "   üìÑ Careers... ‚úì (41,955B / 61B)\n",
      "   üìÑ Blog... ‚úì (630,434B / 254B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (298,093B / 1,148B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (356,156B / 265B)\n",
      "   üìÑ Press... ‚úì (313,123B / 1,236B)\n",
      "   üìÑ Pricing... ‚úì (367,583B / 1,514B)\n",
      "   üìÑ Partners... ‚úì (196,771B / 265B)\n",
      "   üìÑ Contact... ‚úì (201,724B / 662B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[32/50] Pika...\n",
      "\n",
      "======================================================================\n",
      "üîç Pika (pika)\n",
      "   https://pika.art\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (48,338B / 442B)\n",
      "   üìÑ About... ‚úì (48,338B / 442B)\n",
      "   üìÑ Product... ‚úì (48,338B / 442B)\n",
      "   üìÑ Careers... ‚úì (54,468B / 723B)\n",
      "   üìÑ Blog... ‚úì (59,899B / 1,652B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... ‚úì (48,338B / 442B)\n",
      "   üìÑ Investors... ‚úì (48,338B / 442B)\n",
      "   üìÑ Customers... ‚úì (48,338B / 442B)\n",
      "   üìÑ Press... ‚úì (48,338B / 442B)\n",
      "   üìÑ Pricing... ‚úì (103,128B / 6,146B)\n",
      "   üìÑ Partners... ‚úì (48,338B / 442B)\n",
      "   üìÑ Contact... ‚úì (48,338B / 442B)\n",
      "   ‚úÖ 12 total pages/posts scraped\n",
      "[33/50] Runway...\n",
      "\n",
      "======================================================================\n",
      "üîç Runway (runwayml)\n",
      "   https://runwayml.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (828,133B / 1,660B)\n",
      "   üìÑ About... ‚úì (228,780B / 1,073B)\n",
      "   üìÑ Product... ‚úì (870,559B / 3,251B)\n",
      "   üìÑ Careers... ‚úì (571,726B / 2,852B)\n",
      "   üìÑ Blog... ‚úì (71,758B / 2,485B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (63,950B / 5,086B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (68,741B / 2,478B)\n",
      "   üìÑ Press... ‚úì (68,633B / 1,599B)\n",
      "   üìÑ Pricing... ‚úì (71,493B / 2,548B)\n",
      "   üìÑ Partners... ‚úì (43,763B / 2,882B)\n",
      "   üìÑ Contact... ‚úì (36,411B / 686B)\n",
      "   ‚úÖ 31 total pages/posts scraped\n",
      "[34/50] Sakana AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Sakana AI (sakana)\n",
      "   https://sakana.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (6,694B / 251B)\n",
      "   üìÑ About... ‚úì (5,540B / 1,677B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (33,792B / 16,672B)\n",
      "   üìÑ Blog... ‚úì (34,441B / 3,268B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 4 total pages/posts scraped\n",
      "[35/50] SambaNova...\n",
      "\n",
      "======================================================================\n",
      "üîç SambaNova (sambanova)\n",
      "   https://sambanova.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (203,510B / 4,958B)\n",
      "   üìÑ About... ‚úì (128,937B / 1,138B)\n",
      "   üìÑ Product... ‚úì (111,816B / 1,423B)\n",
      "   üìÑ Careers... ‚úì (104,039B / 2,163B)\n",
      "   üìÑ Blog... ‚úì (149,201B / 3,306B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (261,079B / 901B)\n",
      "   üìÑ Investors... ‚úì (128,937B / 1,138B)\n",
      "   üìÑ Customers... ‚úì (108,532B / 1,054B)\n",
      "   üìÑ Press... ‚úì (94,767B / 1,716B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (93,647B / 301B)\n",
      "   ‚úÖ 30 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 35/50 companies | 32 successful | 24.7m elapsed | ~10.6m remaining\n",
      "\n",
      "[36/50] Scale AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Scale AI (scale)\n",
      "   https://scale.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (902,187B / 923B)\n",
      "   üìÑ About... ‚úì (736,160B / 749B)\n",
      "   üìÑ Product... ‚úì (686,674B / 932B)\n",
      "   üìÑ Careers... ‚úì (724,944B / 3,657B)\n",
      "   üìÑ Blog... ‚úì (689,093B / 155B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (445,416B / 822B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (846,796B / 155B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (734,913B / 762B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (723,121B / 334B)\n",
      "   ‚úÖ 29 total pages/posts scraped\n",
      "[37/50] Sierra...\n",
      "\n",
      "======================================================================\n",
      "üîç Sierra (sierra)\n",
      "   https://sierra.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (334,509B / 2,954B)\n",
      "   üìÑ About... ‚úì (249,917B / 1,393B)\n",
      "   üìÑ Product... ‚úì (219,137B / 1,464B)\n",
      "   üìÑ Careers... ‚úì (1,796,515B / 3,653B)\n",
      "   üìÑ Blog... ‚úì (236,158B / 297B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 9 posts\n",
      "      ... 5/9 blog posts scraped\n",
      "      ‚úì 9 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (355,443B / 254B)\n",
      "   üìÑ Press... ‚úì (219,431B / 1,881B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 16 total pages/posts scraped\n",
      "[38/50] Skild AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Skild AI (skild)\n",
      "   https://www.skild.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (23,247B / 1,015B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (13,158B / 158B)\n",
      "   üìÑ Blog... ‚úì (19,245B / 128B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 3 total pages/posts scraped\n",
      "[39/50] Snorkel AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Snorkel AI (snorkel)\n",
      "   https://snorkel.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (274,074B / 2,420B)\n",
      "   üìÑ About... ‚úì (240,123B / 1,519B)\n",
      "   üìÑ Product... ‚úì (266,652B / 1,678B)\n",
      "   üìÑ Careers... ‚úì (218,273B / 2,219B)\n",
      "   üìÑ Blog... ‚úì (361,732B / 7,350B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (240,123B / 1,519B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (230,117B / 630B)\n",
      "   üìÑ Press... ‚úì (170,819B / 329B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... ‚úì (244,773B / 1,383B)\n",
      "   üìÑ Contact... ‚úì (239,335B / 762B)\n",
      "   ‚úÖ 30 total pages/posts scraped\n",
      "[40/50] Speak...\n",
      "\n",
      "======================================================================\n",
      "üîç Speak (speak)\n",
      "   https://www.speak.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (86,851B / 1,199B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (89,842B / 3,021B)\n",
      "   üìÑ Blog... ‚úì (61,546B / 171B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 12 posts\n",
      "      ... 5/12 blog posts scraped\n",
      "      ... 10/12 blog posts scraped\n",
      "      ‚úì 12 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... ‚úó Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úó Not found\n",
      "   ‚úÖ 15 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 40/50 companies | 37 successful | 29.5m elapsed | ~7.4m remaining\n",
      "\n",
      "[41/50] StackBlitz...\n",
      "\n",
      "======================================================================\n",
      "üîç StackBlitz (stackblitz)\n",
      "   https://stackblitz.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (35,427B / 885B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (122,434B / 1,600B)\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (21,421B / 327B)\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (24,473B / 1,863B)\n",
      "   üìÑ Partners... ‚úì (26,275B / 613B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 5 total pages/posts scraped\n",
      "[42/50] Suno...\n",
      "\n",
      "======================================================================\n",
      "üîç Suno (suno)\n",
      "   https://suno.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (80,572B / 23B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (70,513B / 56B)\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... ‚úì (80,887B / 279B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 3 total pages/posts scraped\n",
      "[43/50] Synthesia...\n",
      "\n",
      "======================================================================\n",
      "üîç Synthesia (synthesia)\n",
      "   https://www.synthesia.io\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (581,429B / 4,788B)\n",
      "   üìÑ About... ‚úì (283,894B / 1,106B)\n",
      "   üìÑ Product... ‚úì (412,561B / 2,646B)\n",
      "   üìÑ Careers... ‚úì (328,524B / 1,027B)\n",
      "   üìÑ Blog... ‚úì (327,753B / 490B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... ‚úì (370,934B / 2,964B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (682,730B / 2,158B)\n",
      "   üìÑ Press... ‚úì (304,788B / 495B)\n",
      "   üìÑ Pricing... ‚úì (839,675B / 14,180B)\n",
      "   üìÑ Partners... ‚úì (277,873B / 406B)\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 10 total pages/posts scraped\n",
      "[44/50] Thinking Machine Labs...\n",
      "\n",
      "======================================================================\n",
      "üîç Thinking Machine Labs (thinkingmachines)\n",
      "   https://thinkingmachines.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (16,100B / 4,594B)\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (16,100B / 4,594B)\n",
      "   üìÑ Blog... ‚úì (13,489B / 620B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 6 posts\n",
      "      ... 5/6 blog posts scraped\n",
      "      ‚úì 6 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 9 total pages/posts scraped\n",
      "[45/50] Together AI...\n",
      "\n",
      "======================================================================\n",
      "üîç Together AI (together)\n",
      "   https://www.together.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (444,690B / 3,335B)\n",
      "   üìÑ About... ‚úì (406,505B / 2,536B)\n",
      "   üìÑ Product... ‚úì (244,610B / 1,092B)\n",
      "   üìÑ Careers... ‚úì (406,505B / 2,536B)\n",
      "   üìÑ Blog... ‚úì (254,796B / 257B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 15 posts\n",
      "      ... 5/15 blog posts scraped\n",
      "      ... 10/15 blog posts scraped\n",
      "      ... 15/15 blog posts scraped\n",
      "      ‚úì 15 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (406,505B / 2,536B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (231,595B / 2,393B)\n",
      "   üìÑ Press... ‚úì (254,796B / 257B)\n",
      "   üìÑ Pricing... ‚úì (360,290B / 2,118B)\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... ‚úì (209,373B / 2,361B)\n",
      "   ‚úÖ 25 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 45/50 companies | 42 successful | 31.4m elapsed | ~3.5m remaining\n",
      "\n",
      "[46/50] Vannevar Labs...\n",
      "\n",
      "======================================================================\n",
      "üîç Vannevar Labs (vannevarlabs)\n",
      "   https://www.vannevarlabs.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (14,056B / 2,260B)\n",
      "   üìÑ About... ‚úì (14,178B / 1,781B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... ‚úì (11,152B / 1,722B)\n",
      "   üìÑ Blog... ‚úì (17,762B / 375B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 10 posts\n",
      "      ... 5/10 blog posts scraped\n",
      "      ... 10/10 blog posts scraped\n",
      "      ‚úì 10 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (10,099B / 912B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... ‚úì (15,965B / 424B)\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 16 total pages/posts scraped\n",
      "[47/50] VAST Data...\n",
      "\n",
      "======================================================================\n",
      "üîç VAST Data (vastdata)\n",
      "   https://www.vastdata.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (348,128B / 607B)\n",
      "   üìÑ About... ‚úì (379,940B / 1,001B)\n",
      "   üìÑ Product... ‚úì (815,783B / 9B)\n",
      "   üìÑ Careers... ‚úì (1,039,762B / 2,593B)\n",
      "   üìÑ Blog... ‚úì (172,489B / 457B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 3 posts\n",
      "      ‚úì 3 blog posts saved to blog_posts/\n",
      "   üìÑ Team... ‚úì (815,756B / 9B)\n",
      "   üìÑ Investors... ‚úì (815,801B / 9B)\n",
      "   üìÑ Customers... ‚úì (322,850B / 1,005B)\n",
      "   üìÑ Press... ‚úì (815,765B / 9B)\n",
      "   üìÑ Pricing... ‚úì (816,096B / 9B)\n",
      "   üìÑ Partners... ‚úì (292,175B / 7,968B)\n",
      "   üìÑ Contact... ‚úì (951,069B / 745B)\n",
      "   ‚úÖ 15 total pages/posts scraped\n",
      "[48/50] World Labs...\n",
      "\n",
      "======================================================================\n",
      "üîç World Labs (worldlabs)\n",
      "   https://www.worldlabs.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (12,996B / 10B)\n",
      "   üìÑ About... ‚úì (20,675B / 17B)\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... ‚úì (14,282B / 17B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 0 posts\n",
      "   üìÑ Team... ‚úì (17,717B / 15B)\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 4 total pages/posts scraped\n",
      "[49/50] Writer...\n",
      "\n",
      "======================================================================\n",
      "üîç Writer (writer)\n",
      "   https://writer.com\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage... ‚úì (282,435B / 2,654B)\n",
      "   üìÑ About... ‚úì (346,376B / 3,657B)\n",
      "   üìÑ Product... ‚úì (282,435B / 2,654B)\n",
      "   üìÑ Careers... ‚úì (211,050B / 1,726B)\n",
      "   üìÑ Blog... ‚úì (253,097B / 279B)\n",
      "\n",
      "      üîç Extracting blog posts... Found 20 posts\n",
      "      ... 5/20 blog posts scraped\n",
      "      ... 10/20 blog posts scraped\n",
      "      ... 15/20 blog posts scraped\n",
      "      ... 20/20 blog posts scraped\n",
      "      ‚úì 20 blog posts saved to blog_posts/\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... ‚úì (310,898B / 10,813B)\n",
      "   üìÑ Press... ‚úì (799B / 0B)\n",
      "   üìÑ Pricing... ‚úì (286,875B / 2,881B)\n",
      "   üìÑ Partners... ‚úì (206,305B / 1,757B)\n",
      "   üìÑ Contact... ‚úì (199,616B / 321B)\n",
      "   ‚úÖ 30 total pages/posts scraped\n",
      "[50/50] xAI...\n",
      "\n",
      "======================================================================\n",
      "üîç xAI (x)\n",
      "   https://x.ai\n",
      "   Mode: üöÄ HTTP + Playwright fallback\n",
      "======================================================================\n",
      "   üìÑ Homepage...       ‚Üí Fallback to Playwright\n",
      "‚úó Playwright init failed: It looks like you are using Playwright Sync API in\n",
      "   üìÑ About... Not found\n",
      "   üìÑ Product... Not found\n",
      "   üìÑ Careers... Not found\n",
      "   üìÑ Blog... Not found\n",
      "   üìÑ Team... Not found\n",
      "   üìÑ Investors... Not found\n",
      "   üìÑ Customers... Not found\n",
      "   üìÑ Press... Not found\n",
      "   üìÑ Pricing... Not found\n",
      "   üìÑ Partners... Not found\n",
      "   üìÑ Contact... Not found\n",
      "   ‚úÖ 0 total pages/posts scraped\n",
      "\n",
      "   ‚è±  Progress: 50/50 companies | 46 successful | 34.7m elapsed | ~0.0m remaining\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéâ DEEP SCRAPING COMPLETE - ALL 50 COMPANIES\n",
      "======================================================================\n",
      "‚úì Successful:            46/50 companies (92.0%)\n",
      "üö´ Blocked (robots):      0 companies\n",
      "‚úó Failed:                4 companies\n",
      "üìÑ Total pages/posts:    827\n",
      "üìä Average per company:  16.5 pages/posts\n",
      "‚è±  Total time:           34.7 minutes (41.6s per company)\n",
      "======================================================================\n",
      "\n",
      "‚úó Failed companies:\n",
      "   ‚Ä¢ Midjourney: Unknown error\n",
      "   ‚Ä¢ OpenAI: Unknown error\n",
      "   ‚Ä¢ Perplexity AI: Unknown error\n",
      "   ‚Ä¢ xAI: Unknown error\n"
     ]
    }
   ],
   "source": [
    "# Scrape ALL 50 Companies (Deep Scraping)\n",
    "# This will:\n",
    "# 1. Use HTTP requests (fast and reliable)\n",
    "# 2. Bypass robots.txt to get complete data\n",
    "# 3. Scrape 12 page types + up to 20 blog posts per company\n",
    "# 4. Extract structured data (HQ, team, investors, pricing, etc.)\n",
    "# 5. Save all data to data/raw/<company_id>/initial_pull/\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use ALL companies (not just test subset)\n",
    "companies_to_scrape = all_companies\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ DEEP SCRAPING ALL {len(companies_to_scrape)} FORBES AI50 COMPANIES\")\n",
    "print(f\"   Mode: üöÄ HTTP + Deep Scraping (bypassing robots.txt)\")\n",
    "print(f\"   Pages: 12 page types + up to 20 blog posts each\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for i, company in enumerate(companies_to_scrape, 1):\n",
    "    print(f\"[{i}/{len(companies_to_scrape)}] {company['company_name']}...\")\n",
    "    \n",
    "    try:\n",
    "        # Use HTTP (not Playwright) but don't respect robots.txt\n",
    "        result = scrape_company(company, force_playwright=False, respect_robots=False)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"company_name\": company['company_name'],\n",
    "            \"company_id\": company['company_id'],\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"pages_scraped\": 0,\n",
    "            \"pages_total\": 12\n",
    "        })\n",
    "    \n",
    "    # Progress update every 5 companies\n",
    "    if i % 5 == 0 or i == len(companies_to_scrape):\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time_per_company = elapsed / i\n",
    "        remaining = (len(companies_to_scrape) - i) * avg_time_per_company\n",
    "        successful_so_far = sum(1 for r in results if r.get('status') == 'success')\n",
    "        print(f\"\\n   ‚è±  Progress: {i}/{len(companies_to_scrape)} companies | {successful_so_far} successful | {elapsed/60:.1f}m elapsed | ~{remaining/60:.1f}m remaining\\n\")\n",
    "\n",
    "# Final Summary\n",
    "elapsed = time.time() - start_time\n",
    "successful = [r for r in results if r.get('status') == 'success']\n",
    "blocked = [r for r in results if r.get('status') == 'blocked_by_robots']\n",
    "failed = [r for r in results if r.get('status') not in ['success', 'blocked_by_robots']]\n",
    "total_pages = sum(r.get('pages_scraped', 0) for r in results)\n",
    "avg_pages = total_pages / len(companies_to_scrape) if companies_to_scrape else 0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üéâ DEEP SCRAPING COMPLETE - ALL {len(companies_to_scrape)} COMPANIES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úì Successful:            {len(successful)}/{len(companies_to_scrape)} companies ({len(successful)/len(companies_to_scrape)*100:.1f}%)\")\n",
    "print(f\"üö´ Blocked (robots):      {len(blocked)} companies\")\n",
    "print(f\"‚úó Failed:                {len(failed)} companies\")\n",
    "print(f\"üìÑ Total pages/posts:    {total_pages:,}\")\n",
    "print(f\"üìä Average per company:  {avg_pages:.1f} pages/posts\")\n",
    "print(f\"‚è±  Total time:           {elapsed/60:.1f} minutes ({elapsed/len(companies_to_scrape):.1f}s per company)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Show blocked companies (should be none)\n",
    "if blocked:\n",
    "    print(f\"\\nüö´ Still blocked:\")\n",
    "    for r in blocked:\n",
    "        print(f\"   ‚Ä¢ {r['company_name']}\")\n",
    "\n",
    "# Show failed companies\n",
    "if failed:\n",
    "    print(f\"\\n‚úó Failed companies:\")\n",
    "    for r in failed:\n",
    "        print(f\"   ‚Ä¢ {r['company_name']}: {r.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All companies scraped successfully!\")\n",
    "    print(f\"‚úÖ Expected: 20-40 pages per company (12 main + 10-20 blog posts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä VALIDATION REPORT - DEEP SCRAPING (ALL 50 COMPANIES)\n",
      "======================================================================\n",
      "\n",
      "‚úì Abridge                   - 10/12 pages | 4 blog posts | 6 structured\n",
      "‚úì Anthropic                 - 11/12 pages | 20 blog posts | 5 structured\n",
      "‚úì Anysphere                 - 9/12 pages | 10 blog posts | 5 structured\n",
      "‚úì Baseten                   - 10/12 pages | 9 blog posts | 6 structured\n",
      "‚ö† Captions                  - 7/12 pages | 0 blog posts | 4 structured\n",
      "‚úì Clay                      - 11/12 pages | 20 blog posts | 6 structured\n",
      "‚úì Coactive AI               - 9/12 pages | 18 blog posts | 4 structured\n",
      "‚úì Codeium                   - 12/12 pages | 1 blog posts | 6 structured\n",
      "‚úì Cohere                    - 10/12 pages | 0 blog posts | 6 structured\n",
      "‚úì Crusoe                    - 9/12 pages | 12 blog posts | 5 structured\n",
      "‚úì Databricks                - 11/12 pages | 20 blog posts | 6 structured\n",
      "‚úì Decagon                   - 8/12 pages | 0 blog posts | 3 structured\n",
      "‚úì DeepL                     - 11/12 pages | 10 blog posts | 6 structured\n",
      "‚úì ElevenLabs                - 12/12 pages | 11 blog posts | 8 structured\n",
      "‚ö† Figure AI                 - 5/12 pages | 12 blog posts | 2 structured\n",
      "‚úì Fireworks AI              - 11/12 pages | 20 blog posts | 5 structured\n",
      "‚úì Glean                     - 10/12 pages | 20 blog posts | 5 structured\n",
      "‚úì Harvey                    - 12/12 pages | 14 blog posts | 5 structured\n",
      "‚úì Hebbia                    - 8/12 pages | 1 blog posts | 3 structured\n",
      "‚úì Hugging Face              - 11/12 pages | 20 blog posts | 5 structured\n",
      "‚úì Lambda                    - 8/12 pages | 2 blog posts | 5 structured\n",
      "‚úì LangChain                 - 8/12 pages | 0 blog posts | 5 structured\n",
      "‚úì Luminance                 - 9/12 pages | 13 blog posts | 5 structured\n",
      "‚ö† Mercor                    - 5/12 pages | 11 blog posts | 2 structured\n",
      "‚úó Midjourney                - 0/12 pages | 0 blog posts | 0 structured\n",
      "‚úì Mistral AI                - 9/12 pages | 1 blog posts | 6 structured\n",
      "‚úì Notion                    - 11/12 pages | 10 blog posts | 6 structured\n",
      "‚úó OpenAI                    - 0/12 pages | 0 blog posts | 0 structured\n",
      "‚ö† OpenEvidence              - 3/12 pages | 0 blog posts | 1 structured\n",
      "‚úó Perplexity AI             - 0/12 pages | 0 blog posts | 0 structured\n",
      "‚úì Photoroom                 - 11/12 pages | 20 blog posts | 6 structured\n",
      "‚úì Pika                      - 12/12 pages | 0 blog posts | 4 structured\n",
      "‚úì Runway                    - 11/12 pages | 20 blog posts | 6 structured\n",
      "‚ö† Sakana AI                 - 4/12 pages | 0 blog posts | 2 structured\n",
      "‚úì SambaNova                 - 10/12 pages | 20 blog posts | 3 structured\n",
      "‚úì Scale AI                  - 9/12 pages | 20 blog posts | 5 structured\n",
      "‚ö† Sierra                    - 7/12 pages | 9 blog posts | 3 structured\n",
      "‚ö† Skild AI                  - 3/12 pages | 0 blog posts | 1 structured\n",
      "‚úì Snorkel AI                - 10/12 pages | 20 blog posts | 5 structured\n",
      "‚ö† Speak                     - 3/12 pages | 12 blog posts | 1 structured\n",
      "‚ö† StackBlitz                - 5/12 pages | 0 blog posts | 4 structured\n",
      "‚ö† Suno                      - 3/12 pages | 0 blog posts | 2 structured\n",
      "‚úì Synthesia                 - 10/12 pages | 0 blog posts | 5 structured\n",
      "‚ö† Thinking Machine Labs     - 3/12 pages | 6 blog posts | 1 structured\n",
      "‚úì Together AI               - 10/12 pages | 15 blog posts | 6 structured\n",
      "‚ö† Vannevar Labs             - 6/12 pages | 10 blog posts | 2 structured\n",
      "‚úì VAST Data                 - 12/12 pages | 3 blog posts | 6 structured\n",
      "‚ö† World Labs                - 4/12 pages | 0 blog posts | 1 structured\n",
      "‚úì Writer                    - 10/12 pages | 20 blog posts | 6 structured\n",
      "‚úó xAI                       - 0/12 pages | 0 blog posts | 0 structured\n",
      "\n",
      "======================================================================\n",
      "üìà SUMMARY - ALL 50 COMPANIES\n",
      "======================================================================\n",
      "Companies with data:     46/50 (92.0%)\n",
      "Total pages scraped:     393/600 (expected)\n",
      "Total blog posts:        434\n",
      "Total structured files:  200\n",
      "Total items (all):       827\n",
      "Average pages/company:   7.9\n",
      "Average items/company:   16.5\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Check data/raw/<company_id>/initial_pull/ for:\n",
      "   - HTML & clean text files for 12 page types\n",
      "   - *_structured.json files for parsed data\n",
      "   - blog_posts/ subfolder with individual posts\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Validation Report (Deep Scraping - ALL Companies)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä VALIDATION REPORT - DEEP SCRAPING (ALL {len(all_companies)} COMPANIES)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for company in all_companies:\n",
    "    company_id = company['company_id']\n",
    "    company_name = company['company_name']\n",
    "    company_dir = OUTPUT_DIR / company_id / \"initial_pull\"\n",
    "    \n",
    "    if not company_dir.exists():\n",
    "        print(f\"‚úó {company_name:25} - Folder missing\")\n",
    "        validation_results.append({\n",
    "            \"company\": company_name, \n",
    "            \"status\": \"missing\", \n",
    "            \"pages\": 0,\n",
    "            \"structured_files\": 0,\n",
    "            \"blog_posts\": 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    metadata_path = company_dir / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        metadata = json.loads(metadata_path.read_text())\n",
    "        pages_found = sum(1 for p in metadata['pages'] if p['found'])\n",
    "        \n",
    "        # Count structured JSON files\n",
    "        structured_files = len(list(company_dir.glob(\"*_structured.json\")))\n",
    "        \n",
    "        # Count blog posts\n",
    "        blog_posts_dir = company_dir / \"blog_posts\"\n",
    "        blog_posts_count = 0\n",
    "        if blog_posts_dir.exists():\n",
    "            blog_posts_count = len(list(blog_posts_dir.glob(\"post_*.html\")))\n",
    "        \n",
    "        total_items = pages_found + blog_posts_count\n",
    "        status_symbol = '‚úì' if pages_found >= 8 else '‚ö†' if pages_found > 0 else '‚úó'\n",
    "        \n",
    "        print(f\"{status_symbol} {company_name:25} - {pages_found}/12 pages | {blog_posts_count} blog posts | {structured_files} structured\")\n",
    "        \n",
    "        validation_results.append({\n",
    "            \"company\": company_name, \n",
    "            \"status\": \"ok\", \n",
    "            \"pages\": pages_found,\n",
    "            \"structured_files\": structured_files,\n",
    "            \"blog_posts\": blog_posts_count,\n",
    "            \"total_items\": total_items\n",
    "        })\n",
    "    else:\n",
    "        print(f\"‚ö† {company_name:25} - No metadata\")\n",
    "        validation_results.append({\n",
    "            \"company\": company_name, \n",
    "            \"status\": \"no_metadata\", \n",
    "            \"pages\": 0,\n",
    "            \"structured_files\": 0,\n",
    "            \"blog_posts\": 0\n",
    "        })\n",
    "\n",
    "# Summary stats\n",
    "total_with_data = sum(1 for v in validation_results if v['pages'] > 0)\n",
    "total_pages_all = sum(v['pages'] for v in validation_results)\n",
    "total_blog_posts = sum(v.get('blog_posts', 0) for v in validation_results)\n",
    "total_structured = sum(v.get('structured_files', 0) for v in validation_results)\n",
    "total_items_all = sum(v.get('total_items', v['pages']) for v in validation_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìà SUMMARY - ALL {len(all_companies)} COMPANIES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Companies with data:     {total_with_data}/{len(all_companies)} ({total_with_data/len(all_companies)*100:.1f}%)\")\n",
    "print(f\"Total pages scraped:     {total_pages_all:,}/{len(all_companies) * 12} (expected)\")\n",
    "print(f\"Total blog posts:        {total_blog_posts:,}\")\n",
    "print(f\"Total structured files:  {total_structured:,}\")\n",
    "print(f\"Total items (all):       {total_items_all:,}\")\n",
    "print(f\"Average pages/company:   {total_pages_all/len(all_companies):.1f}\")\n",
    "print(f\"Average items/company:   {total_items_all/len(all_companies):.1f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n‚úÖ Check data/raw/<company_id>/initial_pull/ for:\")\n",
    "print(f\"   - HTML & clean text files for 12 page types\")\n",
    "print(f\"   - *_structured.json files for parsed data\")\n",
    "print(f\"   - blog_posts/ subfolder with individual posts\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results exported to: /Users/RiyanshiKedia/Documents/GitHub/project_orbit/scraping_results_lab1_full.json\n",
      "\n",
      "üìä Quick Stats:\n",
      "   Success rate:        46/50 (92.0%)\n",
      "   Total pages/posts:   827\n",
      "   Average per company: 16.5\n",
      "\n",
      "üéØ Lab 1 Enhanced Checkpoint Complete!\n",
      "   ‚úì Deep scraper implemented (12 page types + blog posts)\n",
      "   ‚úì Structured data parsers (HQ, team, investors, pricing, etc.)\n",
      "   ‚úì Data stored in data/raw/<company_id>/initial_pull/\n",
      "   ‚úì Structured JSON files saved (*_structured.json)\n",
      "   ‚úì Blog posts saved in blog_posts/ subfolder\n",
      "   ‚úì Metadata JSON generated for each company\n",
      "   ‚úì Results summary exported\n",
      "\n",
      "üìÅ Data Structure:\n",
      "   data/raw/<company_id>/initial_pull/\n",
      "      ‚îú‚îÄ‚îÄ homepage.html / homepage_clean.txt\n",
      "      ‚îú‚îÄ‚îÄ about.html / about_clean.txt / about_structured.json\n",
      "      ‚îú‚îÄ‚îÄ team.html / team_clean.txt / team_structured.json\n",
      "      ‚îú‚îÄ‚îÄ investors.html / investors_clean.txt / investors_structured.json\n",
      "      ‚îú‚îÄ‚îÄ ... (12 page types total)\n",
      "      ‚îú‚îÄ‚îÄ blog_posts/\n",
      "      ‚îÇ   ‚îú‚îÄ‚îÄ post_abc123.html / post_abc123_clean.txt\n",
      "      ‚îÇ   ‚îî‚îÄ‚îÄ ... (up to 20 posts)\n",
      "      ‚îî‚îÄ‚îÄ metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Export Results Summary (Lab 1 Enhanced - Deep Scraping ALL Companies)\n",
    "\n",
    "# Calculate statistics\n",
    "total_pages_posts = sum(r.get('pages_scraped', 0) for r in results)\n",
    "avg_per_company = total_pages_posts / len(all_companies) if all_companies else 0\n",
    "\n",
    "# Save detailed results to JSON\n",
    "results_file = PROJECT_ROOT / \"scraping_results_lab1_full.json\"\n",
    "results_file.write_text(json.dumps({\n",
    "    \"scrape_date\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"scraper_version\": \"3.0-deep\",\n",
    "    \"total_companies\": len(all_companies),\n",
    "    \"successful\": len([r for r in results if r.get('status') == 'success']),\n",
    "    \"blocked_by_robots\": len([r for r in results if r.get('status') == 'blocked_by_robots']),\n",
    "    \"failed\": len([r for r in results if r.get('status') not in ['success', 'blocked_by_robots']]),\n",
    "    \"total_pages_posts\": total_pages_posts,\n",
    "    \"average_per_company\": round(avg_per_company, 1),\n",
    "    \"companies\": results\n",
    "}, indent=2), encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Results exported to: {results_file}\")\n",
    "print(f\"\\nüìä Quick Stats:\")\n",
    "print(f\"   Success rate:        {len([r for r in results if r.get('status') == 'success'])}/{len(all_companies)} ({len([r for r in results if r.get('status') == 'success'])/len(all_companies)*100:.1f}%)\")\n",
    "print(f\"   Total pages/posts:   {total_pages_posts:,}\")\n",
    "print(f\"   Average per company: {avg_per_company:.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ Lab 1 Enhanced Checkpoint Complete!\")\n",
    "print(f\"   ‚úì Deep scraper implemented (12 page types + blog posts)\")\n",
    "print(f\"   ‚úì Structured data parsers (HQ, team, investors, pricing, etc.)\")\n",
    "print(f\"   ‚úì Data stored in data/raw/<company_id>/initial_pull/\")\n",
    "print(f\"   ‚úì Structured JSON files saved (*_structured.json)\")\n",
    "print(f\"   ‚úì Blog posts saved in blog_posts/ subfolder\")\n",
    "print(f\"   ‚úì Metadata JSON generated for each company\")\n",
    "print(f\"   ‚úì Results summary exported\")\n",
    "print(f\"\\nüìÅ Data Structure:\")\n",
    "print(f\"   data/raw/<company_id>/initial_pull/\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ homepage.html / homepage_clean.txt\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ about.html / about_clean.txt / about_structured.json\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ team.html / team_clean.txt / team_structured.json\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ investors.html / investors_clean.txt / investors_structured.json\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ ... (12 page types total)\")\n",
    "print(f\"      ‚îú‚îÄ‚îÄ blog_posts/\")\n",
    "print(f\"      ‚îÇ   ‚îú‚îÄ‚îÄ post_abc123.html / post_abc123_clean.txt\")\n",
    "print(f\"      ‚îÇ   ‚îî‚îÄ‚îÄ ... (up to 20 posts)\")\n",
    "print(f\"      ‚îî‚îÄ‚îÄ metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
