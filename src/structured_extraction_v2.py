"""
Lab 5: Dynamic Structured Extraction with Pydantic + Instructor

âœ… Uses PYDANTIC models for type validation
âœ… Uses INSTRUCTOR for structured extraction with retry
âœ… Zero hardcoding - searches ALL sources
âœ… Zero hallucination - validates with Pydantic + post-processing

IMPROVEMENTS IN THIS VERSION:
âœ… is_website_section() - Filters fake products (website pages)
âœ… extract_founded_year_aggressive() - Better year detection
âœ… Unique event IDs with full dates (YYYY_MM_DD)
âœ… Stricter leadership cross-validation
âœ… Timeline-only event extraction
"""

import json
import os
import re
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime, date

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

import instructor
from openai import OpenAI
from bs4 import BeautifulSoup
from pydantic import ValidationError 

try:
    from models import (
        Company, Event, Snapshot, Product, Leadership, Visibility,
        NewsArticle, Provenance, Payload
    )
except ImportError:
    from src.models import (
        Company, Event, Snapshot, Product, Leadership, Visibility,
        NewsArticle, Provenance, Payload
    )
try:
    from google.cloud import storage
    try:
        from google.oauth2 import service_account
    except ImportError:
        service_account = None
    GCS_AVAILABLE = True
except ImportError:
    GCS_AVAILABLE = False
    storage = None
    service_account = None
    print("âš ï¸  Google Cloud Storage not available. Install with: pip install google-cloud-storage google-auth")

storage_client = None

def get_storage_client():
    """Get or create GCS storage client (similar to api.py)"""
    global storage_client
    
    if storage_client is not None:
        return storage_client
    
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    if not bucket_name:
        return None
    
    if not GCS_AVAILABLE:
        print("âš ï¸  GCS_BUCKET_NAME is set but google-cloud-storage is not installed")
        return None
    
    try:
        project_id = os.getenv("PROJECT_ID")
        PROJECT_ROOT = Path(__file__).parent.parent
        credentials_path = PROJECT_ROOT / "config" / "gcp.json"
        
        if credentials_path.exists():
            if service_account is None:
                print("âš ï¸  service_account not available, cannot use credentials file")
                return None
            credentials = service_account.Credentials.from_service_account_file(
                str(credentials_path)
            )
            storage_client = storage.Client(project=project_id, credentials=credentials)
            print(f"âœ… GCS client initialized with credentials from {credentials_path}")
        else:
            # Use Application Default Credentials (production/Cloud Run)
            storage_client = storage.Client(project=project_id)
            print("âœ… GCS client initialized with Application Default Credentials")
        
        return storage_client
    except Exception as e:
        print(f"âš ï¸  Failed to initialize GCS client: {e}")
        return None

# Initialize Instructor client and raw OpenAI client
api_key = os.getenv("OPENAI_API_KEY")
model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
client = instructor.from_openai(OpenAI(api_key=api_key))
openai_client = OpenAI(api_key=api_key)  # Raw client for JSON extraction
print(f"âœ… Instructor client initialized with model: {model_name}")
print(f"âœ… Using Pydantic models for validation")


# ============================================================================
# VALIDATION HELPERS
# ============================================================================

def is_placeholder_name(name: str) -> bool:
    """Check if name is a placeholder."""
    if not name:
        return True
    
    placeholder_names = {
        'john doe', 'jane doe', 'john smith', 'jane smith',
        'unknown', 'test user', 'example user', 'placeholder',
        'anonymous', 'unnamed', 'tbd', 'tba', 'n/a', 'na',
        'ceo', 'cto', 'cfo', 'coo', 'founder', 'executive'
    }
    
    name_lower = name.lower().strip()
    
    if name_lower in placeholder_names:
        return True
    
    placeholder_patterns = [
        r'^john\s+doe',
        r'^jane\s+doe',
        r'^john\s+smith',
        r'^jane\s+smith',
        r'^test\s+',
        r'^example\s+',
        r'^sample\s+',
        r'^dummy\s+'
    ]
    
    for pattern in placeholder_patterns:
        if re.match(pattern, name_lower):
            return True
    
    return False


def is_website_section(name: str) -> bool:
    """
    NEW: Check if 'product' name is actually a website section.
    
    Filters out common false positives like:
    - Website pages: "Blog", "Press Kit", "Newsroom"
    - Legal docs: "Terms and Privacy", "Updates to Policy"
    - Initiatives: "Advisory Council", "Economic Program"
    - Partnerships: "MOU with Government"
    """
    if not name:
        return True
    
    website_sections = {
        'blog', 'videos', 'press kit', 'company', 'newsroom', 'press',
        'careers', 'about', 'contact', 'team', 'investors', 'customers',
        'partners', 'pricing', 'news', 'resources', 'insights', 'events',
        'webinars', 'documentation', 'docs', 'support', 'help center',
        'terms', 'privacy', 'policy', 'legal', 'security', 'compliance',
        'customer info', 'case studies', 'success stories'
    }
    
    name_lower = name.lower().strip()
    
    if name_lower in website_sections:
        return True
    
    # Pattern-based exclusions
    non_product_patterns = [
        r'^updates?\s+to\s+',  # "Updates to Terms"
        r'^signs?\s+',  # "Signs MOU"
        r'^mou\s+with\s+',  # "MOU with UK Government"
        r'^expanding\s+',  # "Expanding Google Cloud TPUs"
        r'^announces?\s+',  # "Announces Partnership"
        r'advisory\s+council',  # "Economic Advisory Council"
        r'futures?\s+program',  # "Economic Futures Program"
        r'program$',  # Ends with "Program" (usually initiatives)
    ]
    
    for pattern in non_product_patterns:
        if re.search(pattern, name_lower):
            return True
    
    return False


def is_valid_full_name(name: str) -> bool:
    """Check if name is a valid full name (First Last)."""
    if not name:
        return False
    
    if ' ' not in name:
        return False
    
    role_words = ['ceo', 'cto', 'cfo', 'chief', 'officer', 'president']
    if any(word in name.lower() for word in role_words):
        return False
    
    return True


def is_placeholder_date(date_value: Any) -> bool:
    """Check if date is a placeholder."""
    if not date_value:
        return False
    
    date_str = str(date_value)
    return 'XX' in date_str or 'xx' in date_str


def normalize_url(url: str, prefix: str = 'https://') -> Optional[str]:
    """Normalize URL (add https:// if missing)."""
    if not url:
        return None
    
    url = url.strip()
    
    if not url.startswith('http://') and not url.startswith('https://'):
        url = prefix + url
    
    return url


def create_provenance(sources: Dict[str, Any], page_types: List[str], 
                     snippet: Optional[str] = None, 
                     blog_post_id: Optional[str] = None) -> List[Provenance]:
    """Create Provenance objects from metadata with real source URLs."""
    provenance_list = []
    url_mapping = sources.get('url_mapping', {})
    metadata = sources.get('metadata', {})
    
    # Page type aliases for better matching
    page_type_aliases = {
        'home': 'homepage',
        'homepage': 'homepage',
        'about': 'about',
        'company': 'about',
        'team': 'team',
        'leadership': 'team',
        'careers': 'careers',
        'jobs': 'careers',
        'press': 'press',
        'newsroom': 'press',
        'blog': 'blog',
        'news': 'blog'
    }
    
    # Try to find URLs for requested page types
    for page_type in page_types:
        # Try direct match first
        if page_type in url_mapping:
            url_info = url_mapping[page_type]
            try:
                prov = Provenance(
                    source_url=url_info['source_url'],
                    crawled_at=url_info['crawled_at'],
                    snippet=snippet[:500] if snippet else None
                )
                provenance_list.append(prov)
                continue
            except Exception as e:
                print(f"   âš ï¸  Failed to create provenance for {page_type}: {e}")
        
        # Try alias match
        alias = page_type_aliases.get(page_type)
        if alias and alias in url_mapping:
            url_info = url_mapping[alias]
            try:
                prov = Provenance(
                    source_url=url_info['source_url'],
                    crawled_at=url_info['crawled_at'],
                    snippet=snippet[:500] if snippet else None
                )
                provenance_list.append(prov)
                continue
            except Exception as e:
                pass
    
    # Handle blog post URLs
    if blog_post_id:
        blog_url_mapping = sources.get('blog_url_mapping', {})
        if blog_post_id in blog_url_mapping:
            url_info = blog_url_mapping[blog_post_id]
            try:
                prov = Provenance(
                    source_url=url_info['source_url'],
                    crawled_at=url_info['crawled_at'],
                    snippet=snippet[:500] if snippet else None
                )
                provenance_list.append(prov)
            except Exception as e:
                pass
    
    # If still no provenance, try to get from metadata pages array
    if not provenance_list and metadata.get('pages'):
        for page in metadata['pages']:
            page_type = page.get('page_type', '')
            source_url = page.get('source_url')
            crawled_at = page.get('crawled_at')
            
            # Check if this page type matches any requested type
            if source_url and crawled_at:
                for requested_type in page_types:
                    if (requested_type.lower() in page_type.lower() or 
                        page_type.lower() in requested_type.lower()):
                        try:
                            prov = Provenance(
                                source_url=source_url,
                                crawled_at=crawled_at,
                                snippet=snippet[:500] if snippet else None
                            )
                            provenance_list.append(prov)
                            break
                        except:
                            pass
                if provenance_list:
                    break
    
    # Last resort: use company website from metadata or construct from company_id
    if not provenance_list:
        company_id = sources.get('company_id') or 'unknown'
        website = f"https://{company_id}.com"
        
        # Try to get website from metadata
        if metadata.get('pages'):
            for page in metadata['pages']:
                if page.get('page_type') == 'homepage' and page.get('source_url'):
                    website = page['source_url']
                    break
        
        scrape_ts = metadata.get('scrape_timestamp', datetime.now().isoformat())
        try:
            prov = Provenance(
                source_url=website,
                crawled_at=scrape_ts,
                snippet=snippet[:500] if snippet else None
            )
            provenance_list.append(prov)
        except:
            pass
    
    return provenance_list


def extract_founded_year_aggressive(sources: Dict[str, Any]) -> Optional[int]:
    """
    NEW: Aggressively search ALL text content for founding year.
    
    Searches through:
    - All text files
    - All blog posts
    - Using patterns: "founded in", "established in", "since", etc.
    """
    all_text = ""
    
    # Combine ALL text files
    for file_name, file_data in sources.get('files', {}).items():
        all_text += file_data['content'] + "\n\n"
    
    # Add ALL blog posts
    for blog in sources.get('blog_posts', []):
        all_text += blog['content'] + "\n\n"
    
    # Search for founding mentions
    founding_patterns = [
        r'founded\s+in\s+(\d{4})',
        r'established\s+in\s+(\d{4})',
        r'started\s+in\s+(\d{4})',
        r'since\s+(\d{4})',
        r'began\s+in\s+(\d{4})',
        r'launched\s+in\s+(\d{4})',
        r'inception\s+in\s+(\d{4})',
        r'created\s+in\s+(\d{4})'
    ]
    
    for pattern in founding_patterns:
        match = re.search(pattern, all_text.lower())
        if match:
            year = int(match.group(1))
            if 2000 <= year <= 2023:
                print(f"   âœ“ Found in text: founded {year}")
                return year
    
    return None


# ============================================================================
# FIELD KEYWORDS - COMPREHENSIVE
# ============================================================================

FIELD_KEYWORDS = {
    'legal_name': ['company', 'incorporated', 'inc', 'llc', 'ltd', 'corp'],
    'founded_year': ['founded', 'established', 'since', 'started', 'began', 'inception'],
    'hq_city': ['headquarters', 'based in', 'located in', 'office', 'hq'],
    'website': ['website', 'visit', 'learn more', 'contact'],
    'categories': ['industry', 'sector', 'focus', 'specializes', 'provides'],
    'related_companies': ['competitor', 'similar', 'alternative', 'compared to', 'vs'],
    'funding': ['raises', 'raised', 'funding', 'series', 'round', 'investment', 'capital', 'valuation', 'investors', 'led by'],
    'investors': ['investor', 'led by', 'backed by', 'participated', 'venture'],
    'founders': ['founder', 'co-founder', 'founded by', 'started by', 'ceo and founder'],
    'executives': ['ceo', 'cto', 'cfo', 'coo', 'chief', 'officer', 'president', 'joins as', 'appointed', 'leadership'],
    'linkedin': ['linkedin.com', 'linkedin profile'],
    'products': ['product', 'platform', 'solution', 'offering', 'service', 'introducing', 'launches', 'release'],
    'pricing': ['pricing', 'price', 'tier', 'plan', 'subscription', 'cost', 'free', 'enterprise'],
    'integrations': ['integrates', 'integration', 'partners with', 'works with', 'compatible'],
    'github': ['github.com', 'github repository', 'open source', 'source code'],
    'license': ['license', 'mit', 'apache', 'gpl', 'bsd', 'open source'],
    'customers': ['customer', 'client', 'uses', 'deployed', 'case study'],
    'hiring': ['hiring', 'careers', 'jobs', 'positions', 'roles', 'join', 'openings'],
    'headcount': ['employees', 'team size', 'headcount', 'people', 'staff'],
    'offices': ['office', 'location', 'expands', 'opens', 'headquarters'],
    'partnerships': ['partnership', 'partners with', 'teams up', 'collaboration', 'announces'],
    'launches': ['launches', 'introducing', 'announces', 'unveils', 'releases'],
    'mna': ['acquires', 'acquisition', 'merger', 'acquired by', 'merges'],
    'integration': ['integrates with', 'integration with', 'now available'],
    'customer_win': ['signs', 'contract', 'major customer', 'enterprise deal'],
    'regulatory': ['compliance', 'certification', 'soc2', 'hipaa', 'gdpr', 'regulatory'],
    'security_incident': ['breach', 'security incident', 'vulnerability', 'attack'],
    'pricing_change': ['price change', 'pricing update', 'new pricing'],
    'layoff': ['layoff', 'downsizing', 'reducing headcount', 'restructuring'],
    'hiring_spike': ['rapid hiring', 'hiring surge', 'expanding team'],
    'office_open': ['opens office', 'new office', 'expands to'],
    'office_close': ['closes office', 'shutting down', 'consolidating'],
    'benchmark': ['benchmark', 'performance', 'evaluation', 'test results'],
    'open_source': ['open source', 'releases on github', 'available on github'],
    'contract_award': ['awarded contract', 'wins contract', 'government contract'],
    'github_stars': ['github stars', 'github repository', 'open source'],
    'glassdoor': ['glassdoor', 'employee rating', 'workplace rating'],
}


# ============================================================================
# HTML & JSON-LD PARSING
# ============================================================================

def extract_jsonld_item(item: Dict[str, Any], jsonld_data: Dict[str, Any]):
    """Helper to extract data from a single JSON-LD item."""
    item_type = item.get('@type')
    
    if item_type == 'Organization':
        jsonld_data.update({
            'name': item.get('name'),
            'legalName': item.get('legalName'),
            'foundingDate': item.get('foundingDate'),
            'url': item.get('url'),
            'address': item.get('address'),
            'description': item.get('description'),
            'numberOfEmployees': item.get('numberOfEmployees')
        })
    
    elif item_type == 'Product':
        if 'products' not in jsonld_data:
            jsonld_data['products'] = []
        jsonld_data['products'].append({
            'name': item.get('name'),
            'description': item.get('description'),
            'offers': item.get('offers')
        })
    
    elif item_type == 'Person':
        if 'people' not in jsonld_data:
            jsonld_data['people'] = []
        jsonld_data['people'].append({
            'name': item.get('name'),
            'jobTitle': item.get('jobTitle'),
            'worksFor': item.get('worksFor'),
            'sameAs': item.get('sameAs')
        })
    
    elif item_type == 'Event':
        if 'events' not in jsonld_data:
            jsonld_data['events'] = []
        jsonld_data['events'].append({
            'name': item.get('name'),
            'startDate': item.get('startDate'),
            'description': item.get('description')
        })


def extract_jsonld_data(html_content: str) -> Dict[str, Any]:
    """Extract JSON-LD structured data from HTML."""
    jsonld_data = {}
    
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        jsonld_scripts = soup.find_all('script', type='application/ld+json')
        
        for script in jsonld_scripts:
            try:
                data = json.loads(script.string)
                
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            extract_jsonld_item(item, jsonld_data)
                elif isinstance(data, dict):
                    extract_jsonld_item(data, jsonld_data)
                
            except json.JSONDecodeError as e:
                print(f"   âš ï¸  JSON-LD parse error: {str(e)[:50]}")
                continue
    
    except Exception as e:
        print(f"   âš ï¸  JSON-LD extraction error: {str(e)[:50]}")
    
    return jsonld_data


def extract_structured_from_html(html_content: str) -> Dict[str, Any]:
    """Extract ALL structured data from HTML patterns."""
    structured = {}
    
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        text = soup.get_text()
        
        # Team members
        team_members = []
        for member_div in soup.find_all(['div', 'article'], class_=lambda x: x and 'team' in x.lower() if x else False):
            name_tag = member_div.find(['h2', 'h3', 'h4', 'strong'])
            role_tag = member_div.find(class_=lambda x: x and 'role' in x.lower() if x else False)
            
            if name_tag:
                team_members.append({
                    'name': name_tag.get_text().strip(),
                    'role': role_tag.get_text().strip() if role_tag else None
                })
        
        if team_members:
            structured['team_members'] = team_members[:20]
        
        # Pricing tiers
        pricing_tiers = []
        for table in soup.find_all('table'):
            headers = [th.get_text().strip() for th in table.find_all('th')]
            if any(word in ' '.join(headers).lower() for word in ['price', 'plan', 'tier']):
                for row in table.find_all('tr')[1:]:
                    cells = [td.get_text().strip() for td in row.find_all('td')]
                    if cells:
                        pricing_tiers.append(cells[0])
        
        for div in soup.find_all('div', class_=lambda x: x and 'price' in x.lower() if x else False):
            tier_name = div.find(['h2', 'h3', 'h4'])
            if tier_name:
                pricing_tiers.append(tier_name.get_text().strip())
        
        if pricing_tiers:
            structured['pricing_tiers'] = list(set(pricing_tiers))[:10]
        
        # Office locations
        locations = []
        for address in soup.find_all('address'):
            address_text = address.get_text().strip()
            cities = re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\b', address_text)
            locations.extend(cities)
        
        if locations:
            structured['locations'] = list(set(locations))[:10]
        
        # Copyright years
        copyright_years = re.findall(r'Â©\s*(\d{4})', html_content)
        if copyright_years:
            years = [int(y) for y in copyright_years if 1990 <= int(y) <= 2023]
            if years:
                structured['copyright_years'] = sorted(set(years))
        
        # Headcount
        headcount_patterns = [
            r'(\d+)\+?\s+employees',
            r'team\s+of\s+(\d+)',
            r'(\d+)\s+people',
            r'headcount[:\s]+(\d+)'
        ]
        for pattern in headcount_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    headcount = int(match.group(1))
                    if 10 <= headcount <= 100000:
                        structured['headcount'] = headcount
                        break
                except:
                    pass
        
        # GitHub repo URLs
        github_links = soup.find_all('a', href=lambda x: x and 'github.com' in x.lower() if x else False)
        if github_links:
            repos = []
            for link in github_links:
                href = link.get('href')
                if '/github.com/' in href and href.count('/') >= 4:
                    repos.append(href)
            if repos:
                structured['github_repos'] = list(set(repos))[:5]
        
        # Glassdoor rating
        glassdoor_patterns = [
            r'glassdoor[:\s]+(\d+\.?\d*)',
            r'(\d+\.?\d*)\s+(?:stars?|rating)\s+on\s+glassdoor',
            r'rated\s+(\d+\.?\d*)\s+on\s+glassdoor'
        ]
        for pattern in glassdoor_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    rating = float(match.group(1))
                    if 0 <= rating <= 5:
                        structured['glassdoor_rating'] = rating
                        break
                except:
                    pass
        
        # Job opening counts
        job_patterns = [
            r'(\d+)\s+open\s+(?:positions|roles|jobs)',
            r'(\d+)\s+(?:positions|roles|jobs)\s+available',
            r'hiring\s+for\s+(\d+)\s+(?:positions|roles)'
        ]
        for pattern in job_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    count = int(match.group(1))
                    if 1 <= count <= 1000:
                        structured['job_openings'] = count
                        break
                except:
                    pass
        
        # Engineering/sales openings
        eng_patterns = [
            r'(\d+)\s+engineering\s+(?:positions|roles|openings)',
            r'(\d+)\s+(?:software|backend|frontend|fullstack)\s+engineer'
        ]
        for pattern in eng_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    structured['engineering_openings'] = int(match.group(1))
                    break
                except:
                    pass
        
        sales_patterns = [
            r'(\d+)\s+sales\s+(?:positions|roles|openings)',
            r'(\d+)\s+(?:account\s+executive|sales\s+rep)'
        ]
        for pattern in sales_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    structured['sales_openings'] = int(match.group(1))
                    break
                except:
                    pass
    
    except Exception as e:
        print(f"   âš ï¸  HTML parsing error: {str(e)[:50]}")
    
    return structured


def search_html_sources(sources: Dict[str, Any], keywords: List[str], max_chars: int = 5000) -> str:
    """Search through HTML files for relevant content."""
    relevant_content = []
    total_chars = 0
    
    for file_name, file_data in sources.get('html_files', {}).items():
        if total_chars >= max_chars:
            break
        
        html = file_data['content']
        
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            paragraphs = soup.find_all(['p', 'div', 'section', 'article'])
            
            for para in paragraphs:
                text = para.get_text().strip()
                if not text or len(text) < 50:
                    continue
                
                text_lower = text.lower()
                
                if any(kw.lower() in text_lower for kw in keywords):
                    snippet = ' '.join(text.split()[:200])
                    relevant_content.append(f"[{file_name.upper()} HTML]\n{snippet}\n")
                    total_chars += len(snippet)
                    
                    if total_chars >= max_chars:
                        break
        
        except Exception as e:
            continue
    
    return '\n---\n'.join(relevant_content)


# ============================================================================
# SOURCE LOADING - COMPREHENSIVE
# ============================================================================

def read_file_from_gcs(bucket_name: str, file_path: str) -> Optional[str]:
    """Read a file from GCS bucket"""
    try:
        client = get_storage_client()
        if not client:
            return None
        
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(file_path)
        
        if not blob.exists():
            return None
        
        return blob.download_as_text()
    except Exception as e:
        print(f"   âš ï¸  Failed to read {file_path} from GCS: {e}")
        return None

def list_files_from_gcs(bucket_name: str, prefix: str) -> List[str]:
    """List files in GCS bucket with given prefix"""
    try:
        client = get_storage_client()
        if not client:
            return []
        
        bucket = client.bucket(bucket_name)
        blobs = bucket.list_blobs(prefix=prefix)
        return [blob.name for blob in blobs]
    except Exception as e:
        print(f"   âš ï¸  Failed to list files from GCS with prefix {prefix}: {e}")
        return []

def write_file_to_gcs(bucket_name: str, file_path: str, content: str) -> bool:
    """Write a file to GCS bucket"""
    try:
        client = get_storage_client()
        if not client:
            return False
        
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(file_path)
        
        blob.upload_from_string(content, content_type='application/json')
        print(f"   âœ… Saved to GCS: gs://{bucket_name}/{file_path}")
        return True
    except Exception as e:
        print(f"   âš ï¸  Failed to write {file_path} to GCS: {e}")
        return False

def save_structured_data(company_id: str, structured_data: Dict[str, Any]) -> Optional[Path]:
    """
    Lab 5: Save structured data to data/structured/<company_id>.json
    Supports both local filesystem and GCS bucket.
    """
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    use_gcs = bucket_name is not None and get_storage_client() is not None
    
    structured_json = json.dumps(structured_data, indent=2, default=str)
    
    if use_gcs:
        # Check for V2_MASTER_FOLDER to use version2/structured/ structure
        v2_master_folder = os.getenv("V2_MASTER_FOLDER", "")
        if v2_master_folder:
            file_path = f"{v2_master_folder}/structured/{company_id}.json"
        else:
            file_path = f"structured/{company_id}.json"
        success = write_file_to_gcs(bucket_name, file_path, structured_json)
        if success:
            return Path(f"gs://{bucket_name}/{file_path}")
        else:
            # Fallback to local if GCS fails
            print(f"   âš ï¸  GCS save failed, falling back to local filesystem")
            use_gcs = False
    
    if not use_gcs:
        # Save to local filesystem
        output_path = Path(f"data/structured/{company_id}.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        output_path.write_text(structured_json, encoding='utf-8')
        print(f"   âœ… Saved structured data: {output_path}")
        return output_path
    
    return None

def save_payload_to_storage(company_id: str, payload: Payload) -> Optional[Path]:
    """
    Lab 6: Save payload to data/payloads/<company_id>.json
    Supports both local filesystem and GCS bucket.
    """
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    use_gcs = bucket_name is not None and get_storage_client() is not None
    
    payload_json = json.dumps(payload.model_dump(), indent=2, default=str)
    
    if use_gcs:
        # Check for V2_MASTER_FOLDER to use version2/payloads/ structure
        v2_master_folder = os.getenv("V2_MASTER_FOLDER", "")
        if v2_master_folder:
            file_path = f"{v2_master_folder}/payloads/{company_id}.json"
        else:
            file_path = f"payloads/{company_id}.json"
        success = write_file_to_gcs(bucket_name, file_path, payload_json)
        if success:
            return Path(f"gs://{bucket_name}/{file_path}")
        else:
            # Fallback to local if GCS fails
            print(f"   âš ï¸  GCS save failed, falling back to local filesystem")
            use_gcs = False
    
    if not use_gcs:
        # Save to local filesystem
        output_path = Path(f"data/payloads/{company_id}.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        output_path.write_text(payload_json, encoding='utf-8')
        print(f"   âœ… Saved payload: {output_path}")
        return output_path
    
    return None


def load_all_sources(company_id: str) -> Dict[str, Any]:
    """Load ALL sources: text, HTML, JSON, JSON-LD, structured, blogs, press, Forbes seed.
    
    Supports both local filesystem and GCS bucket (when GCS_BUCKET_NAME is set).
    """
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    use_gcs = bucket_name is not None and get_storage_client() is not None
    
    # Determine base path (local or GCS prefix)
    # Check for V2_MASTER_FOLDER environment variable for version 2 structure
    v2_master_folder = os.getenv("V2_MASTER_FOLDER", "")
    run_folder = os.getenv("V2_RUN_FOLDER", "comprehensive_extraction")
    
    if use_gcs:
        if v2_master_folder:
            # Use version2/raw/{company_id}/comprehensive_extraction structure
            base_prefix = f"{v2_master_folder}/raw/{company_id}/{run_folder}"
        else:
            # Default structure: raw/{company_id}/comprehensive_extraction
            base_prefix = f"raw/{company_id}/{run_folder}"
        print(f"ðŸ“‚ Loading sources from GCS: gs://{bucket_name}/{base_prefix}")
    else:
        if v2_master_folder:
            base_path = Path(f"data/{v2_master_folder}/raw/{company_id}/{run_folder}")
        else:
            base_path = Path(f"data/raw/{company_id}/{run_folder}")
        print(f"ðŸ“‚ Loading sources from local filesystem: {base_path}")
    
    sources = {
        'files': {},
        'html_files': {},
        'structured_json': {},
        'jsonld_data': {},
        'html_structured': {},
        'blog_posts': [],
        'press_releases': [],
        'metadata': {},
        'url_mapping': {},
        'forbes_seed': {},
        'pre_extracted_entities': {},  # NEW: Pre-extracted data from scraper
        'company_id': company_id  # Store for provenance creation
    }
    
    if use_gcs:
        # Load from GCS
        # List all files with the prefix
        all_files = list_files_from_gcs(bucket_name, base_prefix)
        
        # Text files
        for file_path in all_files:
            if file_path.endswith("_clean.txt") and "blog_posts" not in file_path:
                page_type = Path(file_path).stem.replace("_clean", "")
                content = read_file_from_gcs(bucket_name, file_path)
                if content:
                    sources['files'][page_type] = {
                        'content': content,
                        'path': f"gs://{bucket_name}/{file_path}",
                        'size': len(content)
                    }
        
        # HTML files
        for file_path in all_files:
            if file_path.endswith(".html") and "blog_posts" not in file_path:
                page_type = Path(file_path).stem
                content = read_file_from_gcs(bucket_name, file_path)
                if content:
                    sources['html_files'][page_type] = {
                        'content': content,
                        'path': f"gs://{bucket_name}/{file_path}",
                        'size': len(content)
                    }
                    
                    # Extract JSON-LD
                    jsonld = extract_jsonld_data(content)
                    if jsonld:
                        sources['jsonld_data'][page_type] = jsonld
                    
                    # Extract structured
                    html_struct = extract_structured_from_html(content)
                    if html_struct:
                        sources['html_structured'][page_type] = html_struct
        
        # Structured JSON
        for file_path in all_files:
            if file_path.endswith("_structured.json"):
                page_type = Path(file_path).stem.replace("_structured", "")
                content = read_file_from_gcs(bucket_name, file_path)
                if content:
                    try:
                        data = json.loads(content)
                        sources['structured_json'][page_type] = data
                    except Exception as e:
                        print(f"   âš ï¸  Failed to parse JSON from {file_path}: {e}")
        
        # Metadata (load first so we can use it for blog posts)
        metadata_path = f"{base_prefix}/metadata.json"
        metadata = {}
        content = read_file_from_gcs(bucket_name, metadata_path)
        if content:
            try:
                metadata = json.loads(content)
                sources['metadata'] = metadata
                
                if 'pages' in metadata:
                    for page in metadata['pages']:
                        page_type = page.get('page_type')
                        source_url = page.get('source_url')
                        crawled_at = page.get('crawled_at')
                        
                        if page_type and source_url and crawled_at:
                            sources['url_mapping'][page_type] = {
                                'source_url': source_url,
                                'crawled_at': crawled_at
                            }
            except Exception as e:
                print(f"   âš ï¸  Failed to load metadata: {e}")
        
        # Blog posts (new scraper stores them as blog_*_clean.txt in main directory)
        # Look for blog posts in the main directory, not a subdirectory
        blog_url_mapping = {}
        for file_path in all_files:
            if file_path.startswith(f"{base_prefix}/blog_") and file_path.endswith("_clean.txt"):
                # Extract post ID from filename (e.g., "blog_september-2025-funding-round_clean.txt" -> "september-2025-funding-round")
                post_id = Path(file_path).stem.replace("_clean", "").replace("blog_", "")
                content = read_file_from_gcs(bucket_name, file_path)
                if content:
                    # Try to extract URL from blog post content or metadata
                    blog_url = None
                    blog_crawled_at = None
                    
                    # Check metadata for blog post URLs
                    if metadata.get('pages'):
                        for page in metadata['pages']:
                            if page.get('page_type') == 'blog' or 'blog' in str(page.get('source_url', '')).lower():
                                # Try to match blog post by checking if URL contains post_id
                                page_url = page.get('source_url', '')
                                if post_id.replace('-', '_').lower() in page_url.lower().replace('-', '_'):
                                    blog_url = page_url
                                    blog_crawled_at = page.get('crawled_at')
                                    break
                    
                    # If no URL found, try to extract from content (first line might have URL)
                    if not blog_url:
                        first_lines = content.split('\n')[:5]
                        for line in first_lines:
                            if 'http' in line and company_id.lower() in line.lower():
                                # Try to extract URL
                                url_match = re.search(r'https?://[^\s]+', line)
                                if url_match:
                                    blog_url = url_match.group(0)
                                    break
                    
                    # Store blog post
                    sources['blog_posts'].append({
                        'id': post_id,
                        'content': content,
                        'path': f"gs://{bucket_name}/{file_path}",
                        'size': len(content),
                        'url': blog_url
                    })
                    
                    # Store URL mapping for provenance
                    if blog_url:
                        blog_url_mapping[post_id] = {
                            'source_url': blog_url,
                            'crawled_at': blog_crawled_at or metadata.get('scrape_timestamp', datetime.now().isoformat())
                        }
        
        sources['blog_url_mapping'] = blog_url_mapping
        
        # Forbes seed data from GCS
        seed_file_path = os.getenv("GCS_SEED_FILE_PATH", "seed/forbes_ai50_seed.json")
        content = read_file_from_gcs(bucket_name, seed_file_path)
        if content:
            try:
                forbes_data = json.loads(content)
                for company in forbes_data:
                    website = company.get('website', '').lower()
                    if company_id.lower() in website:
                        sources['forbes_seed'] = company
                        print(f"   âœ“ Loaded Forbes seed data for {company_id}")
                        break
            except Exception as e:
                print(f"   âš ï¸  Failed to load Forbes seed: {e}")
    
    else:
        # Original local filesystem loading code
        # Text files
        for txt_file in base_path.glob("*_clean.txt"):
            page_type = txt_file.stem.replace("_clean", "")
            try:
                content = txt_file.read_text(encoding='utf-8')
                sources['files'][page_type] = {
                    'content': content,
                    'path': str(txt_file),
                    'size': len(content)
                }
            except Exception as e:
                print(f"   âš ï¸  Failed to read {txt_file.name}: {e}")
        
        # HTML files
        for html_file in base_path.glob("*.html"):
            page_type = html_file.stem
            try:
                content = html_file.read_text(encoding='utf-8')
                sources['html_files'][page_type] = {
                    'content': content,
                    'path': str(html_file),
                    'size': len(content)
                }
                
                # Extract JSON-LD
                jsonld = extract_jsonld_data(content)
                if jsonld:
                    sources['jsonld_data'][page_type] = jsonld
                
                # Extract structured
                html_struct = extract_structured_from_html(content)
                if html_struct:
                    sources['html_structured'][page_type] = html_struct
            
            except Exception as e:
                print(f"   âš ï¸  Failed to read {html_file.name}: {e}")
        
        # Structured JSON
        for json_file in base_path.glob("*_structured.json"):
            page_type = json_file.stem.replace("_structured", "")
            try:
                data = json.loads(json_file.read_text(encoding='utf-8'))
                sources['structured_json'][page_type] = data
            except Exception as e:
                print(f"   âš ï¸  Failed to read {json_file.name}: {e}")
        
        # Metadata (load early so we can use it for blog posts)
        metadata = {}
        metadata_file = base_path / "metadata.json"
        if metadata_file.exists():
            try:
                metadata = json.loads(metadata_file.read_text(encoding='utf-8'))
                sources['metadata'] = metadata
                
                if 'pages' in metadata:
                    for page in metadata['pages']:
                        page_type = page.get('page_type')
                        source_url = page.get('source_url')
                        crawled_at = page.get('crawled_at')
                        
                        if page_type and source_url and crawled_at:
                            sources['url_mapping'][page_type] = {
                                'source_url': source_url,
                                'crawled_at': crawled_at
                            }
            except Exception as e:
                print(f"   âš ï¸  Failed to load metadata: {e}")
        
        # Blog posts (new scraper stores them as blog_*_clean.txt in main directory)
        # Also check for blog_clean.txt (single blog page without post ID)
        # Look for blog posts in the main directory, not a subdirectory
        blog_url_mapping = {}
        blog_files = list(base_path.glob("blog_*_clean.txt"))
        # Also check for blog_clean.txt (without underscore pattern)
        blog_clean_file = base_path / "blog_clean.txt"
        if blog_clean_file.exists():
            blog_files.append(blog_clean_file)
        
        for blog_file in sorted(blog_files):
            try:
                # Extract post ID from filename (e.g., "blog_september-2025-funding-round_clean.txt" -> "september-2025-funding-round")
                post_id = blog_file.stem.replace("_clean", "").replace("blog_", "")
                content = blog_file.read_text(encoding='utf-8')
                
                # Try to extract URL from metadata or content
                blog_url = None
                blog_crawled_at = None
                
                # Check metadata for blog post URLs
                if metadata.get('pages'):
                    for page in metadata['pages']:
                        if page.get('page_type') == 'blog' or 'blog' in str(page.get('source_url', '')).lower():
                            # Try to match blog post by checking if URL contains post_id
                            page_url = page.get('source_url', '')
                            if post_id.replace('-', '_').lower() in page_url.lower().replace('-', '_'):
                                blog_url = page_url
                                blog_crawled_at = page.get('crawled_at')
                                break
                
                # If no URL found, try to extract from content (first line might have URL)
                if not blog_url:
                    first_lines = content.split('\n')[:5]
                    for line in first_lines:
                        if 'http' in line and company_id.lower() in line.lower():
                            # Try to extract URL
                            url_match = re.search(r'https?://[^\s]+', line)
                            if url_match:
                                blog_url = url_match.group(0)
                                break
                
                # Store blog post
                sources['blog_posts'].append({
                    'id': post_id,
                    'content': content,
                    'path': str(blog_file),
                    'size': len(content),
                    'url': blog_url
                })
                
                # Store URL mapping for provenance
                if blog_url:
                    blog_url_mapping[post_id] = {
                        'source_url': blog_url,
                        'crawled_at': blog_crawled_at or metadata.get('scrape_timestamp', datetime.now().isoformat())
                    }
            except Exception as e:
                print(f"   âš ï¸  Failed to read blog post {blog_file.name}: {e}")
        
        sources['blog_url_mapping'] = blog_url_mapping
        
        # Metadata (load early so we can use it for blog posts)
        metadata = {}
        metadata_file = base_path / "metadata.json"
        if metadata_file.exists():
            try:
                metadata = json.loads(metadata_file.read_text(encoding='utf-8'))
                sources['metadata'] = metadata
                
                if 'pages' in metadata:
                    for page in metadata['pages']:
                        page_type = page.get('page_type')
                        source_url = page.get('source_url')
                        crawled_at = page.get('crawled_at')
                        
                        if page_type and source_url and crawled_at:
                            sources['url_mapping'][page_type] = {
                                'source_url': source_url,
                                'crawled_at': crawled_at
                            }
            except Exception as e:
                print(f"   âš ï¸  Failed to load metadata: {e}")
        
        # Forbes seed data
        forbes_path = Path("data/forbes_ai50_seed.json")
        if forbes_path.exists():
            try:
                with open(forbes_path, 'r', encoding='utf-8') as f:
                    forbes_data = json.load(f)
                
                for company in forbes_data:
                    website = company.get('website', '').lower()
                    if company_id.lower() in website:
                        sources['forbes_seed'] = company
                        print(f"   âœ“ Loaded Forbes seed data for {company_id}")
                        break
                
                if not sources['forbes_seed']:
                    print(f"   âš ï¸  No Forbes seed data found for {company_id}")
                    
            except Exception as e:
                print(f"   âš ï¸  Failed to load Forbes seed: {e}")
    
    # Press releases (common for both GCS and local)
    if 'press' in sources['files']:
        sources['press_releases'] = parse_press_releases(sources['files']['press']['content'])
    
    # NEW: Load pre-extracted entities from scraper (PRIMARY SOURCE - NO HALLUCINATION)
    if use_gcs:
        extracted_entities_path = f"{base_prefix}/extracted_entities.json"
        content = read_file_from_gcs(bucket_name, extracted_entities_path)
        if content:
            try:
                sources['pre_extracted_entities'] = json.loads(content)
                print(f"   âœ… Loaded pre-extracted entities from scraper (PRIMARY SOURCE)")
            except Exception as e:
                print(f"   âš ï¸  Failed to load extracted_entities.json: {e}")
    else:
        extracted_entities_file = base_path / "extracted_entities.json"
        if extracted_entities_file.exists():
            try:
                sources['pre_extracted_entities'] = json.loads(extracted_entities_file.read_text(encoding='utf-8'))
                print(f"   âœ… Loaded pre-extracted entities from scraper (PRIMARY SOURCE)")
            except Exception as e:
                print(f"   âš ï¸  Failed to load extracted_entities.json: {e}")
    
    return sources


def parse_press_releases(press_text: str) -> List[Dict[str, str]]:
    """Parse press releases into structured format with dates."""
    from dateutil import parser as date_parser
    
    releases = []
    lines = press_text.strip().split('\n')
    
    current_category = None
    current_title = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line in ['Announcements', 'Policy', 'Product', 'Research', 'Engineering']:
            current_category = line
            continue
        
        date_pattern = r'^([A-Z][a-z]{2})\s+(\d{1,2}),?\s+(\d{4})$'
        if re.match(date_pattern, line) and current_title:
            try:
                parsed_date = date_parser.parse(line)
                releases.append({
                    'title': current_title,
                    'date': parsed_date.strftime('%Y-%m-%d'),
                    'category': current_category or 'General'
                })
                current_title = None
            except:
                current_title = line
        else:
            if line and '@' not in line:
                current_title = line
    
    return releases


def get_jsonld_value(sources: Dict[str, Any], field: str) -> Any:
    """Get field from JSON-LD data across all pages."""
    for page_type, jsonld in sources.get('jsonld_data', {}).items():
        if field in jsonld and jsonld[field]:
            return jsonld[field]
    return None


def get_structured_data(sources: Dict[str, Any], field: str) -> Any:
    """Get field from structured JSON files."""
    for page_type, data in sources.get('structured_json', {}).items():
        if field in data and data[field]:
            return data[field]
    return None


def search_all_sources(sources: Dict[str, Any], keywords: List[str], max_chars: int = 5000) -> str:
    """COMPREHENSIVE: Search through ALL sources (text, HTML, blog posts)."""
    relevant_content = []
    total_chars = 0
    
    # Search text files
    for file_name, file_data in sources.get('files', {}).items():
        if total_chars >= max_chars:
            break
        
        content = file_data['content']
        paragraphs = re.split(r'\n\s*\n', content)
        
        for para in paragraphs:
            para_lower = para.lower()
            
            if any(kw.lower() in para_lower for kw in keywords):
                snippet = para.strip()
                relevant_content.append(f"[{file_name.upper()}]\n{snippet}\n")
                total_chars += len(snippet)
                
                if total_chars >= max_chars:
                    break
        
    # Search HTML files
    if total_chars < max_chars:
        html_content = search_html_sources(sources, keywords, max_chars - total_chars)
        if html_content:
            relevant_content.append(html_content)
            total_chars += len(html_content)
    
    # Search blog posts
    if total_chars < max_chars:
        for blog in sources.get('blog_posts', [])[:10]:
            if total_chars >= max_chars:
                break
            
            content = blog['content']
            paragraphs = re.split(r'\n\s*\n', content)
            
            for para in paragraphs:
                para_lower = para.lower()
                
                if any(kw.lower() in para_lower for kw in keywords):
                    snippet = para.strip()
                    relevant_content.append(f"[BLOG: {blog['id']}]\n{snippet}\n")
                    total_chars += len(snippet)
                    
                    if total_chars >= max_chars:
                        break
    
    return '\n---\n'.join(relevant_content)


def get_structured_timeline(sources: Dict[str, Any], event_type: str) -> str:
    """Get structured timeline of events with dates."""
    press_releases = sources.get('press_releases', [])
    
    if event_type == 'funding':
        keywords = FIELD_KEYWORDS['funding']
    elif event_type == 'product':
        keywords = FIELD_KEYWORDS['products']
    elif event_type == 'office':
        keywords = FIELD_KEYWORDS['offices']
    elif event_type == 'leadership':
        keywords = FIELD_KEYWORDS['executives']
    else:
        return '\n'.join([f"{pr['date']}: {pr['title']}" for pr in press_releases])
    
    filtered = [pr for pr in press_releases 
                if any(kw in pr['title'].lower() for kw in keywords)]
    
    return '\n'.join([f"{pr['date']}: {pr['title']}" for pr in filtered])


# ============================================================================
# CONVERSION FUNCTIONS - PRE-EXTRACTED ENTITIES TO PYDANTIC MODELS
# ============================================================================

def convert_pre_extracted_funding_events(pre_extracted: Dict[str, Any], company_id: str) -> List[Event]:
    """Convert pre-extracted funding events from scraper to Pydantic Event models."""
    events = []
    funding_events = pre_extracted.get('funding_events', [])
    
    for idx, fe in enumerate(funding_events):
        try:
            # Parse date if available (check multiple possible fields)
            occurred_on = None
            date_str = fe.get('date') or fe.get('occurred_on') or fe.get('occurredOn')
            if date_str:
                try:
                    from dateutil import parser as date_parser
                    occurred_on = date_parser.parse(str(date_str)).date()
                except:
                    # Try parsing YYYY-MM-DD format directly
                    try:
                        occurred_on = datetime.strptime(str(date_str), '%Y-%m-%d').date()
                    except:
                        pass
            
            # If no date, try to extract from description or title
            if not occurred_on:
                # Look for dates in description/title (e.g., "September 2025", "2025-09")
                date_patterns = [
                    r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
                    r'(\d{4}-\d{2})',  # YYYY-MM
                    r'([A-Z][a-z]+ \d{4})',  # September 2025
                ]
                text_to_search = (fe.get('description') or fe.get('title') or '').lower()
                for pattern in date_patterns:
                    match = re.search(pattern, text_to_search)
                    if match:
                        try:
                            from dateutil import parser as date_parser
                            occurred_on = date_parser.parse(match.group(1)).date()
                            break
                        except:
                            pass
            
            # If still no date, skip this event (don't use placeholder date)
            if not occurred_on:
                print(f"   âš ï¸  No date found for funding event, skipping: {fe.get('title', 'Unknown')}")
                continue
            
            # Generate event_id
            round_slug = (fe.get('round_name') or 'unknown').lower().replace(' ', '_')
            event_id = f"{company_id}_funding_{round_slug}_{occurred_on.year}_{occurred_on.month:02d}_{occurred_on.day:02d}"
            if idx > 0:
                event_id = f"{event_id}_{idx}"
            
            # Post-process: Extract investors, round_name, valuation from description if not already in pre-extracted
            investors = fe.get('investors', []) or []
            round_name = fe.get('round_name')
            valuation_usd = fe.get('valuation_usd')
            description = fe.get('description', '') or fe.get('title', '')
            
            # Extract investors from description if not already extracted
            if not investors and description:
                # Known investor names to look for (common VCs)
                known_investors = [
                    'OpenAI Startup Fund', 'Accel', 'Founders Fund', 'Khosla Ventures',
                    'Y Combinator', 'Sequoia', 'Andreessen Horowitz', 'a16z',
                    'Lachy Groom', 'Sam Altman', 'Peter Thiel', 'Paul Graham',
                    'Jeff Weiner', 'Buckley Ventures', 'Neo', 'GSV', 'Inovia Capital',
                    'Radical Ventures', 'AMD Ventures', 'NVIDIA', 'PSP Investment'
                ]
                
                # First, try to find known investors by name
                found_investors = []
                for inv in known_investors:
                    if inv.lower() in description.lower():
                        found_investors.append(inv)
                
                # If we found known investors, use those
                if found_investors:
                    investors = found_investors
                else:
                    # Otherwise, use regex patterns (more conservative)
                    investor_patterns = [
                        r'led by ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                        r'from ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                        r'investors? (?:like|including|such as) ([A-Z][a-zA-Z\s&,]+?)(?:,|\.|and|with|$)',
                        r'participation from ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                    ]
                    for pattern in investor_patterns:
                        matches = re.findall(pattern, description, re.IGNORECASE)
                        for match in matches:
                            investor_name = match.strip().rstrip(',.and with')
                            # Filter out common false positives
                            if (investor_name and len(investor_name) > 2 and 
                                len(investor_name) < 50 and  # Not too long
                                investor_name.lower() not in ['the', 'this', 'that', 'round', 'funding', 'company', 'all existing', 'new strategic'] and
                                investor_name not in investors):
                                investors.append(investor_name)
                
                # Clean up investors list
                investors = list(dict.fromkeys(investors))  # Remove duplicates
                investors = [inv for inv in investors if len(inv) > 2 and len(inv) < 50]
            
            # Extract round name from description if not already extracted
            if not round_name and description:
                desc_lower = description.lower()
                round_patterns = [
                    (r'series\s+([a-z])\b', lambda m: f"Series {m.group(1).upper()}"),
                    (r'series\s+([a-z]+)\b', lambda m: f"Series {m.group(1).title()}"),
                    (r'([a-z]+)\s+round', lambda m: m.group(1).title()),
                    (r'([a-z]+)\s+funding', lambda m: m.group(1).title()),
                ]
                for pattern, formatter in round_patterns:
                    match = re.search(pattern, desc_lower)
                    if match:
                        round_name = formatter(match)
                        break
            
            # Extract valuation from description if not already extracted
            if not valuation_usd and description:
                valuation_patterns = [
                    r'\$([\d.]+)\s*billion\s*valuation',
                    r'\$([\d.]+)\s*million\s*valuation',
                    r'valued at \$([\d.]+)\s*(billion|million)',
                    r'valuation of \$([\d.]+)\s*(billion|million)',
                    r'\$([\d.]+)\s*(billion|million)\s*valuation',
                ]
                for pattern in valuation_patterns:
                    match = re.search(pattern, description, re.IGNORECASE)
                    if match:
                        amount = float(match.group(1))
                        unit = match.group(2) if len(match.groups()) > 1 else ('billion' if 'billion' in description.lower() else 'million')
                        if 'billion' in unit.lower() or 'billion' in description.lower():
                            valuation_usd = int(amount * 1_000_000_000)
                        else:
                            valuation_usd = int(amount * 1_000_000)
                        break
            
            event = Event(
                event_id=event_id,
                company_id=company_id,
                occurred_on=occurred_on,
                event_type="funding",
                title=fe.get('title') or f"{round_name or 'Funding'} Round",
                description=description,
                round_name=round_name,
                amount_usd=fe.get('amount_usd'),
                valuation_usd=valuation_usd,
                investors=investors,
                actors=investors,
                tags=[round_name] if round_name else [],
                provenance=[Provenance(
                    source_url=fe.get('url', f"https://{company_id}.com"),
                    crawled_at=fe.get('date', datetime.now().isoformat()),
                    snippet=description[:500] if description else None
                )]
            )
            events.append(event)
        except Exception as e:
            print(f"   âš ï¸  Failed to convert funding event: {e}")
            continue
    
    return events


def convert_pre_extracted_leadership(pre_extracted: Dict[str, Any], company_id: str) -> List[Leadership]:
    """Convert pre-extracted team members from scraper to Pydantic Leadership models - COMPREHENSIVE."""
    leaders = []
    team_members = pre_extracted.get('team_members', [])
    
    # False positive keywords to filter out
    false_positive_keywords = [
        'api', 'apis', 'experience', 'cloud', 'reviews', 'model', 'developer',
        'product', 'platform', 'service', 'tool', 'system', 'solution',
        'editor', 'plugin', 'software', 'application', 'framework',
        'agentic', 'ai', 'artificial intelligence'
    ]
    
    # Common false positive patterns (navigation, UI elements, etc.)
    false_positive_patterns = [
        r'^what\s+we\s+',
        r'^welcome,?\s+',
        r'^view\s+all',
        r'^subscribe\s+to',
        r'^announcing\s+',
        r'^how\s+',
        r'^read\s+',
        r'^click\s+',
        r'^learn\s+more',
        r'^get\s+started',
        r'^sign\s+up',
        r'^join\s+',
    ]
    
    def is_false_positive(name: str) -> bool:
        """Check if name looks like a false positive (product/service name, not person)."""
        if not name:
            return True
        
        name_lower = name.lower().strip()
        
        # Check if it's a single word (person names usually have at least first and last)
        if ' ' not in name_lower:
            return True
        
        # Check false positive patterns
        for pattern in false_positive_patterns:
            if re.match(pattern, name_lower):
                return True
        
        words = name_lower.split()
        
        # Check if any word matches false positive keywords
        for word in words:
            if word in false_positive_keywords:
                return True
        
        # Check if it's clearly a product/service name pattern
        if any(pattern in name_lower for pattern in [
            'model api', 'developer experience', 'windsurf review',
            'agentic ai', 'artificial intelligence', 'baseten cloud',
            'newsletter', 'blog', 'press', 'careers', 'about'
        ]):
            return True
        
        # Check if it's a two-word phrase where both words are technical terms
        if len(words) == 2:
            if all(word in false_positive_keywords for word in words):
                return True
        
        # Check if it starts with common non-person phrases
        if name_lower.startswith(('the ', 'our ', 'your ', 'their ', 'this ', 'that ')):
            return True
        
        # Check if it contains common UI/navigation words
        if any(word in name_lower for word in ['offer', 'benefit', 'feature', 'pricing', 'contact']):
            # But allow if it's clearly a person name (e.g., "John Offer" would be OK)
            if len(words) == 2 and words[0][0].isupper() and words[1][0].isupper():
                return False  # Might be a real name
            return True
        
        return False
    
    for member in team_members:
        try:
            name = member.get('name', '').strip()
            if not name or ' ' not in name:
                continue
            
            # Filter out false positives
            if is_false_positive(name):
                print(f"   âš ï¸  Filtered false positive leadership: {name}")
                continue
            
            # Generate person_id
            name_slug = re.sub(r'[^a-z0-9]+', '_', name.lower())
            person_id = f"{company_id}_{name_slug}"
            
            # Parse dates if available
            start_date = None
            end_date = None
            if member.get('start_date'):
                try:
                    from dateutil import parser as date_parser
                    start_date = date_parser.parse(member['start_date']).date()
                except:
                    # Try YYYY-MM-DD format
                    try:
                        start_date = datetime.strptime(member['start_date'], '%Y-%m-%d').date()
                    except:
                        pass
            
            if member.get('end_date'):
                try:
                    from dateutil import parser as date_parser
                    end_date = date_parser.parse(member['end_date']).date()
                except:
                    try:
                        end_date = datetime.strptime(member['end_date'], '%Y-%m-%d').date()
                    except:
                        pass
            
            # Parse LinkedIn URL
            linkedin = None
            linkedin_url = member.get('linkedin') or member.get('sameAs')
            if linkedin_url:
                try:
                    linkedin = normalize_url(linkedin_url)
                except:
                    pass
            
            leader = Leadership(
                person_id=person_id,
                company_id=company_id,
                name=name,
                role=member.get('jobTitle') or member.get('role', '') or 'Executive',
                is_founder=member.get('is_founder', False) or 'founder' in (member.get('jobTitle') or '').lower() or 'founder' in (member.get('role') or '').lower(),
                start_date=start_date,
                end_date=end_date,
                previous_affiliation=member.get('previous_affiliation'),
                education=member.get('education'),
                linkedin=linkedin,
                provenance=[Provenance(
                    source_url=member.get('url', f"https://{company_id}.com/about"),
                    crawled_at=member.get('date', datetime.now().isoformat()) if member.get('date') else datetime.now().isoformat(),
                    snippet=f"{name} - {member.get('jobTitle', member.get('role', ''))}"
                )]
            )
            leaders.append(leader)
        except Exception as e:
            print(f"   âš ï¸  Failed to convert team member {member.get('name', 'unknown')}: {e}")
            continue
    
    return leaders


def convert_pre_extracted_products(pre_extracted: Dict[str, Any], company_id: str) -> List[Product]:
    """Convert pre-extracted products from scraper to Pydantic Product models - COMPREHENSIVE."""
    products = []
    products_list = pre_extracted.get('products', [])
    
    for prod in products_list:
        try:
            name = prod.get('name', '').strip()
            if not name or is_website_section(name):
                continue
            
            # Generate product_id
            name_slug = re.sub(r'[^a-z0-9]+', '_', name.lower())
            product_id = f"{company_id}_{name_slug}"
            
            # Parse dates if available
            ga_date = None
            if prod.get('ga_date') or prod.get('launch_date'):
                try:
                    from dateutil import parser as date_parser
                    ga_date = date_parser.parse(prod.get('ga_date') or prod.get('launch_date')).date()
                except:
                    # Try YYYY-MM-DD format
                    try:
                        date_str = prod.get('ga_date') or prod.get('launch_date')
                        ga_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                    except:
                        pass
            
            # Parse GitHub URL
            github_repo = None
            if prod.get('github_repo'):
                try:
                    github_repo = normalize_url(prod['github_repo'])
                except:
                    pass
            
            # Get pricing info from pre-extracted or pricing data
            pricing_model = prod.get('pricing_model')
            pricing_tiers = prod.get('pricing_tiers', [])
            
            # If not in product, check global pricing data
            if not pricing_model or not pricing_tiers:
                pricing_data = pre_extracted.get('pricing', {})
                if not pricing_model:
                    pricing_model = pricing_data.get('model')
                if not pricing_tiers:
                    pricing_tiers = pricing_data.get('tiers', [])
            
            # Convert pricing_tiers to list of strings (handle dict format from scraper)
            pricing_tiers_public = []
            if pricing_tiers:
                for tier in pricing_tiers:
                    if isinstance(tier, dict):
                        # Extract name and price from dict
                        tier_name = tier.get('name', '')
                        tier_price = tier.get('price', '')
                        if tier_name and tier_price:
                            pricing_tiers_public.append(f"{tier_name}: {tier_price}")
                        elif tier_name:
                            pricing_tiers_public.append(tier_name)
                        elif tier_price:
                            pricing_tiers_public.append(tier_price)
                    elif isinstance(tier, str):
                        pricing_tiers_public.append(tier)
            
            product = Product(
                product_id=product_id,
                company_id=company_id,
                name=name,
                description=prod.get('description'),
                pricing_model=pricing_model,
                pricing_tiers_public=pricing_tiers_public,
                ga_date=ga_date,
                integration_partners=prod.get('integration_partners', []),
                github_repo=github_repo,
                license_type=prod.get('license_type'),
                reference_customers=prod.get('reference_customers', []),
                provenance=[Provenance(
                    source_url=prod.get('url', f"https://{company_id}.com/products"),
                    crawled_at=prod.get('date', datetime.now().isoformat()) if prod.get('date') else datetime.now().isoformat(),
                    snippet=f"Product: {name}"
                )]
            )
            products.append(product)
        except Exception as e:
            print(f"   âš ï¸  Failed to convert product {prod.get('name', 'unknown')}: {e}")
            continue
            
    return products


def clean_geo_presence(geo_list: List[str]) -> List[str]:
    """Clean geo presence list to only include actual city/location names."""
    if not geo_list:
        return []
    
    cleaned = []
    # Common city patterns
    city_patterns = [
        r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)$',  # "San Francisco", "New York"
        r'^([A-Z][a-z]+,\s*[A-Z][a-z]+)$',  # "San Francisco, CA"
    ]
    
    # Known major cities (to validate against)
    major_cities = [
        'San Francisco', 'New York', 'Los Angeles', 'Chicago', 'Seattle',
        'Boston', 'Austin', 'Denver', 'Miami', 'Atlanta', 'Dallas',
        'London', 'Paris', 'Berlin', 'Tokyo', 'Seoul', 'Singapore',
        'Toronto', 'Montreal', 'Vancouver', 'Sydney', 'Melbourne',
        'Mountain View', 'Palo Alto', 'Menlo Park', 'Redwood City',
        'San Jose', 'Santa Clara', 'Cupertino', 'Sunnyvale'
    ]
    
    for item in geo_list:
        if not item or not isinstance(item, str):
            continue
        
        item_clean = item.strip()
        
        # Skip if too short or too long
        if len(item_clean) < 3 or len(item_clean) > 50:
            continue
        
        item_lower = item_clean.lower()
        
        # Skip common false positives
        false_positives = [
            'office', 'location', 'headquarters', 'hq', 'offices', 'locations',
            'we have', 'our', 'the', 'in the', 'across', 'people', 'team',
            'represented', 'global', 'worldwide', 'international', 'global',
            'chief', 'executive', 'president', 'ceo', 'cto', 'cfo',
            'korea is', 'travel between', 'prefer pst', 'home', 'to the',
            'opens', 'new', 'the new', 'the paris', 'our paris', 'our montreal',
            'cohere', 'anthropic', 'speak', 'baseten', 'codeium',  # Company names
            'announces', 'announcing'  # Action words
        ]
        
        # Check if item starts with action words (e.g., "Announces Seoul")
        action_words = ['announces', 'announcing', 'opens', 'opening', 'launches', 'launching']
        if any(item_lower.startswith(aw + ' ') for aw in action_words):
            continue
        
        if any(fp in item_lower for fp in false_positives):
            continue
        
        # Check if it matches city pattern
        is_city = False
        for pattern in city_patterns:
            if re.match(pattern, item_clean):
                is_city = True
                break
        
        # Check if it's a known major city
        if any(city.lower() in item_lower for city in major_cities):
            # Extract the city name (prefer exact match)
            for city in major_cities:
                if city.lower() == item_lower:
                    # Exact match - add it
                    if city not in cleaned:
                        cleaned.append(city)
                    is_city = True
                    break
                elif city.lower() in item_lower:
                    # Partial match - only add if it's a clean match
                    # Check if city name is at the end or start of the string
                    if item_lower.endswith(city.lower()) or item_lower.startswith(city.lower()):
                        # Check if there are too many extra words
                        extra_chars = len(item_lower) - len(city.lower())
                        if extra_chars < 10:  # Allow small prefixes/suffixes like "New York" -> "New York City"
                            if city not in cleaned:
                                cleaned.append(city)
                            is_city = True
                            break
        
        # If it looks like a city and isn't a false positive, add it
        if is_city and item_clean not in cleaned:
            cleaned.append(item_clean)
    
    return cleaned


def clean_hq_city(hq_city: Optional[str]) -> Optional[str]:
    """Clean HQ city to remove surrounding words."""
    if not hq_city:
        return None
    
    # Remove common prefixes
    prefixes = ['our ', 'the ', 'in ', 'at ', 'from ']
    cleaned = hq_city.strip()
    
    for prefix in prefixes:
        if cleaned.lower().startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
    
    # Extract city name if it's in a phrase (e.g., "our Montreal" -> "Montreal")
    # Look for capitalized words (likely city names)
    words = cleaned.split()
    if len(words) > 1:
        # Find the first capitalized word that's likely a city
        for word in words:
            if word[0].isupper() and len(word) > 2:
                # Check if it's a known city pattern
                if re.match(r'^[A-Z][a-z]+$', word):
                    return word
    
    return cleaned


def clean_categories(categories: List[str]) -> List[str]:
    """Clean categories to only include actual industry categories."""
    if not categories:
        return []
    
    # Valid industry categories
    valid_categories = [
        'AI', 'Artificial Intelligence', 'Machine Learning', 'ML',
        'Infrastructure', 'Cloud', 'SaaS', 'Enterprise Software',
        'Developer Tools', 'DevOps', 'Security', 'Cybersecurity',
        'Data', 'Analytics', 'Business Intelligence', 'BI',
        'Legal Tech', 'EdTech', 'FinTech', 'HealthTech',
        'Robotics', 'Autonomous Vehicles', 'Hardware',
        'E-commerce', 'Marketplace', 'Consumer', 'B2B', 'B2C',
        'Gaming', 'Entertainment', 'Media', 'Content',
        'Transportation', 'Logistics', 'Supply Chain',
        'Energy', 'Climate', 'Sustainability', 'CleanTech'
    ]
    
    # Common false positives to filter out
    false_positives = [
        'case studies', 'newsroom', 'read full article', 'company',
        'security', 'pension investment board', 'david stewart',
        'agnostic', 'remote-friendly', 'about', 'contact', 'careers',
        'blog', 'press', 'news', 'resources', 'documentation',
        'pricing', 'features', 'products', 'solutions', 'services'
    ]
    
    cleaned = []
    for cat in categories:
        if not cat or not isinstance(cat, str):
            continue
        
        cat_clean = cat.strip()
        cat_lower = cat_clean.lower()
        
        # Skip false positives
        if cat_lower in false_positives:
            continue
        
        # Check if it matches a valid category
        if any(valid.lower() in cat_lower or cat_lower in valid.lower() for valid in valid_categories):
            # Find the matching valid category
            for valid in valid_categories:
                if valid.lower() in cat_lower or cat_lower in valid.lower():
                    if valid not in cleaned:
                        cleaned.append(valid)
                    break
        elif len(cat_clean) > 2 and len(cat_clean) < 30:
            # If it's a reasonable length and not a false positive, keep it
            if cat_clean not in cleaned:
                cleaned.append(cat_clean)
    
    return cleaned


def convert_pre_extracted_company_info(pre_extracted: Dict[str, Any], company_id: str) -> Dict[str, Any]:
    """Extract company info from pre-extracted entities - COMPREHENSIVE."""
    company_info = pre_extracted.get('company_info', {})
    
    # Parse headquarters - use separate fields if available, otherwise parse combined
    hq_city = company_info.get('hq_city')
    hq_state = company_info.get('hq_state')
    hq_country = company_info.get('hq_country')
    headquarters = company_info.get('headquarters')
    
    if not hq_city and headquarters:
        if isinstance(headquarters, str):
            # Try to parse "City, State, Country" format
            parts = [p.strip() for p in headquarters.split(',')]
            if len(parts) >= 1:
                hq_city = parts[0]
            if len(parts) >= 2:
                hq_state = parts[1]
            if len(parts) >= 3:
                hq_country = parts[2]
        elif isinstance(headquarters, list) and len(headquarters) > 0:
            # Handle list format
            hq_str = str(headquarters[0]) if isinstance(headquarters[0], str) else str(headquarters[0])
            parts = [p.strip() for p in hq_str.split(',')]
            if len(parts) >= 1:
                hq_city = parts[0]
    
    return {
        'legal_name': company_info.get('legal_name'),
        'brand_name': company_info.get('brand_name'),
        'founded_year': company_info.get('founded_year'),
        'hq_city': hq_city,
        'hq_state': hq_state,
        'hq_country': hq_country,
        'description': company_info.get('description'),
        'categories': company_info.get('categories', []),
        'related_companies': company_info.get('related_companies', [])
    }


# ============================================================================
# EXTRACTION FUNCTIONS - COMPREHENSIVE WITH ANTI-HALLUCINATION
# ============================================================================

def extract_funding_events(sources: Dict[str, Any], company_id: str) -> Tuple[List[Event], Dict]:
    """ZERO HALLUCINATION: Use pre-extracted entities first, then parse scraped HTML/text with strict LLM prompts."""
    
    # PRIORITY 1: Pre-extracted entities from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    if pre_extracted.get('funding_events'):
        print(f"   âœ… Using {len(pre_extracted['funding_events'])} pre-extracted funding events (NO HALLUCINATION)")
        events = convert_pre_extracted_funding_events(pre_extracted, company_id)
        
        # Calculate summary
        summary = {
            'total_raised_usd': sum(e.amount_usd or 0 for e in events) or None,
            'last_round_name': None,
            'last_round_date': None,
            'last_disclosed_valuation_usd': None
        }
        
        if events:
            dated = [e for e in events if e.occurred_on]
            if dated:
                recent = max(dated, key=lambda e: e.occurred_on)
                summary.update({
                    'last_round_name': recent.round_name,
                    'last_round_date': recent.occurred_on,
                    'last_disclosed_valuation_usd': recent.valuation_usd
                })
        
        return events, summary
    
    # PRIORITY 2: Parse scraped HTML/text files with strict LLM prompts (ONLY extract what's explicitly stated)
    print(f"   ðŸ” No pre-extracted funding events - parsing scraped HTML/text files (STRICT MODE - NO HALLUCINATION)")
    
    # Search for funding-related content in scraped files
    funding_keywords = ['funding', 'raised', 'series', 'round', 'investment', 'investor', 'valuation', 'million', 'billion']
    funding_context = search_all_sources(sources, funding_keywords, max_chars=8000)
    
    if not funding_context or len(funding_context.strip()) < 100:
        print(f"   âš ï¸  No funding content found in scraped files - returning empty (NOT DISCLOSED)")
        return [], {k: None for k in ['total_raised_usd', 'last_round_name', 'last_round_date', 'last_disclosed_valuation_usd']}

    # Use LLM to extract ONLY what's explicitly stated in the scraped content
    prompt = f"""Extract funding events from the scraped content below. 

ðŸš¨ CRITICAL RULES - ZERO HALLUCINATION:
- ONLY extract information that is EXPLICITLY stated in the text below
- DO NOT infer, guess, or use training data knowledge
- If amount is not stated â†’ amount_usd = null
- If date is not stated â†’ occurred_on = null (DO NOT use today's date as placeholder)
- If round name is not stated â†’ round_name = null
- **AGGRESSIVELY extract investors**: If text mentions investor names (e.g., "led by X", "from Y", "including Z"), extract ALL mentioned investor names
- **AGGRESSIVELY extract round names**: Look for "Series A/B/C", "Seed", "Pre-seed", "Angel", etc. in the text
- **AGGRESSIVELY extract valuations**: Look for "$X billion valuation", "$X million valuation", "valued at $X" patterns
- If valuation is not stated â†’ valuation_usd = null

SCRAPED CONTENT:
{funding_context[:8000]}

Return a JSON object with this structure:
{{
  "events": [
    {{
      "event_id": "{company_id}_funding_YYYY_MM_DD",
      "company_id": "{company_id}",
      "occurred_on": "YYYY-MM-DD",
      "event_type": "funding",
      "title": "Brief description",
      "round_name": "Series A" or null,
      "amount_usd": 50000000 or null,
      "valuation_usd": null,
      "investors": ["Investor Name"] or [],
      "actors": ["Investor Name"] or []
    }}
  ]
}}

Each event must have:
- event_id: "{company_id}_funding_YYYY_MM_DD" (use date from text, or "unknown" if no date)
- company_id: "{company_id}"
- occurred_on: Date from text (YYYY-MM-DD) or null if not found (DO NOT use today's date)
- event_type: "funding"
- title: Brief description (e.g., "Series A funding round")
- round_name: Series A/B/C/Seed/Pre-seed/Angel/etc. if mentioned in text, else null
- amount_usd: Integer amount if stated (e.g., $50M â†’ 50000000, $1.5B â†’ 1500000000), else null
- valuation_usd: Integer if stated (e.g., "$1B valuation" â†’ 1000000000), else null
- investors: List of ALL investor names mentioned in text (e.g., "led by X", "from Y", "including Z", "investors like A, B, C")
- actors: Same as investors

If NO funding events are found in the text â†’ return empty list []"""

    try:
        events = openai_client.chat.completions.create(
            model=model_name,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "You are a data extraction assistant. Extract ONLY information explicitly stated in the provided text. Do not infer or guess."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=2000
        )
        
        # Parse response
        response_text = events.choices[0].message.content
        response_data = json.loads(response_text)
        
        # Extract events list - try multiple possible keys
        events_list = []
        if isinstance(response_data, dict):
            events_list = response_data.get('events', response_data.get('funding_events', []))
        elif isinstance(response_data, list):
            events_list = response_data
        
        if not events_list:
            print(f"   âš ï¸  No funding events found in scraped content - returning empty (NOT DISCLOSED)")
            return [], {k: None for k in ['total_raised_usd', 'last_round_name', 'last_round_date', 'last_disclosed_valuation_usd']}
        
        # Convert to Event models
        parsed_events = []
        for event_data in events_list:
            try:
                # Parse date - be more lenient with date parsing
                occurred_on = None
                if event_data.get('occurred_on'):
                    try:
                        parsed_date = datetime.strptime(event_data['occurred_on'], '%Y-%m-%d').date()
                        # Only use if it's a reasonable date (not today's date as placeholder, and not in future)
                        today = date.today()
                        if parsed_date != today and parsed_date <= today:
                            occurred_on = parsed_date
                    except:
                        # Try parsing with dateutil for more flexible formats
                        try:
                            from dateutil import parser as date_parser
                            parsed_date = date_parser.parse(event_data['occurred_on']).date()
                            today = date.today()
                            if parsed_date != today and parsed_date <= today:
                                occurred_on = parsed_date
                        except:
                            pass
                
                # If still no date, try to extract from title or description
                if not occurred_on:
                    text_to_search = (event_data.get('title', '') + ' ' + event_data.get('description', '')).lower()
                    # Look for dates in text (e.g., "November 2022", "2022-11", "Nov 18, 2022")
                    date_patterns = [
                        r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
                        r'([A-Z][a-z]+ \d{1,2},? \d{4})',  # November 18, 2022
                        r'([A-Z][a-z]+ \d{4})',  # November 2022
                        r'(\d{4}-\d{2})',  # 2022-11
                    ]
                    for pattern in date_patterns:
                        match = re.search(pattern, text_to_search)
                        if match:
                            try:
                                from dateutil import parser as date_parser
                                parsed_date = date_parser.parse(match.group(1)).date()
                                today = date.today()
                                if parsed_date != today and parsed_date <= today:
                                    occurred_on = parsed_date
                                    break
                            except:
                                pass
                
                # If still no valid date, try to extract from description using more patterns
                if not occurred_on:
                    desc = event_data.get('description', '') or event_data.get('title', '')
                    # Look for common date patterns in funding announcements
                    date_patterns = [
                        r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
                        r'(\d{4}-\d{2}-\d{2})',
                        r'([A-Z][a-z]+ \d{4})',
                    ]
                    for pattern in date_patterns:
                        match = re.search(pattern, desc, re.IGNORECASE)
                        if match:
                            try:
                                from dateutil import parser as date_parser
                                parsed_date = date_parser.parse(match.group(0)).date()
                                today = date.today()
                                if parsed_date != today and parsed_date <= today:
                                    occurred_on = parsed_date
                                    break
                            except:
                                pass
                
                # If still no date, skip this event (don't use placeholder)
                if not occurred_on:
                    print(f"   âš ï¸  Skipping event with no valid date: {event_data.get('title', 'Unknown')}")
                    continue
                
                # Post-process: Extract investors from description if not already extracted
                investors = event_data.get('investors', []) or []
                if not investors and event_data.get('description'):
                    # Use more precise regex to find investor names in description
                    desc = event_data.get('description', '')
                    
                    # Known investor names to look for (common VCs)
                    known_investors = [
                        'OpenAI Startup Fund', 'Accel', 'Founders Fund', 'Khosla Ventures',
                        'Y Combinator', 'Sequoia', 'Andreessen Horowitz', 'a16z',
                        'Lachy Groom', 'Sam Altman', 'Peter Thiel', 'Paul Graham',
                        'Jeff Weiner', 'Buckley Ventures', 'Neo', 'GSV', 'Inovia Capital',
                        'Radical Ventures', 'AMD Ventures', 'NVIDIA', 'PSP Investment'
                    ]
                    
                    # First, try to find known investors by name
                    found_investors = []
                    for inv in known_investors:
                        if inv.lower() in desc.lower():
                            found_investors.append(inv)
                    
                    # If we found known investors, use those
                    if found_investors:
                        investors = found_investors
                    else:
                        # Otherwise, use regex patterns (more conservative)
                        investor_patterns = [
                            r'led by ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                            r'from ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                            r'investors? (?:like|including|such as) ([A-Z][a-zA-Z\s&,]+?)(?:,|\.|and|with|$)',
                            r'participation from ([A-Z][a-zA-Z\s&]+?)(?:,|\.|and|with|$)',
                        ]
                        for pattern in investor_patterns:
                            matches = re.findall(pattern, desc, re.IGNORECASE)
                            for match in matches:
                                # Clean up match (remove trailing words like "and", "with", etc.)
                                investor_name = match.strip().rstrip(',.and with')
                                # Filter out common false positives
                                if (investor_name and len(investor_name) > 2 and 
                                    len(investor_name) < 50 and  # Not too long
                                    investor_name.lower() not in ['the', 'this', 'that', 'round', 'funding', 'company', 'all existing', 'new strategic'] and
                                    investor_name not in investors):
                                    investors.append(investor_name)
                    
                    # Clean up investors list - remove duplicates and filter
                    investors = list(dict.fromkeys(investors))  # Remove duplicates while preserving order
                    investors = [inv for inv in investors if len(inv) > 2 and len(inv) < 50]
                
                # Post-process: Extract round name if not already extracted
                round_name = event_data.get('round_name')
                if not round_name and event_data.get('description'):
                    desc = event_data.get('description', '').lower()
                    round_patterns = [
                        r'series ([a-z])',
                        r'([a-z]+) round',
                        r'([a-z]+) funding',
                    ]
                    for pattern in round_patterns:
                        match = re.search(pattern, desc)
                        if match:
                            round_str = match.group(1) if match.group(1) != 'a' else 'Series A'
                            if 'series' in desc:
                                round_name = f"Series {match.group(1).upper()}" if len(match.group(1)) == 1 else match.group(1).title()
                            else:
                                round_name = match.group(1).title()
                            break
                
                # Post-process: Extract valuation if not already extracted
                valuation_usd = event_data.get('valuation_usd')
                if not valuation_usd and event_data.get('description'):
                    desc = event_data.get('description', '')
                    # Look for "$X billion valuation", "$X million valuation", "valued at $X"
                    valuation_patterns = [
                        r'\$([\d.]+)\s*billion\s*valuation',
                        r'\$([\d.]+)\s*million\s*valuation',
                        r'valued at \$([\d.]+)\s*(billion|million)',
                        r'valuation of \$([\d.]+)\s*(billion|million)',
                    ]
                    for pattern in valuation_patterns:
                        match = re.search(pattern, desc, re.IGNORECASE)
                        if match:
                            amount = float(match.group(1))
                            unit = match.group(2) if len(match.groups()) > 1 else (match.group(2) if 'billion' in desc.lower() else 'million')
                            if 'billion' in unit.lower() or 'billion' in desc.lower():
                                valuation_usd = int(amount * 1_000_000_000)
                            else:
                                valuation_usd = int(amount * 1_000_000)
                            break
                
                # Find the best source URL for this funding event
                # Try to find blog post or page that contains this funding info
                best_source_url = None
                best_crawled_at = datetime.now().isoformat()
                
                # Check if funding context mentions a blog post
                for blog in sources.get('blog_posts', []):
                    if any(kw in blog['content'].lower()[:500] for kw in ['funding', 'raised', 'series', 'round']):
                        if blog.get('url'):
                            best_source_url = blog['url']
                            # Get crawled_at from blog URL mapping
                            blog_id = blog.get('id', '')
                            if blog_id in sources.get('blog_url_mapping', {}):
                                best_crawled_at = sources['blog_url_mapping'][blog_id].get('crawled_at', best_crawled_at)
                            break
                
                # Fallback to homepage or about page
                if not best_source_url:
                    url_mapping = sources.get('url_mapping', {})
                    if 'homepage' in url_mapping:
                        best_source_url = url_mapping['homepage']['source_url']
                        best_crawled_at = url_mapping['homepage']['crawled_at']
                    elif 'about' in url_mapping:
                        best_source_url = url_mapping['about']['source_url']
                        best_crawled_at = url_mapping['about']['crawled_at']
                    else:
                        best_source_url = f"https://{company_id}.com"
                
                event = Event(
                    event_id=event_data.get('event_id', f"{company_id}_funding_{occurred_on.strftime('%Y_%m_%d')}"),
                    company_id=company_id,
                    occurred_on=occurred_on,
                    event_type="funding",
                    title=event_data.get('title', 'Funding round'),
                    round_name=event_data.get('round_name'),
                    amount_usd=event_data.get('amount_usd'),
                    valuation_usd=event_data.get('valuation_usd'),
                    investors=event_data.get('investors', []),
                    actors=event_data.get('actors', event_data.get('investors', [])),
                    provenance=[Provenance(
                        source_url=best_source_url,
                        crawled_at=best_crawled_at,
                        snippet=funding_context[:200] if funding_context else None
                    )]
                )
                parsed_events.append(event)
            except ValidationError as e:
                print(f"   âš ï¸  Skipping invalid event: {e}")
                continue
        
        if parsed_events:
            print(f"   âœ… Extracted {len(parsed_events)} funding events from scraped content (STRICT MODE)")
            summary = {
                'total_raised_usd': sum(e.amount_usd or 0 for e in parsed_events) or None,
                'last_round_name': None,
                'last_round_date': None,
                'last_disclosed_valuation_usd': None
            }
            if parsed_events:
                dated = [e for e in parsed_events if e.occurred_on]
                if dated:
                    recent = max(dated, key=lambda e: e.occurred_on)
                    summary.update({
                        'last_round_name': recent.round_name,
                        'last_round_date': recent.occurred_on,
                        'last_disclosed_valuation_usd': recent.valuation_usd
                    })
            return parsed_events, summary
        else:
            print(f"   âš ï¸  No valid funding events extracted - returning empty (NOT DISCLOSED)")
            return [], {k: None for k in ['total_raised_usd', 'last_round_name', 'last_round_date', 'last_disclosed_valuation_usd']}
            
    except Exception as e:
        print(f"   âš ï¸  Error parsing funding events: {e} - returning empty (NOT DISCLOSED)")
        return [], {k: None for k in ['total_raised_usd', 'last_round_name', 'last_round_date', 'last_disclosed_valuation_usd']}


def extract_leadership(sources: Dict[str, Any], company_id: str) -> List[Leadership]:
    """ZERO HALLUCINATION: Use pre-extracted entities first, then parse scraped HTML/text with strict LLM prompts."""
    
    # PRIORITY 1: Pre-extracted entities from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    if pre_extracted.get('team_members'):
        print(f"   âœ… Using {len(pre_extracted['team_members'])} pre-extracted team members (NO HALLUCINATION)")
        return convert_pre_extracted_leadership(pre_extracted, company_id)
    
    # PRIORITY 2: Parse scraped HTML/text files with strict LLM prompts (ONLY extract what's explicitly stated)
    print(f"   ðŸ” No pre-extracted team members - parsing scraped HTML/text files (STRICT MODE - NO HALLUCINATION)")
    
    # Search for leadership-related content
    leadership_keywords = ['team', 'leadership', 'founder', 'ceo', 'cto', 'executive', 'management', 'about us', 'our team']
    leadership_context = search_all_sources(sources, leadership_keywords, max_chars=8000)
    
    if not leadership_context or len(leadership_context.strip()) < 100:
        print(f"   âš ï¸  No leadership content found in scraped files - returning empty (NOT DISCLOSED)")
        return []
    
    # Use LLM to extract ONLY what's explicitly stated
    prompt = f"""Extract leadership/team members from the scraped content below.

ðŸš¨ CRITICAL RULES - ZERO HALLUCINATION:
- ONLY extract names and roles that are EXPLICITLY stated in the text
- DO NOT infer, guess, or use training data knowledge
- If name is not clear â†’ skip that person
- If role is not stated â†’ role = null
- If LinkedIn URL is not stated â†’ linkedin = null
- If founder status is not explicitly stated â†’ is_founder = false

SCRAPED CONTENT:
{leadership_context[:8000]}

Return a JSON object with this structure:
{{
  "leadership": [
    {{
      "name": "Full Name",
      "role": "CEO" or null,
      "is_founder": true or false,
      "linkedin": "https://linkedin.com/..." or null
    }}
  ]
}}

Each member must have:
- person_id: "{company_id}_" + lowercase_name_with_underscores
- company_id: "{company_id}"
- name: Full name as stated in text
- role: Job title/role if stated (e.g., "CEO", "CTO", "Founder"), else null
- is_founder: true ONLY if explicitly stated as "founder" or "co-founder", else false
- linkedin: LinkedIn URL if stated, else null
- start_date: null (not extracting dates)
- end_date: null
- previous_affiliation: null
- education: null

If NO leadership members are found â†’ return empty list []"""
    
    try:
        response = openai_client.chat.completions.create(
            model=model_name,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "You are a data extraction assistant. Extract ONLY information explicitly stated in the provided text. Do not infer or guess."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=2000
        )
        
        response_text = response.choices[0].message.content
        response_data = json.loads(response_text)
        
        # Extract members list - try multiple possible keys
        members_list = []
        if isinstance(response_data, dict):
            members_list = response_data.get('leadership', response_data.get('team_members', response_data.get('members', [])))
        elif isinstance(response_data, list):
            members_list = response_data
        
        if not members_list:
            print(f"   âš ï¸  No leadership members found in scraped content - returning empty (NOT DISCLOSED)")
            return []
        
        # Convert to Leadership models
        parsed_leadership = []
        
        # False positive keywords to filter out
        false_positive_keywords = [
            'api', 'apis', 'experience', 'cloud', 'reviews', 'model', 'developer',
            'product', 'platform', 'service', 'tool', 'system', 'solution',
            'editor', 'plugin', 'software', 'application', 'framework',
            'agentic', 'ai', 'artificial intelligence'
        ]
        
        def is_false_positive(name: str) -> bool:
            """Check if name looks like a false positive (product/service name, not person)."""
            if not name or ' ' not in name:
                return True
            
            name_lower = name.lower()
            words = name_lower.split()
            
            # Check if any word matches false positive patterns
            for word in words:
                if word in false_positive_keywords:
                    return True
            
            # Check if it's clearly a product/service name pattern
            if any(pattern in name_lower for pattern in [
                'model api', 'developer experience', 'windsurf review',
                'agentic ai', 'artificial intelligence'
            ]):
                return True
            
            # Check if it's a two-word phrase where both words are technical terms
            if len(words) == 2:
                if all(word in false_positive_keywords for word in words):
                    return True
            
            return False
        
        for member_data in members_list:
            try:
                name = member_data.get('name', '')
                
                # Filter out false positives
                if is_false_positive(name):
                    print(f"   âš ï¸  Filtered false positive leadership: {name}")
                    continue
                
                # Generate person_id
                name_lower = name.lower().replace(' ', '_').replace('-', '_')
                person_id = f"{company_id}_{name_lower}" if name_lower else f"{company_id}_person_{len(parsed_leadership)}"
                
                # Role is required, use 'Executive' as fallback if not stated
                role = member_data.get('role')
                if not role or not role.strip():
                    role = 'Executive'  # Default fallback
                
                leadership = Leadership(
                    person_id=person_id,
                    company_id=company_id,
                    name=member_data.get('name', ''),
                    role=role,
                    is_founder=member_data.get('is_founder', False),
                    linkedin=member_data.get('linkedin'),
                    provenance=[Provenance(
                        source_url="https://internal/scraped_content",
                        crawled_at=datetime.now().isoformat(),
                        snippet=leadership_context[:200]
                    )]
                )
                parsed_leadership.append(leadership)
            except ValidationError as e:
                print(f"   âš ï¸  Skipping invalid leadership member: {e}")
                continue
        
        if parsed_leadership:
            print(f"   âœ… Extracted {len(parsed_leadership)} leadership members from scraped content (STRICT MODE)")
            return parsed_leadership
        else:
            print(f"   âš ï¸  No valid leadership members extracted - returning empty (NOT DISCLOSED)")
            return []
            
    except Exception as e:
        print(f"   âš ï¸  Error parsing leadership: {e} - returning empty (NOT DISCLOSED)")
        return []


def extract_products(sources: Dict[str, Any], company_id: str) -> List[Product]:
    """ZERO HALLUCINATION: Use pre-extracted entities first, then parse scraped HTML/text with strict LLM prompts."""
    
    # PRIORITY 1: Pre-extracted entities from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    if pre_extracted.get('products'):
        print(f"   âœ… Using {len(pre_extracted['products'])} pre-extracted products (NO HALLUCINATION)")
        return convert_pre_extracted_products(pre_extracted, company_id)
    
    # PRIORITY 2: Parse scraped HTML/text files with strict LLM prompts (ONLY extract what's explicitly stated)
    print(f"   ðŸ” No pre-extracted products - parsing scraped HTML/text files (STRICT MODE - NO HALLUCINATION)")
    
    # Search for product-related content
    product_keywords = ['product', 'platform', 'solution', 'tool', 'service', 'api', 'software', 'app', 'feature']
    product_context = search_all_sources(sources, product_keywords, max_chars=8000)
    
    if not product_context or len(product_context.strip()) < 100:
        print(f"   âš ï¸  No product content found in scraped files - returning empty (NOT DISCLOSED)")
        return []
    
    # Use LLM to extract ONLY what's explicitly stated
    prompt = f"""Extract products from the scraped content below.

ðŸš¨ CRITICAL RULES - ZERO HALLUCINATION:
- ONLY extract products that are EXPLICITLY mentioned as products/services/platforms
- DO NOT include website pages like "Blog", "About", "Careers", "Press", "Resources"
- DO NOT infer, guess, or use training data knowledge
- If product name is not clear â†’ skip it
- If description is not stated â†’ description = null
- If GitHub URL is not stated â†’ github_repo = null
- If pricing is not stated â†’ pricing_model = null, pricing_tiers_public = []

SCRAPED CONTENT:
{product_context[:8000]}

Return a JSON object with this structure:
{{
  "products": [
    {{
      "name": "Product Name",
      "description": "Brief description" or null,
      "pricing_model": "seat" or null,
      "pricing_tiers_public": ["Free", "Pro"] or [],
      "github_repo": "https://github.com/..." or null
    }}
  ]
}}

Each product must have:
- product_id: "{company_id}_" + lowercase_product_name_with_underscores
- company_id: "{company_id}"
- name: Product name as stated
- description: Brief description if stated, else null
- pricing_model: "seat", "usage", "tiered", or null if not stated
- pricing_tiers_public: List of pricing tiers if stated, else []
- ga_date: null (not extracting dates)
- integration_partners: []
- github_repo: GitHub URL if stated, else null
- license_type: null
- reference_customers: []

If NO products are found â†’ return empty list []"""
    
    try:
        response = openai_client.chat.completions.create(
            model=model_name,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "You are a data extraction assistant. Extract ONLY information explicitly stated in the provided text. Do not infer or guess. Exclude website pages like Blog, About, Careers."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=2000
        )
        
        response_text = response.choices[0].message.content
        response_data = json.loads(response_text)
        
        # Extract products list - try multiple possible keys
        products_list = []
        if isinstance(response_data, dict):
            products_list = response_data.get('products', [])
        elif isinstance(response_data, list):
            products_list = response_data
        
        if not products_list:
            print(f"   âš ï¸  No products found in scraped content - returning empty (NOT DISCLOSED)")
            return []
        
        # Convert to Product models
        parsed_products = []
        for product_data in products_list:
            try:
                # Generate product_id
                name_lower = product_data.get('name', '').lower().replace(' ', '_').replace('-', '_')
                product_id = f"{company_id}_{name_lower}" if name_lower else f"{company_id}_product_{len(parsed_products)}"
                
                product = Product(
                    product_id=product_id,
                    company_id=company_id,
                    name=product_data.get('name', ''),
                    description=product_data.get('description'),
                    pricing_model=product_data.get('pricing_model'),
                    pricing_tiers_public=product_data.get('pricing_tiers_public', []),
                    github_repo=product_data.get('github_repo'),
                    provenance=[Provenance(
                        source_url="https://internal/scraped_content",
                        crawled_at=datetime.now().isoformat(),
                        snippet=product_context[:200]
                    )]
                )
                parsed_products.append(product)
            except ValidationError as e:
                print(f"   âš ï¸  Skipping invalid product: {e}")
                continue
        
        if parsed_products:
            print(f"   âœ… Extracted {len(parsed_products)} products from scraped content (STRICT MODE)")
            return parsed_products
        else:
            print(f"   âš ï¸  No valid products extracted - returning empty (NOT DISCLOSED)")
            return []
            
    except Exception as e:
        print(f"   âš ï¸  Error parsing products: {e} - returning empty (NOT DISCLOSED)")
        return []


def extract_company_record(sources: Dict[str, Any], company_id: str, funding_summary: Dict) -> Company:
    """ZERO HALLUCINATION: Use ONLY pre-extracted company info from scraper. Use JSON-LD/Forbes as fallback only."""
    
    # PRIORITY 1: Use pre-extracted company info from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    pre_extracted_company_info = None
    if pre_extracted.get('company_info'):
        print(f"   âœ… Using pre-extracted company info (NO HALLUCINATION)")
        pre_extracted_company_info = convert_pre_extracted_company_info(pre_extracted, company_id)
    
    # Get founding date from JSON-LD (allowed - it's structured data, not LLM)
    jsonld_founding = get_jsonld_value(sources, 'foundingDate')
    founded_year = None
    
    if jsonld_founding:
        try:
            year = int(jsonld_founding.split('-')[0])
            if 1990 <= year <= 2023:
                founded_year = year
                print(f"   âœ“ Founded year from JSON-LD: {year}")
        except:
            pass
    
    # Aggressive text search if JSON-LD didn't have it (allowed - it's regex, not LLM)
    if not founded_year:
        founded_year = extract_founded_year_aggressive(sources)
    
    # Get legal name from JSON-LD (allowed - structured data)
    jsonld_name = get_jsonld_value(sources, 'legalName') or get_jsonld_value(sources, 'name')
    
    # Get actual website URL from metadata
    website_url = f"https://{company_id}.com"
    url_mapping = sources.get('url_mapping', {})
    if 'homepage' in url_mapping:
        website_url = url_mapping['homepage']['source_url']
    elif 'about' in url_mapping:
        # Extract base URL from about page
        about_url = url_mapping['about']['source_url']
        if '/' in about_url:
            website_url = '/'.join(about_url.split('/')[:3])  # Get https://domain.com
    
    # Get scrape date from metadata for as_of field
    scrape_date = date.today()
    if 'metadata' in sources and sources['metadata'].get('scrape_timestamp'):
        try:
            from dateutil import parser as date_parser
            scrape_dt = date_parser.parse(sources['metadata']['scrape_timestamp'])
            scrape_date = scrape_dt.date()
        except:
            pass
    
    # Build company record from available sources (NO LLM)
    company = Company(
        company_id=company_id,
        legal_name=jsonld_name or company_id.title(),
        website=website_url,
        founded_year=founded_year,
        as_of=scrape_date,
        total_raised_usd=funding_summary.get('total_raised_usd'),
        last_round_name=funding_summary.get('last_round_name'),
        last_round_date=funding_summary.get('last_round_date'),
        last_disclosed_valuation_usd=funding_summary.get('last_disclosed_valuation_usd'),
        provenance=create_provenance(sources, ['homepage', 'about'])
    )
    
    # Override with pre-extracted data (highest priority - from scraper)
    if pre_extracted_company_info:
        if pre_extracted_company_info.get('founded_year'):
            company.founded_year = pre_extracted_company_info['founded_year']
            print(f"   âœ“ Founded year from pre-extracted: {company.founded_year}")
        
        if pre_extracted_company_info.get('hq_city'):
            company.hq_city = clean_hq_city(pre_extracted_company_info['hq_city'])
            print(f"   âœ“ HQ city from pre-extracted: {company.hq_city}")
        
        if pre_extracted_company_info.get('hq_state'):
            company.hq_state = pre_extracted_company_info['hq_state']
            print(f"   âœ“ HQ state from pre-extracted: {company.hq_state}")
        
        if pre_extracted_company_info.get('hq_country'):
            company.hq_country = pre_extracted_company_info['hq_country']
            print(f"   âœ“ HQ country from pre-extracted: {company.hq_country}")
        
        if pre_extracted_company_info.get('categories'):
            company.categories = clean_categories(pre_extracted_company_info['categories'])
            print(f"   âœ“ Categories from pre-extracted: {company.categories}")
    
    # FALLBACK: FORBES SEED (NO INFERENCE - ONLY DIRECT OVERRIDE)
    forbes_seed = sources.get('forbes_seed', {})
    
    if forbes_seed:
        print(f"   âœ“ Using Forbes seed as fallback source")
        
        # Override nulls with Forbes data (NO INFERENCE)
        if not company.hq_city and forbes_seed.get('hq_city'):
            company.hq_city = forbes_seed['hq_city']
            print(f"   âœ“ HQ city from Forbes: {company.hq_city}")
        
        if not company.hq_country and forbes_seed.get('hq_country'):
            company.hq_country = forbes_seed['hq_country']
            print(f"   âœ“ HQ country from Forbes: {company.hq_country}")
        
        if not company.categories and forbes_seed.get('category'):
            # Forbes has single category string, convert to list
            company.categories = [forbes_seed['category']]
            print(f"   âœ“ Category from Forbes: {company.categories}")
        
        if not company.founded_year and forbes_seed.get('founded_year'):
            # Validate year from Forbes
            year = forbes_seed['founded_year']
            if isinstance(year, int) and 1990 <= year <= 2023:
                company.founded_year = year
                print(f"   âœ“ Founded year from Forbes: {company.founded_year}")
    
    return company


def extract_snapshot(sources: Dict[str, Any], company_id: str, products: List[Product]) -> Snapshot:
    """ZERO HALLUCINATION: Extract snapshot ONLY from pre-extracted scraper data."""
    
    # Get scrape date from metadata for as_of field
    scrape_date = date.today()
    if 'metadata' in sources and sources['metadata'].get('scrape_timestamp'):
        try:
            from dateutil import parser as date_parser
            scrape_dt = date_parser.parse(sources['metadata']['scrape_timestamp'])
            scrape_date = scrape_dt.date()
        except:
            pass
    
    # PRIORITY 1: Use pre-extracted snapshot data from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    snapshot_data = pre_extracted.get('snapshot_data', {})
    pricing_data = pre_extracted.get('pricing', {})
    
    # Convert pricing_tiers to list of strings (handle dict format from scraper)
    pricing_tiers_raw = pricing_data.get('tiers', [])
    pricing_tiers = []
    if pricing_tiers_raw:
        for tier in pricing_tiers_raw:
            if isinstance(tier, dict):
                # Extract name and price from dict
                tier_name = tier.get('name', '')
                tier_price = tier.get('price', '')
                if tier_name and tier_price:
                    pricing_tiers.append(f"{tier_name}: {tier_price}")
                elif tier_name:
                    pricing_tiers.append(tier_name)
                elif tier_price:
                    pricing_tiers.append(tier_price)
            elif isinstance(tier, str):
                pricing_tiers.append(tier)
    
    # Calculate job count from jobs list if not in snapshot_data
    job_openings_count = snapshot_data.get('job_openings_count')
    if not job_openings_count:
        jobs = pre_extracted.get('jobs', [])
        if jobs:
            job_openings_count = len(jobs)
            print(f"   âœ“ Calculated job openings from jobs list: {job_openings_count}")
    
    # Build snapshot from pre-extracted data
    snapshot = Snapshot(
        company_id=company_id,
        as_of=scrape_date,
        headcount_total=snapshot_data.get('headcount_total'),
        headcount_growth_pct=snapshot_data.get('headcount_growth_pct'),
        job_openings_count=job_openings_count,
        engineering_openings=snapshot_data.get('engineering_openings'),
        sales_openings=snapshot_data.get('sales_openings'),
        hiring_focus=snapshot_data.get('hiring_focus', []),
        pricing_tiers=pricing_tiers,
        active_products=[p.name for p in products],
        geo_presence=clean_geo_presence(snapshot_data.get('geo_presence', [])),
        provenance=create_provenance(sources, ['careers', 'homepage'],
            snippet=f"Headcount: {snapshot_data.get('headcount_total')}, Openings: {job_openings_count}")
    )
    
    # If we have pre-extracted data, use it (NO LLM)
    if snapshot_data or pricing_data:
        print(f"   âœ… Using pre-extracted snapshot data (NO HALLUCINATION)")
        return snapshot
    
    # FALLBACK: Parse from scraped files (ONLY if no pre-extracted data)
    print(f"   ðŸ” No pre-extracted snapshot data - parsing scraped files (STRICT MODE)")
    
    # Extract from jobs count
    jobs = pre_extracted.get('jobs', [])
    if jobs and not snapshot.job_openings_count:
        snapshot.job_openings_count = len(jobs)
    
    # Extract from HTML structured data
    html_locations = []
    for page_type, html_struct in sources.get('html_structured', {}).items():
        if 'locations' in html_struct:
            html_locations.extend(html_struct['locations'])
        if 'headcount' in html_struct and not snapshot.headcount_total:
            snapshot.headcount_total = html_struct['headcount']
        if 'job_openings' in html_struct and not snapshot.job_openings_count:
            snapshot.job_openings_count = html_struct['job_openings']
        if 'engineering_openings' in html_struct and not snapshot.engineering_openings:
            snapshot.engineering_openings = html_struct['engineering_openings']
        if 'sales_openings' in html_struct and not snapshot.sales_openings:
            snapshot.sales_openings = html_struct['sales_openings']
    
    if html_locations:
        snapshot.geo_presence = list(set(html_locations))
    
    # Extract pricing tiers from pricing data
    if pricing_data.get('tiers') and not snapshot.pricing_tiers:
        snapshot.pricing_tiers = pricing_data['tiers']
    
    # Active products from extracted products
    if products:
        snapshot.active_products = [p.name for p in products]
    
    return snapshot


def extract_other_events(sources: Dict[str, Any], company_id: str) -> List[Event]:
    """IMPROVED: Event extraction with RISK and OUTLOOK tagging from scraped sources ONLY."""
    
    timeline = get_structured_timeline(sources, 'all')
    
    # Build comprehensive context for ALL event types
    all_event_keywords = []
    for event_type in ['partnerships', 'launches', 'mna', 'integration', 'customer_win',
                       'regulatory', 'security_incident', 'pricing_change', 'layoff',
                       'hiring_spike', 'office_open', 'office_close', 'benchmark',
                       'open_source', 'contract_award']:
        all_event_keywords.extend(FIELD_KEYWORDS.get(event_type, []))
    
    event_context = search_all_sources(sources, all_event_keywords, max_chars=6000)
    
    # NEW: Search for risk factors in scraped text
    risk_keywords = [
        'risk', 'challenge', 'concern', 'issue', 'problem', 'setback',
        'investigation', 'lawsuit', 'litigation', 'complaint', 'scrutiny',
        'controversy', 'criticism', 'backlash', 'delay', 'regulatory action'
    ]
    risk_context = search_all_sources(sources, risk_keywords, max_chars=3000)
    
    # NEW: Search for outlook/forward-looking statements in scraped text
    outlook_keywords = [
        'plans to', 'will', 'expects', 'forecasts', 'outlook', 'guidance',
        'projects', 'anticipates', 'intends to', 'aims to', 'goals', 'roadmap',
        'future', 'upcoming', 'next year', 'expansion plans', 'strategy'
    ]
    outlook_context = search_all_sources(sources, outlook_keywords, max_chars=3000)
    has_scraped_data = (
        len(sources.get('files', {})) > 0 or 
        len(sources.get('html_files', {})) > 0 or
        len(sources.get('press_releases', [])) > 0
    )
    if not has_scraped_data:
        print(f"   âš ï¸  No scraped data found - returning empty events")
        return []
    
    
    context = f"""EVENT TIMELINE WITH DATES (ONLY SOURCE OF TRUTH):
{timeline}

EVENT DETAILS (from all sources):
{event_context}

RISK FACTORS & CHALLENGES (tag as risk_factor if mentioned):
{risk_context}

OUTLOOK & FORWARD-LOOKING STATEMENTS (tag as outlook_statement if mentioned):
{outlook_context}"""
    
    prompt = f"""Extract ALL company events for {company_id} (NOT funding).

ðŸš¨ ZERO HALLUCINATION + STRICT TIMELINE VALIDATION ðŸš¨

The TIMELINE above from press releases is your ONLY SOURCE OF TRUTH for dates.

Extract ALL event types:
1. product_release: Product launches
2. mna: Mergers & acquisitions ("acquires", "acquired by", "merger")
3. integration: Technology integrations ("integrates with X")
4. partnership: Business partnerships
5. customer_win: Major customer signings ("signs", "contract with")
6. leadership_change: Executive appointments/departures
7. regulatory: Compliance certifications ("SOC2", "HIPAA", "ISO")
8. security_incident: Security breaches or incidents
9. pricing_change: Pricing updates
10. layoff: Workforce reductions
11. hiring_spike: Rapid hiring announcements
12. office_open: New office openings
13. office_close: Office closures
14. benchmark: Performance benchmarks published
15. open_source_release: OSS releases
16. contract_award: Contract wins (esp. government)
17. other: Anything else significant

For EACH event:
- event_id: "{company_id}_{{type}}_{{title_slug}}_{{YYYY}}_{{MM}}_{{DD}}" (MUST be UNIQUE with full date)
- company_id: "{company_id}"
- occurred_on: VALID date from TIMELINE (YYYY-MM-DD) - REQUIRED
  * âŒ If no date in timeline â†’ DO NOT extract that event
  * âŒ DO NOT use "2024-XX-XX" or placeholders
  * âœ… ONLY use dates that appear in TIMELINE
- event_type: Choose correct type from list above
- title: Brief title (from timeline preferred)
- description: Full details (or null)
- actors: Companies/people involved (or empty list)
- tags: Relevant tags (CRITICAL - see below)
- amount_usd: Dollar amount if applicable (or null)

ðŸŽ¯ TAGGING RULES (CRITICAL):
Use tags to categorize events based on SCRAPED TEXT ONLY:

RISK FACTORS:
- IF event mentions risks/challenges/problems in RISK FACTORS section above
- THEN add: ["risk_factor"] or ["risk_factor", "legal"] or ["risk_factor", "regulatory"]
- Example: "Investigation announced" â†’ tags: ["risk_factor", "legal"]
- âŒ DO NOT tag as risk if not explicitly mentioned as risk/challenge

OUTLOOK STATEMENTS:
- IF event mentions future plans/expectations in OUTLOOK section above
- THEN add: ["outlook_statement"] or ["outlook_statement", "expansion"]
- Example: "Plans to expand to Europe" â†’ tags: ["outlook_statement", "expansion"]
- âŒ DO NOT tag as outlook if not forward-looking

REGULATORY:
- IF regulatory/compliance event â†’ tags: ["regulatory", "compliance"]
- Example: "Achieves SOC2" â†’ tags: ["regulatory", "compliance"]

OTHER CATEGORIES:
- Use descriptive tags: ["strategic", "international"], ["AI", "research"], etc.
- ONLY use tags if explicitly supported by text

EXAMPLES:
âœ… Text says "investigation into practices" â†’ tags: ["risk_factor", "legal"]
âœ… Text says "plans to launch in 2025" â†’ tags: ["outlook_statement", "expansion"]
âœ… Text says "achieves SOC2 certification" â†’ tags: ["regulatory", "compliance"]
âŒ Product launch with no risk mentioned â†’ tags: [] (not ["risk_factor"])
âŒ Event with no outlook mentioned â†’ tags: [] (not ["outlook_statement"])

CRITICAL TIMELINE VALIDATION:
- Event MUST appear in TIMELINE with a real date
- If event is NOT in timeline â†’ SKIP IT ENTIRELY
- ONLY extract events explicitly stated in timeline
- event_id MUST be unique (use full date YYYY_MM_DD)

{context}"""
    
    try:
        events = client.chat.completions.create(
            model=model_name,
            response_model=List[Event],
            messages=[
                {"role": "system", "content": "Extract ALL event types from TIMELINE ONLY. Tag risks and outlook based on SCRAPED TEXT ONLY. STRICT validation. NO placeholders. NO hallucinated events."},
                {"role": "user", "content": prompt}
            ],
            max_retries=3
        )
        
        valid_events = []
        seen_ids = set()
        
        for event in events:
            if event.occurred_on:
                date_str = str(event.occurred_on)
                if 'XX' in date_str or 'xx' in date_str:
                    print(f"   âš ï¸  Filtered placeholder date: {date_str}")
                    continue
            else:
                print(f"   âš ï¸  Skipped event with no date: {event.title}")
                continue
            
            # IMPROVED: Ensure unique event_id with full date
            if event.event_id in seen_ids:
                month = str(event.occurred_on.month).zfill(2)
                day = str(event.occurred_on.day).zfill(2)
                title_slug = re.sub(r'[^a-z0-9]+', '_', event.title.lower())[:30]
                event.event_id = f"{company_id}_{event.event_type}_{title_slug}_{event.occurred_on.year}_{month}_{day}"
                print(f"   âš ï¸  Regenerated unique ID: {event.event_id}")
            
            seen_ids.add(event.event_id)
            
            # Log tagging
            if event.tags:
                if 'risk_factor' in event.tags:
                    print(f"   âœ“ Tagged as RISK: {event.title}")
                if 'outlook_statement' in event.tags:
                    print(f"   âœ“ Tagged as OUTLOOK: {event.title}")
            
            event.provenance = create_provenance(sources, ['press', 'homepage'],
                snippet=f"{event.event_type}: {event.title}")
            
            valid_events.append(event)
        
        # Summary stats
        risk_events = [e for e in valid_events if e.tags and 'risk_factor' in e.tags]
        outlook_events = [e for e in valid_events if e.tags and 'outlook_statement' in e.tags]
        
        if risk_events:
            print(f"   âœ“ Extracted {len(risk_events)} risk factors")
        if outlook_events:
            print(f"   âœ“ Extracted {len(outlook_events)} outlook statements")
        
        return valid_events
        
    except ValidationError as e:
        print(f"   âš ï¸  Pydantic validation failed: {e}")
        return []
    except Exception as e:
        print(f"   âš ï¸  Events failed: {e}")
        return []


def extract_company_record(sources: Dict[str, Any], company_id: str, funding_summary: Dict) -> Company:
    """ZERO HALLUCINATION: Use ONLY pre-extracted company info from scraper. Use JSON-LD/Forbes as fallback only."""
    
    # PRIORITY 1: Use pre-extracted company info from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    pre_extracted_company_info = None
    if pre_extracted.get('company_info'):
        print(f"   âœ… Using pre-extracted company info (NO HALLUCINATION)")
        pre_extracted_company_info = convert_pre_extracted_company_info(pre_extracted, company_id)
    
    # Get founding date from JSON-LD (allowed - it's structured data, not LLM)
    jsonld_founding = get_jsonld_value(sources, 'foundingDate')
    founded_year = None
    
    if jsonld_founding:
        try:
            year = int(jsonld_founding.split('-')[0])
            if 1990 <= year <= 2023:
                founded_year = year
                print(f"   âœ“ Founded year from JSON-LD: {year}")
        except:
            pass
    
    # Aggressive text search if JSON-LD didn't have it (allowed - it's regex, not LLM)
    if not founded_year:
        founded_year = extract_founded_year_aggressive(sources)
    
    # Get legal name from JSON-LD (allowed - structured data)
    jsonld_name = get_jsonld_value(sources, 'legalName') or get_jsonld_value(sources, 'name')
    
    # Get actual website URL from metadata
    website_url = f"https://{company_id}.com"
    url_mapping = sources.get('url_mapping', {})
    if 'homepage' in url_mapping:
        website_url = url_mapping['homepage']['source_url']
    elif 'about' in url_mapping:
        # Extract base URL from about page
        about_url = url_mapping['about']['source_url']
        if '/' in about_url:
            website_url = '/'.join(about_url.split('/')[:3])  # Get https://domain.com
    
    # Get scrape date from metadata for as_of field
    scrape_date = date.today()
    if 'metadata' in sources and sources['metadata'].get('scrape_timestamp'):
        try:
            from dateutil import parser as date_parser
            scrape_dt = date_parser.parse(sources['metadata']['scrape_timestamp'])
            scrape_date = scrape_dt.date()
        except:
            pass
    
    # Build company record from available sources (NO LLM)
    company = Company(
        company_id=company_id,
        legal_name=jsonld_name or company_id.title(),
        website=website_url,
        founded_year=founded_year,
        as_of=scrape_date,
        total_raised_usd=funding_summary.get('total_raised_usd'),
        last_round_name=funding_summary.get('last_round_name'),
        last_round_date=funding_summary.get('last_round_date'),
        last_disclosed_valuation_usd=funding_summary.get('last_disclosed_valuation_usd'),
        provenance=create_provenance(sources, ['homepage', 'about'])
    )
    
    # Override with pre-extracted data (highest priority - from scraper)
    if pre_extracted_company_info:
        if pre_extracted_company_info.get('founded_year'):
            company.founded_year = pre_extracted_company_info['founded_year']
            print(f"   âœ“ Founded year from pre-extracted: {company.founded_year}")
        
        if pre_extracted_company_info.get('hq_city'):
            company.hq_city = clean_hq_city(pre_extracted_company_info['hq_city'])
            print(f"   âœ“ HQ city from pre-extracted: {company.hq_city}")
        
        if pre_extracted_company_info.get('hq_state'):
            company.hq_state = pre_extracted_company_info['hq_state']
            print(f"   âœ“ HQ state from pre-extracted: {company.hq_state}")
        
        if pre_extracted_company_info.get('hq_country'):
            company.hq_country = pre_extracted_company_info['hq_country']
            print(f"   âœ“ HQ country from pre-extracted: {company.hq_country}")
        
        if pre_extracted_company_info.get('categories'):
            company.categories = clean_categories(pre_extracted_company_info['categories'])
            print(f"   âœ“ Categories from pre-extracted: {company.categories}")
    
    # FALLBACK: FORBES SEED (NO INFERENCE - ONLY DIRECT OVERRIDE)
    forbes_seed = sources.get('forbes_seed', {})
    
    if forbes_seed:
        print(f"   âœ“ Using Forbes seed as fallback source")
        
        # Override nulls with Forbes data (NO INFERENCE)
        if not company.hq_city and forbes_seed.get('hq_city'):
            company.hq_city = forbes_seed['hq_city']
            print(f"   âœ“ HQ city from Forbes: {company.hq_city}")
        
        if not company.hq_country and forbes_seed.get('hq_country'):
            company.hq_country = forbes_seed['hq_country']
            print(f"   âœ“ HQ country from Forbes: {company.hq_country}")
        
        if not company.categories and forbes_seed.get('category'):
            # Forbes has single category string, convert to list
            company.categories = [forbes_seed['category']]
            print(f"   âœ“ Category from Forbes: {company.categories}")
        
        if not company.founded_year and forbes_seed.get('founded_year'):
            # Validate year from Forbes
            year = forbes_seed['founded_year']
            if isinstance(year, int) and 1990 <= year <= 2023:
                company.founded_year = year
                print(f"   âœ“ Founded year from Forbes: {company.founded_year}")
    
    return company


def extract_visibility(sources: Dict[str, Any], company_id: str) -> Visibility:
    """ZERO HALLUCINATION: Extract visibility ONLY from pre-extracted scraper data."""
    
    # PRIORITY 1: Use pre-extracted visibility data from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    visibility_data = pre_extracted.get('visibility_data', {})
    
    # Count news articles from pre-extracted
    news_articles = pre_extracted.get('news_articles', [])
    press_releases = sources.get('press_releases', [])
    
    from datetime import timedelta
    thirty_days_ago = datetime.now() - timedelta(days=30)
    
    # Count recent news mentions
    recent_count = 0
    for article in news_articles:
        article_date = article.get('date_published') or article.get('date')
        if article_date:
            try:
                from dateutil import parser as date_parser
                article_dt = date_parser.parse(article_date)
                if article_dt >= thirty_days_ago:
                    recent_count += 1
            except:
                pass
    
    for pr in press_releases:
        try:
            pr_date = datetime.strptime(pr['date'], '%Y-%m-%d')
            if pr_date >= thirty_days_ago:
                recent_count += 1
        except:
            pass
    
    # Calculate sentiment from news articles
    positive_kw = ['launches', 'raises', 'partners', 'expands', 'announces', 'introduces']
    negative_kw = ['layoff', 'closes', 'incident', 'breach', 'lawsuit', 'investigation']
    
    positive = sum(1 for article in news_articles if any(kw in (article.get('title') or '').lower() for kw in positive_kw))
    negative = sum(1 for article in news_articles if any(kw in (article.get('title') or '').lower() for kw in negative_kw))
    
    positive += sum(1 for pr in press_releases if any(kw in pr['title'].lower() for kw in positive_kw))
    negative += sum(1 for pr in press_releases if any(kw in pr['title'].lower() for kw in negative_kw))
    
    total = positive + negative
    sentiment = (positive / total) if total > 0 else None
    
    # Get scrape date from metadata for as_of field
    scrape_date = date.today()
    if 'metadata' in sources and sources['metadata'].get('scrape_timestamp'):
        try:
            from dateutil import parser as date_parser
            scrape_dt = date_parser.parse(sources['metadata']['scrape_timestamp'])
            scrape_date = scrape_dt.date()
        except:
            pass
    
    visibility = Visibility(
        company_id=company_id,
        as_of=scrape_date,
        news_mentions_30d=recent_count if recent_count > 0 else None,
        avg_sentiment=sentiment,
        github_stars=visibility_data.get('github_stars'),
        glassdoor_rating=visibility_data.get('glassdoor_rating'),
        provenance=create_provenance(sources, ['press', 'homepage'],
            snippet=f"News mentions (30d): {recent_count}, Sentiment: {sentiment:.2f}" if total > 0 else None)
    )
    
    if visibility_data.get('github_stars'):
        print(f"   âœ“ GitHub stars from pre-extracted: {visibility_data['github_stars']}")
    if visibility_data.get('glassdoor_rating'):
        print(f"   âœ“ Glassdoor rating from pre-extracted: {visibility_data['glassdoor_rating']}")
    
    return visibility


def extract_news_articles(sources: Dict[str, Any], company_id: str) -> List[NewsArticle]:
    """ZERO HALLUCINATION: Extract news articles from pre-extracted scraper data or blog posts."""
    
    # PRIORITY 1: Use pre-extracted news articles from scraper (REAL DATA - NO HALLUCINATION)
    pre_extracted = sources.get('pre_extracted_entities', {})
    news_articles_raw = pre_extracted.get('news_articles', [])
    
    # PRIORITY 2: If no pre-extracted news, try to convert blog posts to news articles
    if not news_articles_raw:
        blog_posts = sources.get('blog_posts', [])
        if blog_posts:
            print(f"   ðŸ” No pre-extracted news articles - converting {len(blog_posts)} blog posts to news articles")
            # Convert blog posts to news articles format
            news_articles_raw = []
            for blog in blog_posts:
                # Extract title from first line or content
                content = blog.get('content', '')
                lines = [l.strip() for l in content.split('\n') if l.strip()]
                
                # Try to find a good title (skip very short lines, URLs, dates)
                title = blog.get('id', 'Untitled')
                for line in lines[:10]:
                    line_clean = line.strip()
                    # Skip if it's too short, looks like a URL, or is a date
                    if (len(line_clean) > 10 and len(line_clean) < 200 and 
                        not line_clean.startswith('http') and 
                        not re.match(r'^\d{4}-\d{2}-\d{2}', line_clean) and
                        not re.match(r'^[A-Z][a-z]+\s+\d{1,2},\s+\d{4}', line_clean)):
                        title = line_clean
                        break
                
                if len(title) > 200:
                    title = title[:200]
                
                # Extract excerpt (first substantial paragraph)
                excerpt = None
                paragraphs = content.split('\n\n')
                for para in paragraphs:
                    para_clean = para.strip()
                    if len(para_clean) > 50 and len(para_clean) < 500:
                        excerpt = para_clean
                        break
                
                # Try to extract date from content
                date_published = None
                date_patterns = [
                    r'(\d{4}-\d{2}-\d{2})',
                    r'([A-Z][a-z]+\s+\d{1,2},\s+\d{4})',
                    r'(\d{1,2}\s+[A-Z][a-z]+\s+\d{4})'
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, content[:500])
                    if match:
                        date_published = match.group(1)
                        break
                
                news_articles_raw.append({
                    'title': title,
                    'content': content,
                    'excerpt': excerpt,
                    'url': blog.get('url'),
                    'date_published': date_published,
                    'author': None,
                    'categories': [],
                    'tags': []
                })
        else:
            print(f"   âš ï¸  No news articles found in scraped data")
            return []
    
    if news_articles_raw:
        print(f"   âœ… Using {len(news_articles_raw)} news articles (NO HALLUCINATION)")
    
    articles = []
    blog_url_mapping = sources.get('blog_url_mapping', {})
    
    for idx, article_raw in enumerate(news_articles_raw):
        try:
            # Generate article_id
            title_slug = re.sub(r'[^a-z0-9]+', '_', (article_raw.get('title', '') or 'untitled').lower())[:50]
            article_id = f"{company_id}_news_{title_slug}_{idx}"
            
            # Get URL from blog_url_mapping if available
            url = article_raw.get('url')
            if not url:
                # Try to find URL from blog_url_mapping using article content
                article_content = (article_raw.get('content', '') or article_raw.get('title', ''))[:200]
                for blog_id, blog_info in blog_url_mapping.items():
                    if article_content[:100] in blog_info.get('content', '')[:500]:
                        url = blog_info.get('url')
                        break
            
            # Parse date if available
            date_published = article_raw.get('date_published') or article_raw.get('date')
            
            # Create provenance
            provenance_list = []
            if url:
                # Find crawled_at from blog_url_mapping or use current time
                crawled_at = datetime.now().isoformat()
                for blog_id, blog_info in blog_url_mapping.items():
                    if blog_info.get('url') == url:
                        crawled_at = blog_info.get('crawled_at', crawled_at)
                        break
                
                provenance_list.append(Provenance(
                    source_url=url,
                    crawled_at=crawled_at,
                    snippet=article_raw.get('excerpt') or article_raw.get('title', '')[:200]
                ))
            else:
                provenance_list.append(Provenance(
                    source_url=f"https://{company_id}.com/blog",
                    crawled_at=datetime.now().isoformat(),
                    snippet=article_raw.get('excerpt') or article_raw.get('title', '')[:200]
                ))
            
            # Truncate content if too long (keep first 5000 chars for payload)
            content = article_raw.get('content', '')
            if content and len(content) > 5000:
                content = content[:5000] + "..."
            
            # Handle author - convert list to string if needed
            author = article_raw.get('author')
            if isinstance(author, list):
                author = ', '.join(author) if author else None
            elif not isinstance(author, str):
                author = None
            
            article = NewsArticle(
                article_id=article_id,
                company_id=company_id,
                title=article_raw.get('title', 'Untitled'),
                url=url,
                author=author,
                date_published=date_published,
                excerpt=article_raw.get('excerpt'),
                content=content,
                categories=article_raw.get('categories', []),
                tags=article_raw.get('tags', []),
                word_count=article_raw.get('word_count'),
                reading_time=article_raw.get('reading_time'),
                provenance=provenance_list
            )
            articles.append(article)
        except Exception as e:
            print(f"   âš ï¸  Failed to convert news article {idx}: {e}")
            continue
    
    return articles


# ============================================================================
# MAIN ORCHESTRATOR
# ============================================================================

def extract_company_payload(company_id: str) -> Payload:
    """Extract complete payload using COMPREHENSIVE search + STRICT validation."""
    
    print(f"\n{'='*60}")
    print(f"ðŸ” EXTRACTING: {company_id}")
    print(f"ðŸ¤– Model: {model_name}")
    print(f"ðŸš« ZERO HALLUCINATION MODE ENABLED")
    print(f"{'='*60}")
    
    # Load ALL sources (text, HTML, JSON-LD, structured JSON)
    print("ðŸ“‚ Loading all sources...")
    sources = load_all_sources(company_id)
    print(f"   âœ“ {len(sources['files'])} text files")
    print(f"   âœ“ {len(sources['html_files'])} HTML files")
    print(f"   âœ“ {len(sources['structured_json'])} structured JSON files")
    print(f"   âœ“ {len(sources['jsonld_data'])} pages with JSON-LD")
    print(f"   âœ“ {len(sources['blog_posts'])} blog posts")
    print(f"   âœ“ {len(sources['press_releases'])} press releases")
    
    # Extract
    print("\nðŸ’° Funding...")
    funding_events, funding_summary = extract_funding_events(sources, company_id)
    print(f"   âœ“ {len(funding_events)} events")
    if funding_summary['total_raised_usd']:
        print(f"   âœ“ Total raised: ${funding_summary['total_raised_usd']:,}")
    else:
        print(f"   âš ï¸  Total raised: Not disclosed")
    
    print("\nðŸ‘¥ Leadership...")
    leadership = extract_leadership(sources, company_id)
    founders = [l for l in leadership if l.is_founder]
    print(f"   âœ“ {len(founders)} founders, {len(leadership)-len(founders)} executives")
    if len(leadership) == 0:
        print(f"   âš ï¸  No leadership found in scraped data")
    
    print("\nðŸ› ï¸  Products...")
    products = extract_products(sources, company_id)
    print(f"   âœ“ {len(products)} products")
    if len(products) == 0:
        print(f"   âš ï¸  No products found in scraped data")
    
    print("\nðŸ“Š Snapshot...")
    snapshot = extract_snapshot(sources, company_id, products)
    print(f"   âœ“ Snapshot: {snapshot.job_openings_count or 'hiring not disclosed'}")
    
    print("\nðŸ“… Events...")
    other_events = extract_other_events(sources, company_id)
    print(f"   âœ“ {len(other_events)} events")
    if len(other_events) == 0:
        print(f"   âš ï¸  No non-funding events with dates found")
    
    print("\nðŸ¢ Company record...")
    company = extract_company_record(sources, company_id, funding_summary)
    print(f"   âœ“ {company.legal_name}")
    print(f"   âœ“ Founded: {company.founded_year or 'Not disclosed'}")
    print(f"   âœ“ HQ: {company.hq_city or 'Not disclosed'}")
    
    print("\nðŸ“° Visibility...")
    visibility = extract_visibility(sources, company_id)
    print(f"   âœ“ News mentions (30d): {visibility.news_mentions_30d or 'Not available'}")
    
    print("\nðŸ“° News Articles...")
    news_articles = extract_news_articles(sources, company_id)
    print(f"   âœ“ {len(news_articles)} news articles")
    
    # Lab 5: Save structured data before payload assembly
    print("\nðŸ’¾ Lab 5: Saving structured data...")
    structured_data = {
        'company_id': company_id,
        'company': company.model_dump(),
        'events': [e.model_dump() for e in funding_events + other_events],
        'products': [p.model_dump() for p in products],
        'leadership': [l.model_dump() for l in leadership],
        'snapshot': snapshot.model_dump(),
        'visibility': visibility.model_dump(),
        'extracted_at': datetime.now().isoformat(),
        'sources_summary': {
            'text_files': len(sources['files']),
            'html_files': len(sources['html_files']),
            'blog_posts': len(sources['blog_posts']),
            'press_releases': len(sources['press_releases'])
        }
    }
    structured_path = save_structured_data(company_id, structured_data)
    if structured_path:
        print(f"   âœ… Structured data saved: {structured_path}")
    
    all_events = funding_events + other_events
    
    # Lab 6: Build Payload
    payload = Payload(
        company_record=company,
        events=all_events,
        snapshots=[snapshot],
        products=products,
        leadership=leadership,
        visibility=[visibility],
        news_articles=news_articles,
        notes="",
        provenance_policy="ZERO HALLUCINATION: Only data from scraped sources. Missing = null or 'Not disclosed'."
    )
    
    # Data quality summary
    print(f"\n{'='*60}")
    print(f"ðŸ“Š DATA QUALITY SUMMARY")
    print(f"{'='*60}")
    print(f"Events: {len(all_events)} total")
    print(f"  â””â”€ Funding: {len(funding_events)}")
    print(f"  â””â”€ Other: {len(other_events)}")
    if other_events:
        event_types = {}
        for e in other_events:
            event_types[e.event_type] = event_types.get(e.event_type, 0) + 1
        for event_type, count in sorted(event_types.items()):
            print(f"     â€¢ {event_type}: {count}")
    
    print(f"\nProducts: {len(products)}")
    if products:
        products_with_github = sum(1 for p in products if p.github_repo)
        products_with_license = sum(1 for p in products if p.license_type)
        print(f"  â””â”€ With GitHub repo: {products_with_github}")
        print(f"  â””â”€ With license info: {products_with_license}")
    
    print(f"\nLeadership: {len(leadership)}")
    print(f"  â””â”€ Founders: {len(founders)}")
    print(f"  â””â”€ Executives: {len(leadership)-len(founders)}")
    if leadership:
        with_linkedin = sum(1 for l in leadership if l.linkedin)
        with_education = sum(1 for l in leadership if l.education)
        print(f"  â””â”€ With LinkedIn: {with_linkedin}")
        print(f"  â””â”€ With education: {with_education}")
    
    print(f"\nSnapshot:")
    print(f"  â””â”€ Headcount: {snapshot.headcount_total or 'Not disclosed'}")
    print(f"  â””â”€ Job openings: {snapshot.job_openings_count or 'Not disclosed'}")
    print(f"  â””â”€ Engineering openings: {snapshot.engineering_openings or 'Not disclosed'}")
    print(f"  â””â”€ Sales openings: {snapshot.sales_openings or 'Not disclosed'}")
    print(f"  â””â”€ Offices: {len(snapshot.geo_presence)} locations")
    
    print(f"\nVisibility:")
    print(f"  â””â”€ News (30d): {visibility.news_mentions_30d or 'Not available'}")
    print(f"  â””â”€ Sentiment: {visibility.avg_sentiment or 'Not available'}")
    print(f"  â””â”€ GitHub stars: {visibility.github_stars or 'Not available'}")
    print(f"  â””â”€ Glassdoor: {visibility.glassdoor_rating or 'Not available'}")
    
    print(f"\nðŸ“° News Articles: {len(news_articles)} total")
    if news_articles:
        recent_articles = [a for a in news_articles if a.date_published][:5]
        print(f"  â””â”€ Recent articles: {len(recent_articles)}")
        for article in recent_articles[:3]:
            print(f"     â€¢ {article.title[:60]}...")
    
    print(f"{'='*60}\n")
    
    return payload


def process_companies(company_ids: List[str]):
    """Process multiple companies."""
    print(f"\n{'='*60}")
    print(f"ðŸš€ BATCH: {len(company_ids)} companies")
    print(f"{'='*60}")
    
    results = []
    
    for idx, company_id in enumerate(company_ids, 1):
        print(f"\n[{idx}/{len(company_ids)}] {company_id}")
        
        try:
            payload = extract_company_payload(company_id)
            
            # Lab 6: Save payload (supports both local and GCS)
            print(f"\nðŸ’¾ Lab 6: Saving payload...")
            payload_path = save_payload_to_storage(company_id, payload)
            if payload_path:
                print(f"âœ… Saved payload: {payload_path}")
                results.append({'company_id': company_id, 'status': 'success', 'payload_path': str(payload_path)})
            else:
                print(f"âš ï¸  Failed to save payload")
                results.append({'company_id': company_id, 'status': 'failed', 'error': 'Failed to save payload'})
            
        except Exception as e:
            print(f"âŒ Failed: {e}")
            results.append({'company_id': company_id, 'status': 'failed', 'error': str(e)})
    
    successful = [r for r in results if r['status'] == 'success']
    print(f"\n{'='*60}")
    print(f"âœ… Successful: {len(successful)}/{len(company_ids)}")
    print(f"{'='*60}")
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        # Use command line arguments if provided
        company_ids = sys.argv[1:]
        results = process_companies(company_ids)
    else:
        # Default test companies
        test_companies = ["harvey", "figure", "anthropic"]
        results = process_companies(test_companies)


# ============================================================================
# IMPROVEMENTS SUMMARY
# ============================================================================

"""
âœ… What Was Added (Anti-Hallucination Improvements):

1. is_website_section() - Filters 30+ website section patterns
   - Removes: "Blog", "Press Kit", "MOU with X", "Updates to Y", "Council", "Program"
   
2. extract_founded_year_aggressive() - Searches ALL text with 8 patterns
   - Patterns: "founded in", "established in", "since", "started in", etc.
   
3. Unique event IDs - Now includes full date (YYYY_MM_DD)
   - OLD: "anthropic_product_release_2025"
   - NEW: "anthropic_product_release_claude_sonnet_2025_09_29"
   
4. Leadership cross-validation - Stronger prompts
   - "Is this person CURRENTLY at {company}?"
   - "If they work at another company â†’ SKIP"
   
5. Timeline-only event extraction - STRICT validation
   - "Event MUST appear in TIMELINE"
   - "If NOT in timeline â†’ DO NOT extract"

âœ… What Was Kept (Comprehensive Search):

- ALL HTML parsing (team, pricing, locations, copyright, headcount, GitHub, Glassdoor, jobs)
- ALL JSON-LD extraction (Organization, Product, Person, Event schemas)
- ALL source searching (text files, HTML files, blog posts)
- ALL field keyword mappings (30+ categories)
- ALL structured data extraction
- Complete provenance tracking
"""